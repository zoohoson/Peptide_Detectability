{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Al3Xe5wkve0s"
   },
   "source": [
    "# 1. Extract data\n",
    "\n",
    ">1|0<br>\n",
    "AAYLLAKINLKALAALAKKIL<br>\n",
    ">2|0<br>\n",
    "AEKVDPVKLNLTLSAAAEALTGLGDK<br>\n",
    ">3|0<br>\n",
    "AGYLLGKINLKALAALAKKIL<br>\n",
    "...<br>\n",
    ">372|1<br>\n",
    "TKKDLTQWFFKITDYADELLDKLD<br>\n",
    ">373|1<br>\n",
    "SGWNAYIDTMTAAAP<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "xZB3HGozvfNE",
    "outputId": "2235c2dd-4a07-4f78-a696-90e43bde01ac"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Seqs</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAAAARRRIRKQAHAHSK</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADVFDRGGPYLQRGVADLVPTATLLDTYSP</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AGCKNFFWKTFTSC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AGYLLGKLKALAALAKKIL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AHALCLTERQIKSNRRMKWKKEN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Seqs  Label\n",
       "0              AAAAARRRIRKQAHAHSK      1\n",
       "1  ADVFDRGGPYLQRGVADLVPTATLLDTYSP      1\n",
       "2                  AGCKNFFWKTFTSC      1\n",
       "3             AGYLLGKLKALAALAKKIL      1\n",
       "4         AHALCLTERQIKSNRRMKWKKEN      1"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# filename = 'CPP.txt'\n",
    "# x = []\n",
    "# y = []\n",
    "# with open(filename) as f:\n",
    "#     for index,line in enumerate(f.readlines()):\n",
    "#         if(index%2==0):\n",
    "#             label = int(line.split('|')[1])\n",
    "#             y.append(label)\n",
    "        \n",
    "#         else:\n",
    "#             x.append(line.strip('\\n'))\n",
    "\n",
    "# c={\"Seqs\" : x,\n",
    "#    \"Label\" : y}\n",
    "# cppdata=pd.DataFrame(c)\n",
    "# cppdata.to_csv(\"CPP.csv\",index=False)\n",
    "# cppdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-23T16:04:13.891873Z",
     "start_time": "2022-01-23T16:04:13.365678Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Seqs</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>QELNEPPKQSTSFLVLQEILESEEKGDPNK</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Seqs  Label\n",
       "0  QELNEPPKQSTSFLVLQEILESEEKGDPNK      0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cppdata = pd.read_csv('train_.csv')\n",
    "\n",
    "cppdata.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WfPCPTO-ztId"
   },
   "source": [
    "# 2. Divide training set and test set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-23T16:04:26.035553Z",
     "start_time": "2022-01-23T16:04:19.242873Z"
    },
    "id": "CCCkkzWjwRX-"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "# create train and test set \n",
    "kf = KFold(n_splits=10,shuffle=True)\n",
    "a= 0\n",
    "for train, test in kf.split(cppdata):\n",
    "  a=a+1 \n",
    "  temptrain = cppdata.iloc[train]\n",
    "  temptest = cppdata.iloc[test]\n",
    "  temptrain.to_csv(\"train%d.csv\" %a, index=False)\n",
    "  temptest.to_csv(\"test%d.csv\" %a, index=False)\n",
    "# train, test = train_test_split(cppdata, test_size=0.1)\n",
    "# train.to_csv(\"train.csv\", index=False)\n",
    "# test.to_csv(\"test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWBAcEQv0Fez"
   },
   "source": [
    "# 3. Preprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-23T16:04:26.685052Z",
     "start_time": "2022-01-23T16:04:26.037375Z"
    },
    "id": "IevxcQX9w6oF"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import spacy\n",
    "import torch\n",
    "from torchtext import data, datasets  # .legacy\n",
    "from torchtext.vocab import Vectors\n",
    "from torch.nn import init\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from torch.autograd import Variable\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-23T16:04:26.691254Z",
     "start_time": "2022-01-23T16:04:26.686761Z"
    },
    "id": "zXZxsFQa0LMP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bis/miniconda3/envs/torch/lib/python3.6/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenizer(text): # create a tokenizer function\n",
    "    \"\"\"\n",
    "    定义分词操作\n",
    "    \"\"\"\n",
    "    return list(text)\n",
    "\n",
    "\"\"\"\n",
    "field在默认的情况下都期望一个输入是一组单词的序列，并且将单词映射成整数。\n",
    "这个映射被称为vocab。如果一个field已经被数字化了并且不需要被序列化，\n",
    "可以将参数设置为use_vocab=False以及sequential=False。\n",
    "\"\"\"\n",
    "LABEL = data.Field(sequential=False, use_vocab=False)\n",
    "TEXT = data.Field(sequential=True, tokenize=tokenizer,fix_length=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-23T16:04:52.553550Z",
     "start_time": "2022-01-23T16:04:46.616135Z"
    },
    "id": "hQQEhZkN0c99"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bis/miniconda3/envs/torch/lib/python3.6/site-packages/torchtext/data/example.py:68: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
      "/home/bis/miniconda3/envs/torch/lib/python3.6/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "train,val,test = data.TabularDataset.splits(\n",
    "        path='.', train='train_.csv',validation='val_.csv',test='test_.csv', format='csv',skip_header=True,\n",
    "        fields=[('Seqs', TEXT), ('Label', LABEL)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-23T16:04:52.558946Z",
     "start_time": "2022-01-23T16:04:52.555361Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "53cbs89Z0ean",
    "outputId": "4e71d6a2-dbcd-42e7-b3d6-f6bf15f2c184"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchtext.data.example.Example object at 0x7efd53da1198>\n",
      "dict_keys(['Seqs', 'Label'])\n",
      "['H', 'H', 'C', 'A', 'R', 'Q', 'R', 'L', 'R'] 0\n"
     ]
    }
   ],
   "source": [
    "print(train[5])\n",
    "print(train[5].__dict__.keys())\n",
    "print(train[5].Seqs,train[5].Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-23T16:04:55.001145Z",
     "start_time": "2022-01-23T16:04:52.560285Z"
    },
    "id": "HU3H4qB-08mN"
   },
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train,val,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-23T16:04:55.005239Z",
     "start_time": "2022-01-23T16:04:55.002524Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WKvWW1JE1GL1",
    "outputId": "994e8d1a-2c8e-4483-eddc-46e394437997"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad>\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.itos[1])\n",
    "print(TEXT.vocab.stoi['N'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-23T16:04:56.081185Z",
     "start_time": "2022-01-23T16:04:56.076384Z"
    },
    "id": "rnkUfE981XVM"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bis/miniconda3/envs/torch/lib/python3.6/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "train_iter = data.BucketIterator(train, batch_size=32, sort_key=lambda x: len(x.Seqs), \n",
    "                                 shuffle=True,device=DEVICE)\n",
    "\n",
    "val_iter = data.BucketIterator(val, batch_size=32, sort_key=lambda x: len(x.Seqs), \n",
    "                                 shuffle=True,device=DEVICE)\n",
    "\n",
    "test_iter = data.BucketIterator(val, batch_size=32, sort_key=lambda x: len(x.Seqs), \n",
    "                                 shuffle=True,device=DEVICE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HEozgl3s2Mcq"
   },
   "source": [
    "# 4. Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-23T16:05:00.371642Z",
     "start_time": "2022-01-23T16:05:00.368544Z"
    },
    "id": "ObFiT7jXqsFU"
   },
   "outputs": [],
   "source": [
    "epsilon = 0.00000001\n",
    "def squash(x):\n",
    "    # not concern batch_size, maybe rewrite\n",
    "    s_squared_norm = torch.sum(x*x,1,keepdim=True) + epsilon\n",
    "    scale = torch.sqrt(s_squared_norm)/(1. + s_squared_norm)\n",
    "    # out = (batch_size,1,10)*(batch_size,16,10) = (batch_size,16,10)\n",
    "    out = scale * x\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-23T16:05:01.215225Z",
     "start_time": "2022-01-23T16:05:01.206777Z"
    },
    "id": "H_kJkmS-qxIl"
   },
   "outputs": [],
   "source": [
    "class Capsule(nn.Module):\n",
    "\n",
    "    def __init__(self, in_units,in_channels, num_capsule, dim_capsule, routings=3, **kwargs):\n",
    "        super(Capsule, self).__init__(**kwargs)\n",
    "        self.in_units = in_units\n",
    "        self.in_channels = in_channels\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        # (in_units,10,128,16)\n",
    "        self.W = nn.Parameter((torch.randn(self.in_units,self.num_capsule,self.in_channels, self.dim_capsule)))\n",
    "\n",
    "    def forward(self, u_vecs):\n",
    "        u_vecs = u_vecs.permute(0,2,1)\n",
    "        u_vecs = u_vecs.unsqueeze(2)\n",
    "        u_vecs = u_vecs.unsqueeze(2)\n",
    "\n",
    "        \n",
    "        # (batch_size,in_units,1,1,in_channels)*(in_units,10,in_channels,16) = (batch_size,in_units,10,1,16)\n",
    "        u_hat_vecs = torch.matmul(u_vecs,self.W)\n",
    "        # (batch_size,in_units,10,16)\n",
    "        u_hat_vecs = u_hat_vecs.permute(0,1,2,4,3).squeeze(4)\n",
    "        \n",
    "        # (batch_size,10,in_units,16)\n",
    "        u_hat_vecs2 = u_hat_vecs.permute(0,2,1,3)\n",
    "    \n",
    "        # (batch_size,10,1,in_units)\n",
    "        b = torch.zeros(u_hat_vecs.size(0),self.num_capsule,1,self.in_units,device=DEVICE)\n",
    "        for i in range(self.routings):\n",
    "            # (batch_size,10,1,in_units)\n",
    "            c = F.softmax(b,-1)\n",
    "            # s = (batch_size,10,1,in_units)*(batch_size,10,in_units,16) = (batch_size,10,1,16)\n",
    "            s = torch.matmul(c,u_hat_vecs2)\n",
    "            # (batch_size,16,10)\n",
    "            s = s.permute(0,3,1,2).squeeze(3)\n",
    "            # (batch_size,16,10)\n",
    "            v = squash(s)\n",
    "            # here\n",
    "            # (batch_size,10,16,1)\n",
    "            v = v.permute(0,2,1).unsqueeze(3)\n",
    "            # (batch_size,10,in_units,16)*(batch_size,10,16,1) = (batch_size,10,in_units,1)\n",
    "            sim = torch.matmul(u_hat_vecs2,v)\n",
    "            # (batch_size,10,1,in_units)\n",
    "            sim = sim.permute(0,1,3,2)\n",
    "            b = b+sim\n",
    "        # (batch_size,16,10)\n",
    "        return v.permute(0,2,1,3).squeeze(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-23T16:05:02.071965Z",
     "start_time": "2022-01-23T16:05:02.067101Z"
    },
    "id": "OnDBKedYQE_f"
   },
   "outputs": [],
   "source": [
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-23T16:05:02.295597Z",
     "start_time": "2022-01-23T16:05:02.220610Z"
    },
    "id": "DfjBegd-1-j0"
   },
   "outputs": [],
   "source": [
    "len_vocab = len(TEXT.vocab)\n",
    "embed_size=20\n",
    "n_class=2\n",
    "n_hidden =32\n",
    "\n",
    "energy = [\n",
    "[-1.65,  -2.83, 1.16,\t1.80,\t-3.73,\t-0.41,\t1.90,\t-3.69,\t0.49,\t-3.01,\t-2.08,\t0.66,\t1.54,\t1.20,\t0.98, -0.08,  0.46, -2.31,\t0.32,\t-4.62],\n",
    "[-2.83,\t-39.58,\t-0.82,\t-0.53,\t-3.07,\t-2.96,\t-4.98,\t0.34,\t-1.38,\t-2.15,\t1.43,\t-4.18,\t-2.13,\t-2.91,\t-0.41,\t-2.33,\t-1.84,\t-0.16,\t4.26,\t-4.46],\n",
    "[1.16,\t-0.82,\t0.84,\t1.97,\t-0.92,\t0.88,\t-1.07,\t0.68,\t-1.93,\t0.23,\t0.61,\t0.32,\t3.31,\t2.67,\t-2.02,\t0.91,\t-0.65,\t0.94,\t-0.71,\t0.90],\n",
    "[1.80,\t-0.53,\t1.97,\t1.45,\t0.94,\t1.31,\t0.61,\t1.30,\t-2.51,\t1.14,\t2.53,\t0.20,\t1.44,\t0.10,\t-3.13,\t0.81,\t1.54,\t0.12,\t-1.07,\t1.29],\n",
    "[-3.73,\t-3.07,\t-0.92,\t0.94,\t-11.25,\t0.35,\t-3.57,\t-5.88,\t-0.82,\t-8.59,\t-5.34,\t0.73,\t0.32,\t0.77,\t-0.40,\t-2.22,\t0.11,\t-7.05,\t-7.09,\t-8.80],\n",
    "[-0.41,\t-2.96,\t0.88,\t1.31,\t0.35,\t-0.20,\t1.09,\t-0.65,\t-0.16,\t-0.55,\t-0.52,\t-0.32,\t2.25,\t1.11,\t0.84,\t0.71,\t0.59,\t-0.38,\t1.69,\t-1.90],\n",
    "[1.90,\t-4.98,\t-1.07,\t0.61,\t-3.57,\t1.09,\t1.97,\t-0.71,\t2.89,\t-0.86,\t-0.75,\t1.84,\t0.35,\t2.64,\t2.05,\t0.82,\t-0.01,\t0.27,\t-7.58,\t-3.20],\n",
    "[-3.69,\t0.34,\t0.68,\t1.30,\t-5.88,\t-0.65,\t-0.71,\t-6.74,\t-0.01,\t-9.01,\t-3.62,\t-0.07,\t0.12,\t-0.18,\t0.19,\t-0.15,\t0.63,\t-6.54,\t-3.78,\t-5.26],\n",
    "[0.49,\t-1.38,\t-1.93,\t-2.51,\t-0.82,\t-0.16,\t2.89,\t-0.01,\t1.24,\t0.49,\t1.61,\t1.12,\t0.51,\t0.43,\t2.34,\t0.19,\t-1.11,\t0.19,\t0.02,\t-1.19],\n",
    "[-3.01,\t-2.15,\t0.23,\t1.14,\t-8.59,\t-0.55,\t-0.86,\t-9.01,\t0.49,\t-6.37,\t-2.88,\t0.97,\t1.81,\t-0.58,\t-0.60,\t-0.41,\t0.72,\t-5.43,\t-8.31,\t-4.90],\n",
    "[-2.08,\t1.43,\t0.61,\t2.53,\t-5.34,\t-0.52,\t-0.75,\t-3.62,\t1.61,\t-2.88,\t-6.49,\t0.21,\t0.75,\t1.90,\t2.09,\t1.39,\t0.63,\t-2.59,\t-6.88,\t-9.73],\n",
    "[0.66,\t-4.18,\t0.32,\t0.20,\t0.73,\t-0.32,\t1.84,\t-0.07,\t1.12,\t0.97,\t0.21,\t0.61,\t1.15,\t1.28,\t1.08,\t0.29,\t0.46,\t0.93,\t-0.74,\t0.93],\n",
    "[1.54,\t-2.13,\t3.31,\t1.44,\t0.32,\t2.25,\t0.35,\t0.12,\t0.51,\t1.81,\t0.75,\t1.15,\t-0.42,\t2.97,\t1.06,\t1.12,\t1.65,\t0.38,\t-2.06,\t-2.09],\n",
    "[1.20,\t-2.91,\t2.67,\t0.10,\t0.77,\t1.11,\t2.64,\t-0.18,\t0.43,\t-0.58,\t1.90,\t1.28,\t2.97,\t-1.54,\t0.91,\t0.85,\t-0.07,\t-1.91,\t-0.76,\t0.01],\n",
    "[0.98,\t-0.41,\t-2.02,\t-3.13,\t-0.40,\t0.84,\t2.05,\t0.19,\t2.34,\t-0.60,\t2.09,\t1.08,\t1.06,\t0.91,\t0.21,\t0.95,\t0.98,\t0.08,\t-5.89,\t0.36],\n",
    "[-0.08,\t-2.33,\t0.91,\t0.81,\t-2.22,\t0.71,\t0.82,\t-0.15,\t0.19,\t-0.41,\t1.39,\t0.29,\t1.12,\t0.85,\t0.95,\t-0.48,\t-0.06,\t0.13,\t-3.03,\t-0.82],\n",
    "[0.46,\t-1.84,\t-0.65,\t1.54,\t0.11,\t0.59,\t-0.01,\t0.63,\t-1.11,\t0.72,\t0.63,\t0.46,\t1.65,\t-0.07,\t0.98,\t-0.06,\t-0.96,\t1.14,\t-0.65,\t-0.37],\n",
    "[-2.31,\t-0.16,\t0.94,\t0.12,\t-7.05,\t-0.38,\t0.27,\t-6.54,\t0.19,\t-5.43,\t-2.59,\t0.93,\t0.38,\t-1.91,\t0.08,\t0.13,\t1.14,\t-4.82,\t-2.13,\t-3.59],\n",
    "[0.32,\t4.26,\t-0.71,\t-1.07,\t-7.09,\t1.69,\t-7.58,\t-3.78,\t0.02,\t-8.31,\t-6.88,\t-0.74,\t-2.06,\t-0.76,\t-5.89,\t-3.03,\t-0.65,\t-2.13,\t-1.73,\t-12.39],\n",
    "[-4.62,\t-4.46,\t0.90,\t1.29,\t-8.80,\t-1.90,\t-3.20,\t-5.26,\t-1.19,\t-4.90,\t-9.73,\t0.93,\t-2.09,\t0.01,\t0.36,\t-0.82,\t-0.37,\t-3.59,\t-12.39,\t-2.68],\n",
    "]\n",
    "\n",
    "physicochemical = [\n",
    "    [-0.4, -0.5, 15, 8.1, 0.046, 0.67, 1.28, 0.3, 0, 0.687, 115, 0.28, 154.330012, 27.5, 1.181, 0.0072,0,0,0,0],\n",
    "    [0.17, -1, 47, 5.5, 0.128, 0.38, 1.77, 0.9, 2.75, 0.263, 135, 0.28, 219.789, 44.6, 1.461, -0.037,0,0,0,0],\n",
    "    [-1.31, 3.0, 59, 13.0, 0.105, -1.2, 1.6, -0.6, 1.38, 0.632, 150, 0.21, 194.910002, 40.0, 1.587, 0.0238,0,0,0,0],\n",
    "    [-1.22, 3.0, 73, 12.3, 0.151, -0.76, 1.56, -0.7, 0.92, 0.669, 190, 0.33, 223.160, 62, 1.862, 0.0068,0,0,0,0],\n",
    "    [1.92, -2.5, 91, 5.2, 0.29, 2.3, 2.94, 0.5, 0, 0.577, 210, 2.18, 204.7, 115.5, 2.228, 0.0376,0,0,0,0],\n",
    "    [-0.67, 0, 1, 9, 0, 0, 0, 0.3, 0.74, 0.67, 75, 0.18, 127.9, 0, 0.881, 0.179,0,0,0,0],\n",
    "    [-0.64, -0.5, 82, 10.4, 0.23, 0.64, 2.99, -0.1, 0.58, 0.594, 195, 0.21, 242.539, 79, 2.025, -0.011,0,0,0,0],\n",
    "    [1.25, -1.5, 57, 5.2, 0.186, 1.9, 4.19, 0.7, 0, 0.564, 175, 0.82, 233.210, 93.5, 1.81, 0.0216,0,0,0,0],\n",
    "    [-0.67, 3, 73, 11.3, 0.219, -0.57, 1.89, -1.8, 0.33, 0.407, 200, 0.09, 300.459, 100, 2.258, 0.0177,0,0,0,0],\n",
    "    [1.22, -1.8, 57, 4.9, 0.186, 1.9, 2.59, 0.5, 0, 0.541, 170, 1, 232.3, 93.5, 1.931, 0.0517,0,0,0,0],\n",
    "    [1.02, -1.3, 75, 5.7, 0.0221, 2.4, 2.35, 0.4, 0, 0.328, 185, 0.74, 202.699, 94.1, 2.034, 0.0027,0,0,0,0],\n",
    "    [-0.92, 0.2, 58, 11.6, 0.134, -0.61, 1.6, -0.5, 1.33, 0.489, 160, 0.25, 207.899, 58.7, 1.655, 0.0054,0,0,0,0],\n",
    "    [-0.49, 0, 42, 8.0, 0.131, 102, 2.67, -0.3, 0.39, 0.600, 145, 0.39, 179.929, 41.9, 1.468, 0.239,0,0,0,0],\n",
    "    [-0.91, 0.2, 72, 10.5, 0.180, -0.22, 1.56, -0.7, 0.9, 0.527, 183, 0.35, 235.509, 80.7, 1.932, 0.0692,0,0,0,0],\n",
    "    [-0.59, 3, 101, 10.5, 0.291, -2.10, 2.34, -1.4, 0.64, 0.591, 225, 0.1, 341.0, 105, 2.56, 0.0436,0,0,0,0],\n",
    "    [-0.55, 0.3, 31, 9.2, 0.062, 0.01, 1.31, -0.1, 1.41, 0.693, 116, 0.12, 174.059, 29.3, 1.298, 0.0043,0,0,0,0],\n",
    "    [-0.28, -0.4, 45, 8.6, 0.108, 0.52, 3.03, -0.2, 0.71, 0.713, 142, 0.21, 205.5, 51.3, 1.525, 0.034,0,0,0,0],\n",
    "    [0.91, -1.5, 43, 5.9, 0.14, 1.5, 3.67, 0.6, 0, 0.529, 157, 0.6, 207, 71.5, 1.645, 0.057,0,0,0,0],\n",
    "    [0.5, -3.4, 130, 5.4, 0.409, 2.6, 3.21, 0.3, 0.12, 0.632, 258, 5.7, 237, 145.5, 2.663, 0.058,0,0,0,0],\n",
    "    [1.67, -2.3, 107, 6.2, 0.298, 1.6, 2.94, -0.4, 0.21, 0.493, 234, 1.26, 229.14, 117.3, 2.368, 0.0236,0,0,0,0]\n",
    "]\n",
    "\n",
    "RE = {'A':0,'C':1,'D':2,'E':3,'F':4,'G':5,'H':6,'I':7,'K':8,'L':9,'M':10,'N':11,'P':12,'Q':13,'R':14,'S':15,'T':16,'V':17,'W':18,'Y':19}\n",
    "def RECMEncoding(inpStr):\n",
    "  RECMT=[]\n",
    "  for x in inpStr:\n",
    "    if x in RE:\n",
    "      oneTi = energy[RE.get(x)]\n",
    "      RECMT.append(oneTi)\n",
    "  return RECMT\n",
    "    \n",
    "def RECMcompositionEncoding(inpStr):\n",
    "  RECMcomposition=[]\n",
    "  countNum = {'A':0,'C':0,'D':0,'E':0,'F':0,'G':0,'H':0,'I':0,'K':0,'L':0,'M':0,'N':0,'P':0,'Q':0,'R':0,'S':0,'T':0,'V':0,'W':0,'Y':0}\n",
    "  for i in inpStr:\n",
    "    if i in countNum:\n",
    "      value = countNum.get(i)+1\n",
    "      countNum[i] = value\n",
    "  for i in countNum:\n",
    "    oneTi = np.array(energy[RE.get(i)])*int(countNum.get(i))\n",
    "    # a = GetPseRECM(RECMEncoding(i))\n",
    "    # i = np.concatenate((oneTi,a),0)\n",
    "    RECMcomposition.append(oneTi)\n",
    "  RECMcomposition = np.array(RECMcomposition)\n",
    "  return RECMcomposition\n",
    "      \n",
    "def GetPseRECM(RECMT):\n",
    "  feature =[]\n",
    "  legth = 0\n",
    "  r = 3\n",
    "  legth = 20+20*(r-1)\n",
    "  # 取平均特征\n",
    "  for j in range(20):\n",
    "    averageColumn = 0\n",
    "    for i in range(len(RECMT)):\n",
    "      averageColumn = averageColumn + RECMT[i][j]\n",
    "    averageColumn = averageColumn/len(RECMT)\n",
    "    feature.append(averageColumn)\n",
    "  for k in range(1,r):\n",
    "    for j in range(20):\n",
    "      dist = 0\n",
    "      for i in range(len(RECMT)-k):\n",
    "        dist = dist +pow((RECMT[i][j]-RECMT[i+k][j]),2)\n",
    "      dist = dist/(len(RECMT)-k)\n",
    "      feature.append(dist)\n",
    "  feature = np.array(feature)\n",
    "  return feature\n",
    "\n",
    "def residueRatio(inpStr):\n",
    "    feature = []\n",
    "    countNum = {'A':0,'C':0,'D':0,'E':0,'F':0,'G':0,'H':0,'I':0,'K':0,'L':0,'M':0,'N':0,'P':0,'Q':0,'R':0,'S':0,'T':0,'V':0,'W':0,'Y':0}\n",
    "    total = 0\n",
    "    for i in inpStr:\n",
    "        total = total+1\n",
    "        if i in countNum:\n",
    "            value = countNum.get(i)+1\n",
    "            countNum[i] = value\n",
    "    for i in countNum:\n",
    "        oneResidueRatio = countNum.get(i)#/total\n",
    "        feature.append(oneResidueRatio)\n",
    "    feature = np.array(feature)\n",
    "    return feature\n",
    "\n",
    "\n",
    "\n",
    "def dipeptideRatio(inpStr):\n",
    "    # print(inpStr)\n",
    "    dipeptideFeature = np.zeros((20,20))\n",
    "    total = 0\n",
    "    for i in range(len(inpStr)-1):\n",
    "        total = total+1\n",
    "        x = RE.get(inpStr[i])\n",
    "        y = RE.get(inpStr[i+1])\n",
    "        dipeptideFeature[x][y] = dipeptideFeature[x][y] +1\n",
    "    # dipeptideFeature = dipeptideFeature/total\n",
    "    return dipeptideFeature\n",
    "\n",
    "\n",
    "def physicochemicalFeature(inpStr,fixlength):\n",
    "    pfeature=[]\n",
    "    Slength = 0\n",
    "    fix = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "    for x in inpStr:\n",
    "        if x in RE:\n",
    "            Slength = Slength+1\n",
    "            oneTi = physicochemical[RE.get(x)]\n",
    "            pfeature.append(oneTi)\n",
    "    fixlength = fixlength-Slength\n",
    "    for i in range(fixlength):\n",
    "        pfeature.append(fix)\n",
    "    pfeature = np.array(pfeature)\n",
    "    return pfeature\n",
    "\n",
    "def featureGenera(t):\n",
    "  flag =0 \n",
    "  for i in t:\n",
    "    protein = ''   \n",
    "    flag = flag+1\n",
    "    for j in i:\n",
    "      if j!=1:\n",
    "        a = TEXT.vocab.itos[j]\n",
    "        protein = protein+a\n",
    "    # print(protein)\n",
    "    featureOne = RECMcompositionEncoding(protein)\n",
    "    # print(featureOne.shape)\n",
    "    featureTwo = GetPseRECM(RECMEncoding(protein))\n",
    "    featureThree = dipeptideRatio(protein)\n",
    "    featureFour = residueRatio(protein)\n",
    "    featureFive = physicochemicalFeature(protein,45)\n",
    "    # print(featureTwo.shape)\n",
    "    featureTwo =featureTwo.reshape(3,20)\n",
    "    featureFour = featureFour.reshape(1,20)\n",
    "    featureOne = torch.from_numpy(featureOne)\n",
    "    featureTwo = torch.from_numpy(featureTwo)\n",
    "    featureThree = torch.from_numpy(featureThree)\n",
    "    featureFour = torch.from_numpy(featureFour)\n",
    "    featureFive = torch.from_numpy(featureFive)\n",
    "    featureThree.type_as(featureTwo)\n",
    "    featureFour.type_as(featureTwo)\n",
    "    featureFive.type_as(featureTwo)\n",
    "    # print(featureOne.shape)\n",
    "    # print(featureTwo.shape)\n",
    "    # print(featureThree.shape)\n",
    "    # print(featureFour.shape)\n",
    "    feature1 = torch.cat((featureOne,featureTwo),0)\n",
    "    # print(feature1.shape)\n",
    "    feature2 = torch.cat((featureThree,featureFour.type_as(featureThree)),0)\n",
    "    # # print(feature2.shape)\n",
    "    feature3 = torch.cat((feature1,feature2),0)\n",
    "    feature = torch.cat((feature3,featureFive),0)\n",
    "\n",
    "    # print(feature.shape)\n",
    "    # print(feature.shape)\n",
    "    if(flag==1):\n",
    "      # print(feature.shape)\n",
    "      feature = feature.unsqueeze(0)\n",
    "      temp = feature\n",
    "    if(flag!=1):\n",
    "      # print(feature.shape)\n",
    "      feature = feature.unsqueeze(0)\n",
    "      temp = torch.cat((temp,feature),0)\n",
    "  # print(temp.shape)\n",
    "  return temp\n",
    "\n",
    "\n",
    "class Enet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Enet, self).__init__()\n",
    "        self.embedding = nn.Embedding(len_vocab,embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size,64,batch_first=True)#,bidirectional=True)\n",
    "        self.conv = nn.Conv1d(embed_size,32,3)\n",
    "        self.pool = nn.MaxPool1d(32)\n",
    "        self.linear = nn.Linear(64,n_class)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size,seq_num = x.shape\n",
    "        y = featureGenera(x)\n",
    "        # print(x.shape)\n",
    "        vec = self.embedding(x)\n",
    "        # print(vec.shape)\n",
    "        # vec = torch.cat((vec,y.type_as(vec)),1) \n",
    "        out, (hn, cn) = self.lstm(vec)\n",
    "        # print(out.shape)\n",
    "        #out = self.conv(vec.permute(0,2,1))\n",
    "        out = F.relu(out)\n",
    "        out = self.linear(out[:,-1,:])\n",
    "        # out = self.linear(out)\n",
    "        out = F.softmax(out,-1)\n",
    "        return out\n",
    "\n",
    "class CBAMBlock(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(CBAMBlock, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.channel_excitation = nn.Sequential(nn.Linear(channel,int(channel//reduction),bias=False),\n",
    "                                                nn.ReLU(inplace=True),\n",
    "                                                nn.Linear(int(channel//reduction),channel,bias=False),\n",
    "                                                )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        self.spatial_excitation = nn.Sequential(nn.Conv2d(2, 1, kernel_size=7,\n",
    "                                                 stride=1, padding=3, bias=False),\n",
    "                                               )\n",
    "\n",
    "    def forward(self, x):\n",
    "        bahs, chs, _, _ = x.size()\n",
    "\n",
    "        # Returns a new tensor with the same data as the self tensor but of a different size.\n",
    "        chn_avg = self.avg_pool(x).view(bahs, chs)\n",
    "        chn_avg = self.channel_excitation(chn_avg).view(bahs, chs, 1, 1)\n",
    "        chn_max = self.max_pool(x).view(bahs, chs)\n",
    "        chn_max = self.channel_excitation(chn_max).view(bahs, chs, 1, 1)\n",
    "        chn_add=chn_avg+chn_max\n",
    "        chn_add=self.sigmoid(chn_add)\n",
    "\n",
    "        chn_cbam = torch.mul(x, chn_add)\n",
    "\n",
    "        avg_out = torch.mean(chn_cbam, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(chn_cbam, dim=1, keepdim=True)\n",
    "        cat = torch.cat([avg_out, max_out], dim=1)\n",
    "        spa_add = self.spatial_excitation(cat)\n",
    "        spa_add=self.sigmoid(spa_add)\n",
    "\n",
    "        spa_cbam = torch.mul(chn_cbam, spa_add)\n",
    "        return spa_cbam\n",
    "\n",
    "\n",
    "class CapsuleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 1,28x28\n",
    "        self.embedding = nn.Embedding(len_vocab,embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size,40,batch_first=True)#,bidirectional=True)\n",
    "        self.conv1=nn.Conv2d(1,256,9)\n",
    "        self.conv3=nn.Conv2d(1,256,8)\n",
    "        self.cbamBlock = CBAMBlock(256) \n",
    "        self.conv2=nn.Conv2d(256,32*8,9,2)\n",
    "        self.conv4=nn.Conv2d(256,32*8,8,2) \n",
    "        self.capsule = Capsule(2304,16,2,32)\n",
    "        # self.Decoder = Decoder()\n",
    "   \n",
    "    def forward(self,x):\n",
    "        batch_size = x.size(0)\n",
    "        # Conv1\n",
    "        # print(x)\n",
    "        y = featureGenera(x)\n",
    "        # print(y.shape)\n",
    "        out = self.embedding(x)\n",
    "        # print(out.shape)\n",
    "        #out, (hn, cn) = self.lstm(out)\n",
    "        # print(out.shape)\n",
    "        #out = out[:,-1,:].reshape(batch_size,2,20)\n",
    "        y = y.type_as(out)\n",
    "        # out = y\n",
    "        #out = torch.cat((out,y.type_as(out)),1)\n",
    "        # print(out)\n",
    "        out = out.unsqueeze(1)\n",
    "        y = y.unsqueeze(1)\n",
    "        #（16,1,25,20）\n",
    "        out = self.conv1(out)\n",
    "        out = self.cbamBlock(out)\n",
    "        y = self.conv3(y)\n",
    "        y = self.cbamBlock(y)\n",
    "        #（16,256,17,12）\n",
    "        out = F.relu(out)\n",
    "        y = F.relu(y)\n",
    "\n",
    "        #out = self.seLayer(out)\n",
    "        # PrimaryCaps\n",
    "        out = self.conv2(out)\n",
    "        y = self.conv4(y)\n",
    "        #(16,256,5,2)\n",
    "        out = F.relu(out)\n",
    "        y = F.relu(y)\n",
    "        out = out.view(batch_size,16,-1)\n",
    "        y = y.view(batch_size,16,-1)\n",
    "        # print(y.shape)\n",
    "        # print(out.shape)\n",
    "        out = torch.cat((out,y),2)\n",
    "        #(16,8,320)\n",
    "        out = squash(out)\n",
    "        # wj(batch_size,8,1152)\n",
    "        out = out.view(out.size(0),out.size(1),-1)\n",
    "        #(16,8,320)\n",
    "        # Capsule\n",
    "        # wj(batch_size,16,10)\n",
    "        out = self.capsule(out)\n",
    "        #(16,16,2)\n",
    "        # wj(batch_size,10,16)\n",
    "        out = out.permute(0,2,1)\n",
    "        # (16,2,16)\n",
    "        # decoder = self.Decoder(out,label)\n",
    "        return out#,decoder\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iH1NzWFc2Tr2"
   },
   "source": [
    "# 5. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-23T16:05:06.174769Z",
     "start_time": "2022-01-23T16:05:03.428908Z"
    },
    "colab": {
     "background_save": true
    },
    "id": "FHgwvO-_2PYI"
   },
   "outputs": [],
   "source": [
    "model = CapsuleNet()\n",
    "\"\"\"\n",
    "将前面生成的词向量矩阵拷贝到模型的embedding层\n",
    "这样就自动的可以将输入的word index转为词向量\n",
    "\"\"\"\n",
    "\n",
    "# 训练\n",
    "model.to(DEVICE)\n",
    "\n",
    "# 训练\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
    "\n",
    "n_epoch = 12\n",
    "\n",
    "best_val_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-23T16:05:06.194062Z",
     "start_time": "2022-01-23T16:05:06.176119Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+------------+\n",
      "|                Modules                | Parameters |\n",
      "+---------------------------------------+------------+\n",
      "|            embedding.weight           |    440     |\n",
      "|           lstm.weight_ih_l0           |    3200    |\n",
      "|           lstm.weight_hh_l0           |    6400    |\n",
      "|            lstm.bias_ih_l0            |    160     |\n",
      "|            lstm.bias_hh_l0            |    160     |\n",
      "|              conv1.weight             |   20736    |\n",
      "|               conv1.bias              |    256     |\n",
      "|              conv3.weight             |   16384    |\n",
      "|               conv3.bias              |    256     |\n",
      "| cbamBlock.channel_excitation.0.weight |    4096    |\n",
      "| cbamBlock.channel_excitation.2.weight |    4096    |\n",
      "| cbamBlock.spatial_excitation.0.weight |     98     |\n",
      "|              conv2.weight             |  5308416   |\n",
      "|               conv2.bias              |    256     |\n",
      "|              conv4.weight             |  4194304   |\n",
      "|               conv4.bias              |    256     |\n",
      "|               capsule.W               |  2359296   |\n",
      "+---------------------------------------+------------+\n",
      "Total Trainable Params: 11918810\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11918810"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        param = parameter.numel()\n",
    "        table.add_row([name, param])\n",
    "        total_params+=param\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-23T16:05:06.197890Z",
     "start_time": "2022-01-23T16:05:06.195262Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 918, 810)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "11,918,810"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-24T11:22:10.684201Z",
     "start_time": "2022-01-23T16:05:06.198930Z"
    },
    "colab": {
     "background_save": true
    },
    "id": "FHgwvO-_2PYI",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bis/miniconda3/envs/torch/lib/python3.6/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/home/bis/miniconda3/envs/torch/lib/python3.6/site-packages/ipykernel_launcher.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 \t batch_idx : 99 \t loss: 0.2038 \t train acc: 0.5000\n",
      "epoch: 0 \t batch_idx : 199 \t loss: 0.1819 \t train acc: 0.6250\n",
      "epoch: 0 \t batch_idx : 299 \t loss: 0.1229 \t train acc: 0.7812\n",
      "epoch: 0 \t batch_idx : 399 \t loss: 0.1631 \t train acc: 0.7188\n",
      "epoch: 0 \t batch_idx : 499 \t loss: 0.1314 \t train acc: 0.7812\n",
      "epoch: 0 \t batch_idx : 599 \t loss: 0.1174 \t train acc: 0.7188\n",
      "epoch: 0 \t batch_idx : 699 \t loss: 0.1532 \t train acc: 0.7500\n",
      "epoch: 0 \t batch_idx : 799 \t loss: 0.1302 \t train acc: 0.7812\n",
      "epoch: 0 \t batch_idx : 899 \t loss: 0.1482 \t train acc: 0.7500\n",
      "epoch: 0 \t batch_idx : 999 \t loss: 0.1289 \t train acc: 0.7812\n",
      "epoch: 0 \t batch_idx : 1099 \t loss: 0.1410 \t train acc: 0.7188\n",
      "epoch: 0 \t batch_idx : 1199 \t loss: 0.1642 \t train acc: 0.6875\n",
      "epoch: 0 \t batch_idx : 1299 \t loss: 0.1414 \t train acc: 0.8125\n",
      "epoch: 0 \t batch_idx : 1399 \t loss: 0.1150 \t train acc: 0.8125\n",
      "epoch: 0 \t batch_idx : 1499 \t loss: 0.1448 \t train acc: 0.7812\n",
      "epoch: 0 \t batch_idx : 1599 \t loss: 0.1454 \t train acc: 0.7812\n",
      "epoch: 0 \t batch_idx : 1699 \t loss: 0.1262 \t train acc: 0.8125\n",
      "epoch: 0 \t batch_idx : 1799 \t loss: 0.1449 \t train acc: 0.7812\n",
      "epoch: 0 \t batch_idx : 1899 \t loss: 0.1022 \t train acc: 0.8750\n",
      "epoch: 0 \t batch_idx : 1999 \t loss: 0.1698 \t train acc: 0.7812\n",
      "epoch: 0 \t batch_idx : 2099 \t loss: 0.1599 \t train acc: 0.7500\n",
      "epoch: 0 \t batch_idx : 2199 \t loss: 0.1458 \t train acc: 0.7812\n",
      "epoch: 0 \t batch_idx : 2299 \t loss: 0.1559 \t train acc: 0.7812\n",
      "epoch: 0 \t batch_idx : 2399 \t loss: 0.1744 \t train acc: 0.7188\n",
      "epoch: 0 \t batch_idx : 2499 \t loss: 0.1040 \t train acc: 0.7500\n",
      "epoch: 0 \t batch_idx : 2599 \t loss: 0.1213 \t train acc: 0.8438\n",
      "epoch: 0 \t batch_idx : 2699 \t loss: 0.0979 \t train acc: 0.8750\n",
      "epoch: 0 \t batch_idx : 2799 \t loss: 0.1621 \t train acc: 0.6875\n",
      "epoch: 0 \t batch_idx : 2899 \t loss: 0.1075 \t train acc: 0.8438\n",
      "epoch: 0 \t batch_idx : 2999 \t loss: 0.1345 \t train acc: 0.7500\n",
      "epoch: 0 \t batch_idx : 3099 \t loss: 0.0949 \t train acc: 0.8750\n",
      "epoch: 0 \t batch_idx : 3199 \t loss: 0.1201 \t train acc: 0.7812\n",
      "epoch: 0 \t batch_idx : 3299 \t loss: 0.2027 \t train acc: 0.5938\n",
      "epoch: 0 \t batch_idx : 3399 \t loss: 0.1668 \t train acc: 0.7188\n",
      "epoch: 0 \t batch_idx : 3499 \t loss: 0.1646 \t train acc: 0.7188\n",
      "epoch: 0 \t batch_idx : 3599 \t loss: 0.0928 \t train acc: 0.8125\n",
      "epoch: 0 \t batch_idx : 3699 \t loss: 0.1307 \t train acc: 0.7812\n",
      "epoch: 0 \t batch_idx : 3799 \t loss: 0.1068 \t train acc: 0.8750\n",
      "epoch: 0 \t batch_idx : 3899 \t loss: 0.1690 \t train acc: 0.6875\n",
      "epoch: 0 \t batch_idx : 3999 \t loss: 0.0885 \t train acc: 0.8750\n",
      "epoch: 0 \t batch_idx : 4099 \t loss: 0.0887 \t train acc: 0.9062\n",
      "epoch: 0 \t batch_idx : 4199 \t loss: 0.1217 \t train acc: 0.7812\n",
      "epoch: 0 \t batch_idx : 4299 \t loss: 0.1183 \t train acc: 0.8125\n",
      "epoch: 0 \t batch_idx : 4399 \t loss: 0.1829 \t train acc: 0.8125\n",
      "epoch: 0 \t batch_idx : 4499 \t loss: 0.0853 \t train acc: 0.8750\n",
      "epoch: 0 \t batch_idx : 4599 \t loss: 0.1289 \t train acc: 0.7812\n",
      "epoch: 0 \t batch_idx : 4699 \t loss: 0.1470 \t train acc: 0.7812\n",
      "epoch: 0 \t batch_idx : 4799 \t loss: 0.1202 \t train acc: 0.7500\n",
      "epoch: 0 \t batch_idx : 4899 \t loss: 0.1306 \t train acc: 0.7812\n",
      "epoch: 0 \t batch_idx : 4999 \t loss: 0.1357 \t train acc: 0.7500\n",
      "epoch: 0 \t batch_idx : 5099 \t loss: 0.1170 \t train acc: 0.7812\n",
      "epoch: 0 \t batch_idx : 5199 \t loss: 0.1196 \t train acc: 0.8438\n",
      "epoch: 0 \t batch_idx : 5299 \t loss: 0.1255 \t train acc: 0.8125\n",
      "epoch: 0 \t batch_idx : 5399 \t loss: 0.1907 \t train acc: 0.6875\n",
      "epoch: 0 \t batch_idx : 5499 \t loss: 0.1273 \t train acc: 0.7500\n",
      "epoch: 0 \t batch_idx : 5599 \t loss: 0.1390 \t train acc: 0.7500\n",
      "epoch: 0 \t batch_idx : 5699 \t loss: 0.1050 \t train acc: 0.8438\n",
      "epoch: 0 \t batch_idx : 5799 \t loss: 0.0835 \t train acc: 0.8750\n",
      "epoch: 0 \t batch_idx : 5899 \t loss: 0.1724 \t train acc: 0.7500\n",
      "epoch: 0 \t batch_idx : 5999 \t loss: 0.1478 \t train acc: 0.7500\n",
      "epoch: 0 \t batch_idx : 6099 \t loss: 0.1531 \t train acc: 0.7188\n",
      "epoch: 0 \t batch_idx : 6199 \t loss: 0.1697 \t train acc: 0.7188\n",
      "epoch: 0 \t batch_idx : 6299 \t loss: 0.0818 \t train acc: 0.9062\n",
      "epoch: 0 \t batch_idx : 6399 \t loss: 0.1081 \t train acc: 0.8125\n",
      "epoch: 0 \t batch_idx : 6499 \t loss: 0.1357 \t train acc: 0.8125\n",
      "epoch: 0 \t batch_idx : 6599 \t loss: 0.1511 \t train acc: 0.6875\n",
      "epoch: 0 \t batch_idx : 6699 \t loss: 0.1853 \t train acc: 0.6250\n",
      "epoch: 0 \t batch_idx : 6799 \t loss: 0.1661 \t train acc: 0.7188\n",
      "epoch: 0 \t batch_idx : 6899 \t loss: 0.1062 \t train acc: 0.8125\n",
      "epoch: 0 \t batch_idx : 6999 \t loss: 0.1359 \t train acc: 0.7500\n",
      "epoch: 0 \t batch_idx : 7099 \t loss: 0.1087 \t train acc: 0.8750\n",
      "epoch: 0 \t batch_idx : 7199 \t loss: 0.1234 \t train acc: 0.8125\n",
      "epoch: 0 \t batch_idx : 7299 \t loss: 0.1141 \t train acc: 0.8438\n",
      "epoch: 0 \t batch_idx : 7399 \t loss: 0.1713 \t train acc: 0.6875\n",
      "epoch: 0 \t batch_idx : 7499 \t loss: 0.1370 \t train acc: 0.8438\n",
      "epoch: 0 \t batch_idx : 7599 \t loss: 0.1561 \t train acc: 0.7500\n",
      "epoch: 0 \t batch_idx : 7699 \t loss: 0.0586 \t train acc: 0.9062\n",
      "epoch: 0 \t batch_idx : 7799 \t loss: 0.1002 \t train acc: 0.8125\n",
      "epoch: 0 \t batch_idx : 7899 \t loss: 0.0823 \t train acc: 0.8750\n",
      "epoch: 0 \t batch_idx : 7999 \t loss: 0.0879 \t train acc: 0.9375\n",
      "epoch: 0 \t batch_idx : 8099 \t loss: 0.1131 \t train acc: 0.8438\n",
      "epoch: 0 \t batch_idx : 8199 \t loss: 0.1043 \t train acc: 0.8438\n",
      "epoch: 0 \t batch_idx : 8299 \t loss: 0.0896 \t train acc: 0.8438\n",
      "epoch: 0 \t batch_idx : 8399 \t loss: 0.1586 \t train acc: 0.7188\n",
      "epoch: 0 \t batch_idx : 8499 \t loss: 0.1250 \t train acc: 0.8125\n",
      "epoch: 0 \t batch_idx : 8599 \t loss: 0.0967 \t train acc: 0.8438\n",
      "epoch: 0 \t batch_idx : 8699 \t loss: 0.0993 \t train acc: 0.9062\n",
      "epoch: 0 \t batch_idx : 8799 \t loss: 0.1300 \t train acc: 0.7500\n",
      "epoch: 0 \t batch_idx : 8899 \t loss: 0.1482 \t train acc: 0.7500\n",
      "epoch: 0 \t batch_idx : 8999 \t loss: 0.1224 \t train acc: 0.7500\n",
      "epoch: 0 \t batch_idx : 9099 \t loss: 0.0972 \t train acc: 0.9062\n",
      "epoch: 0 \t batch_idx : 9199 \t loss: 0.0964 \t train acc: 0.8438\n",
      "epoch: 0 \t batch_idx : 9299 \t loss: 0.1354 \t train acc: 0.8125\n",
      "epoch: 0 \t batch_idx : 9399 \t loss: 0.1396 \t train acc: 0.7500\n",
      "epoch: 0 \t batch_idx : 9499 \t loss: 0.0915 \t train acc: 0.9062\n",
      "epoch: 0 \t batch_idx : 9599 \t loss: 0.0880 \t train acc: 0.9062\n",
      "epoch: 0 \t batch_idx : 9699 \t loss: 0.1397 \t train acc: 0.8438\n",
      "epoch: 0 \t batch_idx : 9799 \t loss: 0.0928 \t train acc: 0.7812\n",
      "epoch: 0 \t batch_idx : 9899 \t loss: 0.1320 \t train acc: 0.8125\n",
      "epoch: 0 \t batch_idx : 9999 \t loss: 0.1602 \t train acc: 0.7500\n",
      "epoch: 0 \t batch_idx : 10099 \t loss: 0.1110 \t train acc: 0.8438\n",
      "epoch: 0 \t batch_idx : 10199 \t loss: 0.1324 \t train acc: 0.8125\n",
      "epoch: 0 \t batch_idx : 10299 \t loss: 0.1040 \t train acc: 0.8750\n",
      "epoch: 0 \t batch_idx : 10399 \t loss: 0.1009 \t train acc: 0.8750\n",
      "epoch: 0 \t batch_idx : 10499 \t loss: 0.0811 \t train acc: 0.8750\n",
      "epoch: 0 \t batch_idx : 10599 \t loss: 0.0982 \t train acc: 0.8438\n",
      "epoch: 0 \t batch_idx : 10699 \t loss: 0.1112 \t train acc: 0.8438\n",
      "epoch: 0 \t batch_idx : 10799 \t loss: 0.1038 \t train acc: 0.8125\n",
      "epoch: 0 \t batch_idx : 10899 \t loss: 0.1205 \t train acc: 0.8438\n",
      "epoch: 0 \t batch_idx : 10999 \t loss: 0.0886 \t train acc: 0.9375\n",
      "epoch: 0 \t batch_idx : 11099 \t loss: 0.1571 \t train acc: 0.7500\n",
      "epoch: 0 \t batch_idx : 11199 \t loss: 0.1014 \t train acc: 0.9062\n",
      "epoch: 0 \t batch_idx : 11299 \t loss: 0.1587 \t train acc: 0.6875\n",
      "epoch: 0 \t batch_idx : 11399 \t loss: 0.1553 \t train acc: 0.7812\n",
      "epoch: 0 \t batch_idx : 11499 \t loss: 0.1272 \t train acc: 0.8125\n",
      "epoch: 0 \t batch_idx : 11599 \t loss: 0.0829 \t train acc: 0.8125\n",
      "epoch: 0 \t batch_idx : 11699 \t loss: 0.1524 \t train acc: 0.7188\n",
      "epoch: 0 \t batch_idx : 11799 \t loss: 0.1068 \t train acc: 0.8125\n",
      "epoch: 0 \t batch_idx : 11899 \t loss: 0.0718 \t train acc: 0.9062\n",
      "epoch: 0 \t batch_idx : 11999 \t loss: 0.1403 \t train acc: 0.7500\n",
      "epoch: 0 \t batch_idx : 12099 \t loss: 0.1504 \t train acc: 0.6875\n",
      "epoch: 0 \t batch_idx : 12199 \t loss: 0.0864 \t train acc: 0.8750\n",
      "epoch: 0 \t batch_idx : 12299 \t loss: 0.1342 \t train acc: 0.7812\n",
      "epoch: 0 \t batch_idx : 12399 \t loss: 0.1469 \t train acc: 0.6562\n",
      "epoch: 0 \t batch_idx : 12499 \t loss: 0.1375 \t train acc: 0.7188\n",
      "epoch: 0 \t batch_idx : 12599 \t loss: 0.1118 \t train acc: 0.8125\n",
      "epoch: 0 \t batch_idx : 12699 \t loss: 0.1700 \t train acc: 0.6875\n",
      "epoch: 0 \t batch_idx : 12799 \t loss: 0.1175 \t train acc: 0.8125\n",
      "epoch: 0 \t batch_idx : 12899 \t loss: 0.1130 \t train acc: 0.7812\n",
      "epoch: 0 \t batch_idx : 12999 \t loss: 0.1178 \t train acc: 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 \t batch_idx : 13099 \t loss: 0.1043 \t train acc: 0.8438\n",
      "epoch: 0 \t batch_idx : 13199 \t loss: 0.0955 \t train acc: 0.8750\n",
      "epoch: 0 \t batch_idx : 13299 \t loss: 0.1401 \t train acc: 0.7812\n",
      "epoch: 0 \t batch_idx : 13399 \t loss: 0.1362 \t train acc: 0.6875\n",
      "epoch: 0 \t batch_idx : 13499 \t loss: 0.1135 \t train acc: 0.7500\n",
      "epoch: 0 \t batch_idx : 13599 \t loss: 0.1455 \t train acc: 0.7500\n",
      "epoch: 0 \t batch_idx : 13699 \t loss: 0.1589 \t train acc: 0.6875\n",
      "epoch: 0 \t batch_idx : 13799 \t loss: 0.0792 \t train acc: 0.8750\n",
      "epoch: 0 \t batch_idx : 13899 \t loss: 0.1404 \t train acc: 0.7500\n",
      "epoch: 0 \t batch_idx : 13999 \t loss: 0.0720 \t train acc: 0.9375\n",
      "epoch: 0 \t batch_idx : 14099 \t loss: 0.1412 \t train acc: 0.8438\n",
      "epoch: 0 \t batch_idx : 14199 \t loss: 0.0867 \t train acc: 0.8750\n",
      "epoch: 0 \t batch_idx : 14299 \t loss: 0.0798 \t train acc: 0.8438\n",
      "epoch: 0 \t batch_idx : 14399 \t loss: 0.1259 \t train acc: 0.8125\n",
      "epoch: 0 \t batch_idx : 14499 \t loss: 0.1461 \t train acc: 0.7500\n",
      "epoch: 0 \t batch_idx : 14599 \t loss: 0.1074 \t train acc: 0.8438\n",
      "epoch: 0 \t batch_idx : 14699 \t loss: 0.0902 \t train acc: 0.8750\n",
      "epoch: 0 \t batch_idx : 14799 \t loss: 0.1102 \t train acc: 0.8750\n",
      "epoch: 0 \t batch_idx : 14899 \t loss: 0.1550 \t train acc: 0.7500\n",
      "epoch: 0 \t batch_idx : 14999 \t loss: 0.1102 \t train acc: 0.7812\n",
      "epoch: 0 \t batch_idx : 15099 \t loss: 0.1002 \t train acc: 0.8125\n",
      "epoch: 0 \t batch_idx : 15199 \t loss: 0.0825 \t train acc: 0.9062\n",
      "epoch: 0 \t batch_idx : 15299 \t loss: 0.1543 \t train acc: 0.7188\n",
      "epoch: 0 \t batch_idx : 15399 \t loss: 0.1091 \t train acc: 0.8438\n",
      "epoch: 0 \t batch_idx : 15499 \t loss: 0.1788 \t train acc: 0.6875\n",
      "epoch: 0 \t batch_idx : 15599 \t loss: 0.1414 \t train acc: 0.8125\n",
      "epoch: 0 \t batch_idx : 15699 \t loss: 0.1248 \t train acc: 0.7500\n",
      "epoch: 0 \t batch_idx : 15799 \t loss: 0.1254 \t train acc: 0.8125\n",
      "epoch: 0 \t batch_idx : 15899 \t loss: 0.1990 \t train acc: 0.5938\n",
      "epoch: 0 \t batch_idx : 15999 \t loss: 0.1491 \t train acc: 0.7812\n",
      "epoch: 0 \t batch_idx : 16099 \t loss: 0.1128 \t train acc: 0.8438\n",
      "epoch: 0 \t batch_idx : 16199 \t loss: 0.1257 \t train acc: 0.7812\n",
      "epoch: 0 \t batch_idx : 16299 \t loss: 0.1017 \t train acc: 0.8438\n",
      "epoch: 0 \t batch_idx : 16399 \t loss: 0.1087 \t train acc: 0.7812\n",
      "epoch: 0 \t batch_idx : 16499 \t loss: 0.1001 \t train acc: 0.8438\n",
      "epoch: 0 \t batch_idx : 16599 \t loss: 0.1373 \t train acc: 0.7812\n",
      "epoch: 0 \t batch_idx : 16699 \t loss: 0.1630 \t train acc: 0.6875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bis/miniconda3/envs/torch/lib/python3.6/site-packages/ipykernel_launcher.py:41: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc : 0.8041 > 0.0000 saving model\n",
      "test acc: 0.8041\n",
      "epoch: 1 \t batch_idx : 99 \t loss: 0.0854 \t train acc: 0.8750\n",
      "epoch: 1 \t batch_idx : 199 \t loss: 0.1159 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 299 \t loss: 0.1156 \t train acc: 0.7812\n",
      "epoch: 1 \t batch_idx : 399 \t loss: 0.1462 \t train acc: 0.7500\n",
      "epoch: 1 \t batch_idx : 499 \t loss: 0.1485 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 599 \t loss: 0.1768 \t train acc: 0.7188\n",
      "epoch: 1 \t batch_idx : 699 \t loss: 0.1137 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 799 \t loss: 0.1402 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 899 \t loss: 0.1001 \t train acc: 0.8438\n",
      "epoch: 1 \t batch_idx : 999 \t loss: 0.0942 \t train acc: 0.9062\n",
      "epoch: 1 \t batch_idx : 1099 \t loss: 0.1035 \t train acc: 0.8438\n",
      "epoch: 1 \t batch_idx : 1199 \t loss: 0.1063 \t train acc: 0.8438\n",
      "epoch: 1 \t batch_idx : 1299 \t loss: 0.1344 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 1399 \t loss: 0.1344 \t train acc: 0.7500\n",
      "epoch: 1 \t batch_idx : 1499 \t loss: 0.0800 \t train acc: 0.9062\n",
      "epoch: 1 \t batch_idx : 1599 \t loss: 0.1199 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 1699 \t loss: 0.1284 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 1799 \t loss: 0.0861 \t train acc: 0.8750\n",
      "epoch: 1 \t batch_idx : 1899 \t loss: 0.0745 \t train acc: 0.9062\n",
      "epoch: 1 \t batch_idx : 1999 \t loss: 0.0673 \t train acc: 0.9062\n",
      "epoch: 1 \t batch_idx : 2099 \t loss: 0.1805 \t train acc: 0.6562\n",
      "epoch: 1 \t batch_idx : 2199 \t loss: 0.1189 \t train acc: 0.8438\n",
      "epoch: 1 \t batch_idx : 2299 \t loss: 0.1586 \t train acc: 0.6875\n",
      "epoch: 1 \t batch_idx : 2399 \t loss: 0.1492 \t train acc: 0.7188\n",
      "epoch: 1 \t batch_idx : 2499 \t loss: 0.1595 \t train acc: 0.7500\n",
      "epoch: 1 \t batch_idx : 2599 \t loss: 0.1493 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 2699 \t loss: 0.1332 \t train acc: 0.7500\n",
      "epoch: 1 \t batch_idx : 2799 \t loss: 0.1210 \t train acc: 0.8438\n",
      "epoch: 1 \t batch_idx : 2899 \t loss: 0.0717 \t train acc: 0.9375\n",
      "epoch: 1 \t batch_idx : 2999 \t loss: 0.1163 \t train acc: 0.8750\n",
      "epoch: 1 \t batch_idx : 3099 \t loss: 0.1611 \t train acc: 0.6875\n",
      "epoch: 1 \t batch_idx : 3199 \t loss: 0.0953 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 3299 \t loss: 0.1021 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 3399 \t loss: 0.0584 \t train acc: 0.9062\n",
      "epoch: 1 \t batch_idx : 3499 \t loss: 0.0993 \t train acc: 0.8750\n",
      "epoch: 1 \t batch_idx : 3599 \t loss: 0.1392 \t train acc: 0.7500\n",
      "epoch: 1 \t batch_idx : 3699 \t loss: 0.1197 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 3799 \t loss: 0.1397 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 3899 \t loss: 0.1875 \t train acc: 0.7188\n",
      "epoch: 1 \t batch_idx : 3999 \t loss: 0.1488 \t train acc: 0.7500\n",
      "epoch: 1 \t batch_idx : 4099 \t loss: 0.1344 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 4199 \t loss: 0.1277 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 4299 \t loss: 0.0928 \t train acc: 0.8750\n",
      "epoch: 1 \t batch_idx : 4399 \t loss: 0.1075 \t train acc: 0.7812\n",
      "epoch: 1 \t batch_idx : 4499 \t loss: 0.1405 \t train acc: 0.7500\n",
      "epoch: 1 \t batch_idx : 4599 \t loss: 0.0837 \t train acc: 0.8438\n",
      "epoch: 1 \t batch_idx : 4699 \t loss: 0.1311 \t train acc: 0.7812\n",
      "epoch: 1 \t batch_idx : 4799 \t loss: 0.0856 \t train acc: 0.8438\n",
      "epoch: 1 \t batch_idx : 4899 \t loss: 0.1301 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 4999 \t loss: 0.1378 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 5099 \t loss: 0.0726 \t train acc: 0.8750\n",
      "epoch: 1 \t batch_idx : 5199 \t loss: 0.0764 \t train acc: 0.8750\n",
      "epoch: 1 \t batch_idx : 5299 \t loss: 0.1033 \t train acc: 0.8750\n",
      "epoch: 1 \t batch_idx : 5399 \t loss: 0.0705 \t train acc: 0.8750\n",
      "epoch: 1 \t batch_idx : 5499 \t loss: 0.1063 \t train acc: 0.8750\n",
      "epoch: 1 \t batch_idx : 5599 \t loss: 0.0849 \t train acc: 0.8750\n",
      "epoch: 1 \t batch_idx : 5699 \t loss: 0.0565 \t train acc: 0.9688\n",
      "epoch: 1 \t batch_idx : 5799 \t loss: 0.0818 \t train acc: 0.8438\n",
      "epoch: 1 \t batch_idx : 5899 \t loss: 0.1422 \t train acc: 0.6875\n",
      "epoch: 1 \t batch_idx : 5999 \t loss: 0.0730 \t train acc: 0.9375\n",
      "epoch: 1 \t batch_idx : 6099 \t loss: 0.1641 \t train acc: 0.7188\n",
      "epoch: 1 \t batch_idx : 6199 \t loss: 0.0737 \t train acc: 0.8750\n",
      "epoch: 1 \t batch_idx : 6299 \t loss: 0.0906 \t train acc: 0.8438\n",
      "epoch: 1 \t batch_idx : 6399 \t loss: 0.1084 \t train acc: 0.8438\n",
      "epoch: 1 \t batch_idx : 6499 \t loss: 0.1400 \t train acc: 0.7500\n",
      "epoch: 1 \t batch_idx : 6599 \t loss: 0.1398 \t train acc: 0.7812\n",
      "epoch: 1 \t batch_idx : 6699 \t loss: 0.1920 \t train acc: 0.6562\n",
      "epoch: 1 \t batch_idx : 6799 \t loss: 0.1413 \t train acc: 0.7812\n",
      "epoch: 1 \t batch_idx : 6899 \t loss: 0.1302 \t train acc: 0.6875\n",
      "epoch: 1 \t batch_idx : 6999 \t loss: 0.1269 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 7099 \t loss: 0.0964 \t train acc: 0.8438\n",
      "epoch: 1 \t batch_idx : 7199 \t loss: 0.0998 \t train acc: 0.8438\n",
      "epoch: 1 \t batch_idx : 7299 \t loss: 0.0972 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 7399 \t loss: 0.1390 \t train acc: 0.7500\n",
      "epoch: 1 \t batch_idx : 7499 \t loss: 0.0874 \t train acc: 0.8438\n",
      "epoch: 1 \t batch_idx : 7599 \t loss: 0.1310 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 7699 \t loss: 0.0688 \t train acc: 0.9062\n",
      "epoch: 1 \t batch_idx : 7799 \t loss: 0.1292 \t train acc: 0.7500\n",
      "epoch: 1 \t batch_idx : 7899 \t loss: 0.0756 \t train acc: 0.8750\n",
      "epoch: 1 \t batch_idx : 7999 \t loss: 0.1068 \t train acc: 0.8750\n",
      "epoch: 1 \t batch_idx : 8099 \t loss: 0.1407 \t train acc: 0.7812\n",
      "epoch: 1 \t batch_idx : 8199 \t loss: 0.0961 \t train acc: 0.8438\n",
      "epoch: 1 \t batch_idx : 8299 \t loss: 0.0884 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 8399 \t loss: 0.0919 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 8499 \t loss: 0.1497 \t train acc: 0.7188\n",
      "epoch: 1 \t batch_idx : 8599 \t loss: 0.0368 \t train acc: 0.9688\n",
      "epoch: 1 \t batch_idx : 8699 \t loss: 0.1454 \t train acc: 0.7812\n",
      "epoch: 1 \t batch_idx : 8799 \t loss: 0.1099 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 8899 \t loss: 0.0538 \t train acc: 0.9688\n",
      "epoch: 1 \t batch_idx : 8999 \t loss: 0.0776 \t train acc: 0.8438\n",
      "epoch: 1 \t batch_idx : 9099 \t loss: 0.0887 \t train acc: 0.8438\n",
      "epoch: 1 \t batch_idx : 9199 \t loss: 0.1356 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 9299 \t loss: 0.1314 \t train acc: 0.7500\n",
      "epoch: 1 \t batch_idx : 9399 \t loss: 0.0889 \t train acc: 0.8750\n",
      "epoch: 1 \t batch_idx : 9499 \t loss: 0.0993 \t train acc: 0.7812\n",
      "epoch: 1 \t batch_idx : 9599 \t loss: 0.0777 \t train acc: 0.8750\n",
      "epoch: 1 \t batch_idx : 9699 \t loss: 0.1142 \t train acc: 0.8438\n",
      "epoch: 1 \t batch_idx : 9799 \t loss: 0.0816 \t train acc: 0.9062\n",
      "epoch: 1 \t batch_idx : 9899 \t loss: 0.0620 \t train acc: 0.9375\n",
      "epoch: 1 \t batch_idx : 9999 \t loss: 0.0666 \t train acc: 0.9062\n",
      "epoch: 1 \t batch_idx : 10099 \t loss: 0.1075 \t train acc: 0.8438\n",
      "epoch: 1 \t batch_idx : 10199 \t loss: 0.1605 \t train acc: 0.7188\n",
      "epoch: 1 \t batch_idx : 10299 \t loss: 0.1387 \t train acc: 0.7500\n",
      "epoch: 1 \t batch_idx : 10399 \t loss: 0.1124 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 10499 \t loss: 0.1095 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 10599 \t loss: 0.1354 \t train acc: 0.7188\n",
      "epoch: 1 \t batch_idx : 10699 \t loss: 0.1032 \t train acc: 0.7812\n",
      "epoch: 1 \t batch_idx : 10799 \t loss: 0.1131 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 10899 \t loss: 0.1112 \t train acc: 0.7500\n",
      "epoch: 1 \t batch_idx : 10999 \t loss: 0.0622 \t train acc: 0.9375\n",
      "epoch: 1 \t batch_idx : 11099 \t loss: 0.0829 \t train acc: 0.9062\n",
      "epoch: 1 \t batch_idx : 11199 \t loss: 0.1350 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 11299 \t loss: 0.0491 \t train acc: 0.9688\n",
      "epoch: 1 \t batch_idx : 11399 \t loss: 0.1441 \t train acc: 0.7500\n",
      "epoch: 1 \t batch_idx : 11499 \t loss: 0.1138 \t train acc: 0.7812\n",
      "epoch: 1 \t batch_idx : 11599 \t loss: 0.1314 \t train acc: 0.7812\n",
      "epoch: 1 \t batch_idx : 11699 \t loss: 0.1051 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 11799 \t loss: 0.0805 \t train acc: 0.8750\n",
      "epoch: 1 \t batch_idx : 11899 \t loss: 0.1489 \t train acc: 0.7812\n",
      "epoch: 1 \t batch_idx : 11999 \t loss: 0.1760 \t train acc: 0.7188\n",
      "epoch: 1 \t batch_idx : 12099 \t loss: 0.1699 \t train acc: 0.7500\n",
      "epoch: 1 \t batch_idx : 12199 \t loss: 0.1537 \t train acc: 0.7812\n",
      "epoch: 1 \t batch_idx : 12299 \t loss: 0.0497 \t train acc: 0.9375\n",
      "epoch: 1 \t batch_idx : 12399 \t loss: 0.0942 \t train acc: 0.8750\n",
      "epoch: 1 \t batch_idx : 12499 \t loss: 0.0968 \t train acc: 0.8750\n",
      "epoch: 1 \t batch_idx : 12599 \t loss: 0.0978 \t train acc: 0.8750\n",
      "epoch: 1 \t batch_idx : 12699 \t loss: 0.1195 \t train acc: 0.8438\n",
      "epoch: 1 \t batch_idx : 12799 \t loss: 0.1278 \t train acc: 0.8438\n",
      "epoch: 1 \t batch_idx : 12899 \t loss: 0.1847 \t train acc: 0.7188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 \t batch_idx : 12999 \t loss: 0.0837 \t train acc: 0.9062\n",
      "epoch: 1 \t batch_idx : 13099 \t loss: 0.1134 \t train acc: 0.8438\n",
      "epoch: 1 \t batch_idx : 13199 \t loss: 0.1320 \t train acc: 0.7500\n",
      "epoch: 1 \t batch_idx : 13299 \t loss: 0.0629 \t train acc: 0.9375\n",
      "epoch: 1 \t batch_idx : 13399 \t loss: 0.0921 \t train acc: 0.8438\n",
      "epoch: 1 \t batch_idx : 13499 \t loss: 0.0760 \t train acc: 0.8750\n",
      "epoch: 1 \t batch_idx : 13599 \t loss: 0.1116 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 13699 \t loss: 0.0965 \t train acc: 0.9062\n",
      "epoch: 1 \t batch_idx : 13799 \t loss: 0.1600 \t train acc: 0.7500\n",
      "epoch: 1 \t batch_idx : 13899 \t loss: 0.1832 \t train acc: 0.6875\n",
      "epoch: 1 \t batch_idx : 13999 \t loss: 0.1306 \t train acc: 0.7812\n",
      "epoch: 1 \t batch_idx : 14099 \t loss: 0.0883 \t train acc: 0.8750\n",
      "epoch: 1 \t batch_idx : 14199 \t loss: 0.1539 \t train acc: 0.7188\n",
      "epoch: 1 \t batch_idx : 14299 \t loss: 0.1385 \t train acc: 0.7500\n",
      "epoch: 1 \t batch_idx : 14399 \t loss: 0.0693 \t train acc: 0.8438\n",
      "epoch: 1 \t batch_idx : 14499 \t loss: 0.1041 \t train acc: 0.8750\n",
      "epoch: 1 \t batch_idx : 14599 \t loss: 0.1910 \t train acc: 0.6875\n",
      "epoch: 1 \t batch_idx : 14699 \t loss: 0.1316 \t train acc: 0.7812\n",
      "epoch: 1 \t batch_idx : 14799 \t loss: 0.0749 \t train acc: 0.9375\n",
      "epoch: 1 \t batch_idx : 14899 \t loss: 0.1277 \t train acc: 0.7812\n",
      "epoch: 1 \t batch_idx : 14999 \t loss: 0.1026 \t train acc: 0.8750\n",
      "epoch: 1 \t batch_idx : 15099 \t loss: 0.0362 \t train acc: 0.9688\n",
      "epoch: 1 \t batch_idx : 15199 \t loss: 0.1292 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 15299 \t loss: 0.1017 \t train acc: 0.8438\n",
      "epoch: 1 \t batch_idx : 15399 \t loss: 0.1235 \t train acc: 0.7812\n",
      "epoch: 1 \t batch_idx : 15499 \t loss: 0.2096 \t train acc: 0.6250\n",
      "epoch: 1 \t batch_idx : 15599 \t loss: 0.1023 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 15699 \t loss: 0.0856 \t train acc: 0.8438\n",
      "epoch: 1 \t batch_idx : 15799 \t loss: 0.1227 \t train acc: 0.7500\n",
      "epoch: 1 \t batch_idx : 15899 \t loss: 0.1205 \t train acc: 0.7812\n",
      "epoch: 1 \t batch_idx : 15999 \t loss: 0.0568 \t train acc: 0.9375\n",
      "epoch: 1 \t batch_idx : 16099 \t loss: 0.1045 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 16199 \t loss: 0.1031 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 16299 \t loss: 0.1270 \t train acc: 0.8750\n",
      "epoch: 1 \t batch_idx : 16399 \t loss: 0.0632 \t train acc: 0.9688\n",
      "epoch: 1 \t batch_idx : 16499 \t loss: 0.0849 \t train acc: 0.8750\n",
      "epoch: 1 \t batch_idx : 16599 \t loss: 0.1027 \t train acc: 0.8438\n",
      "epoch: 1 \t batch_idx : 16699 \t loss: 0.1442 \t train acc: 0.8438\n",
      "val acc : 0.8179 > 0.8041 saving model\n",
      "test acc: 0.8179\n",
      "epoch: 2 \t batch_idx : 99 \t loss: 0.1040 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 199 \t loss: 0.1409 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 299 \t loss: 0.1210 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 399 \t loss: 0.1028 \t train acc: 0.8438\n",
      "epoch: 2 \t batch_idx : 499 \t loss: 0.1085 \t train acc: 0.8438\n",
      "epoch: 2 \t batch_idx : 599 \t loss: 0.0762 \t train acc: 0.9375\n",
      "epoch: 2 \t batch_idx : 699 \t loss: 0.0931 \t train acc: 0.8438\n",
      "epoch: 2 \t batch_idx : 799 \t loss: 0.1042 \t train acc: 0.7812\n",
      "epoch: 2 \t batch_idx : 899 \t loss: 0.0944 \t train acc: 0.8750\n",
      "epoch: 2 \t batch_idx : 999 \t loss: 0.0904 \t train acc: 0.8750\n",
      "epoch: 2 \t batch_idx : 1099 \t loss: 0.0990 \t train acc: 0.8750\n",
      "epoch: 2 \t batch_idx : 1199 \t loss: 0.0814 \t train acc: 0.8750\n",
      "epoch: 2 \t batch_idx : 1299 \t loss: 0.1652 \t train acc: 0.7188\n",
      "epoch: 2 \t batch_idx : 1399 \t loss: 0.1371 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 1499 \t loss: 0.0986 \t train acc: 0.8438\n",
      "epoch: 2 \t batch_idx : 1599 \t loss: 0.0840 \t train acc: 0.8750\n",
      "epoch: 2 \t batch_idx : 1699 \t loss: 0.1381 \t train acc: 0.7500\n",
      "epoch: 2 \t batch_idx : 1799 \t loss: 0.1118 \t train acc: 0.7812\n",
      "epoch: 2 \t batch_idx : 1899 \t loss: 0.0822 \t train acc: 0.9062\n",
      "epoch: 2 \t batch_idx : 1999 \t loss: 0.1279 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 2099 \t loss: 0.0741 \t train acc: 0.8750\n",
      "epoch: 2 \t batch_idx : 2199 \t loss: 0.1404 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 2299 \t loss: 0.1143 \t train acc: 0.7812\n",
      "epoch: 2 \t batch_idx : 2399 \t loss: 0.1388 \t train acc: 0.7500\n",
      "epoch: 2 \t batch_idx : 2499 \t loss: 0.0948 \t train acc: 0.8750\n",
      "epoch: 2 \t batch_idx : 2599 \t loss: 0.1112 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 2699 \t loss: 0.0998 \t train acc: 0.7812\n",
      "epoch: 2 \t batch_idx : 2799 \t loss: 0.0525 \t train acc: 0.9062\n",
      "epoch: 2 \t batch_idx : 2899 \t loss: 0.0846 \t train acc: 0.8750\n",
      "epoch: 2 \t batch_idx : 2999 \t loss: 0.1743 \t train acc: 0.7188\n",
      "epoch: 2 \t batch_idx : 3099 \t loss: 0.0675 \t train acc: 0.9062\n",
      "epoch: 2 \t batch_idx : 3199 \t loss: 0.0770 \t train acc: 0.8438\n",
      "epoch: 2 \t batch_idx : 3299 \t loss: 0.1453 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 3399 \t loss: 0.0485 \t train acc: 0.9375\n",
      "epoch: 2 \t batch_idx : 3499 \t loss: 0.1637 \t train acc: 0.6875\n",
      "epoch: 2 \t batch_idx : 3599 \t loss: 0.1013 \t train acc: 0.8750\n",
      "epoch: 2 \t batch_idx : 3699 \t loss: 0.1143 \t train acc: 0.8438\n",
      "epoch: 2 \t batch_idx : 3799 \t loss: 0.0946 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 3899 \t loss: 0.0810 \t train acc: 0.8750\n",
      "epoch: 2 \t batch_idx : 3999 \t loss: 0.0455 \t train acc: 0.9375\n",
      "epoch: 2 \t batch_idx : 4099 \t loss: 0.0718 \t train acc: 0.9062\n",
      "epoch: 2 \t batch_idx : 4199 \t loss: 0.0638 \t train acc: 0.9375\n",
      "epoch: 2 \t batch_idx : 4299 \t loss: 0.1153 \t train acc: 0.7500\n",
      "epoch: 2 \t batch_idx : 4399 \t loss: 0.1597 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 4499 \t loss: 0.0811 \t train acc: 0.9062\n",
      "epoch: 2 \t batch_idx : 4599 \t loss: 0.0807 \t train acc: 0.9062\n",
      "epoch: 2 \t batch_idx : 4699 \t loss: 0.1207 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 4799 \t loss: 0.1122 \t train acc: 0.7812\n",
      "epoch: 2 \t batch_idx : 4899 \t loss: 0.1187 \t train acc: 0.8438\n",
      "epoch: 2 \t batch_idx : 4999 \t loss: 0.0868 \t train acc: 0.8750\n",
      "epoch: 2 \t batch_idx : 5099 \t loss: 0.1241 \t train acc: 0.7812\n",
      "epoch: 2 \t batch_idx : 5199 \t loss: 0.1261 \t train acc: 0.7812\n",
      "epoch: 2 \t batch_idx : 5299 \t loss: 0.0802 \t train acc: 0.9062\n",
      "epoch: 2 \t batch_idx : 5399 \t loss: 0.1745 \t train acc: 0.7188\n",
      "epoch: 2 \t batch_idx : 5499 \t loss: 0.1135 \t train acc: 0.8438\n",
      "epoch: 2 \t batch_idx : 5599 \t loss: 0.1244 \t train acc: 0.7812\n",
      "epoch: 2 \t batch_idx : 5699 \t loss: 0.1035 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 5799 \t loss: 0.1455 \t train acc: 0.7812\n",
      "epoch: 2 \t batch_idx : 5899 \t loss: 0.0986 \t train acc: 0.8750\n",
      "epoch: 2 \t batch_idx : 5999 \t loss: 0.1492 \t train acc: 0.7188\n",
      "epoch: 2 \t batch_idx : 6099 \t loss: 0.1878 \t train acc: 0.6875\n",
      "epoch: 2 \t batch_idx : 6199 \t loss: 0.1107 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 6299 \t loss: 0.1105 \t train acc: 0.8438\n",
      "epoch: 2 \t batch_idx : 6399 \t loss: 0.1444 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 6499 \t loss: 0.1206 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 6599 \t loss: 0.0588 \t train acc: 0.9062\n",
      "epoch: 2 \t batch_idx : 6699 \t loss: 0.1175 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 6799 \t loss: 0.0571 \t train acc: 0.9375\n",
      "epoch: 2 \t batch_idx : 6899 \t loss: 0.0504 \t train acc: 0.9375\n",
      "epoch: 2 \t batch_idx : 6999 \t loss: 0.1558 \t train acc: 0.7812\n",
      "epoch: 2 \t batch_idx : 7099 \t loss: 0.1287 \t train acc: 0.7812\n",
      "epoch: 2 \t batch_idx : 7199 \t loss: 0.0932 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 7299 \t loss: 0.1038 \t train acc: 0.8438\n",
      "epoch: 2 \t batch_idx : 7399 \t loss: 0.1112 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 7499 \t loss: 0.1080 \t train acc: 0.8750\n",
      "epoch: 2 \t batch_idx : 7599 \t loss: 0.1166 \t train acc: 0.7812\n",
      "epoch: 2 \t batch_idx : 7699 \t loss: 0.0619 \t train acc: 0.9062\n",
      "epoch: 2 \t batch_idx : 7799 \t loss: 0.0813 \t train acc: 0.8438\n",
      "epoch: 2 \t batch_idx : 7899 \t loss: 0.1473 \t train acc: 0.7500\n",
      "epoch: 2 \t batch_idx : 7999 \t loss: 0.0877 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 8099 \t loss: 0.1140 \t train acc: 0.7812\n",
      "epoch: 2 \t batch_idx : 8199 \t loss: 0.1240 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 8299 \t loss: 0.1056 \t train acc: 0.8438\n",
      "epoch: 2 \t batch_idx : 8399 \t loss: 0.0809 \t train acc: 0.9062\n",
      "epoch: 2 \t batch_idx : 8499 \t loss: 0.0716 \t train acc: 0.9062\n",
      "epoch: 2 \t batch_idx : 8599 \t loss: 0.0998 \t train acc: 0.8750\n",
      "epoch: 2 \t batch_idx : 8699 \t loss: 0.1208 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 8799 \t loss: 0.1231 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 8899 \t loss: 0.0880 \t train acc: 0.8438\n",
      "epoch: 2 \t batch_idx : 8999 \t loss: 0.1061 \t train acc: 0.8438\n",
      "epoch: 2 \t batch_idx : 9099 \t loss: 0.1401 \t train acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 \t batch_idx : 9199 \t loss: 0.0598 \t train acc: 0.9375\n",
      "epoch: 2 \t batch_idx : 9299 \t loss: 0.0567 \t train acc: 1.0000\n",
      "epoch: 2 \t batch_idx : 9399 \t loss: 0.0925 \t train acc: 0.8438\n",
      "epoch: 2 \t batch_idx : 9499 \t loss: 0.0909 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 9599 \t loss: 0.1554 \t train acc: 0.7500\n",
      "epoch: 2 \t batch_idx : 9699 \t loss: 0.1392 \t train acc: 0.7812\n",
      "epoch: 2 \t batch_idx : 9799 \t loss: 0.0837 \t train acc: 0.8750\n",
      "epoch: 2 \t batch_idx : 9899 \t loss: 0.0863 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 9999 \t loss: 0.1607 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 10099 \t loss: 0.0686 \t train acc: 0.9062\n",
      "epoch: 2 \t batch_idx : 10199 \t loss: 0.1311 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 10299 \t loss: 0.0928 \t train acc: 0.8438\n",
      "epoch: 2 \t batch_idx : 10399 \t loss: 0.0923 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 10499 \t loss: 0.1228 \t train acc: 0.8438\n",
      "epoch: 2 \t batch_idx : 10599 \t loss: 0.1059 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 10699 \t loss: 0.1011 \t train acc: 0.8750\n",
      "epoch: 2 \t batch_idx : 10799 \t loss: 0.0973 \t train acc: 0.8438\n",
      "epoch: 2 \t batch_idx : 10899 \t loss: 0.1731 \t train acc: 0.6562\n",
      "epoch: 2 \t batch_idx : 10999 \t loss: 0.1264 \t train acc: 0.7812\n",
      "epoch: 2 \t batch_idx : 11099 \t loss: 0.0836 \t train acc: 0.9062\n",
      "epoch: 2 \t batch_idx : 11199 \t loss: 0.1803 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 11299 \t loss: 0.1205 \t train acc: 0.7812\n",
      "epoch: 2 \t batch_idx : 11399 \t loss: 0.1565 \t train acc: 0.7500\n",
      "epoch: 2 \t batch_idx : 11499 \t loss: 0.1245 \t train acc: 0.8438\n",
      "epoch: 2 \t batch_idx : 11599 \t loss: 0.0848 \t train acc: 0.9062\n",
      "epoch: 2 \t batch_idx : 11699 \t loss: 0.0721 \t train acc: 0.9062\n",
      "epoch: 2 \t batch_idx : 11799 \t loss: 0.0900 \t train acc: 0.8750\n",
      "epoch: 2 \t batch_idx : 11899 \t loss: 0.1323 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 11999 \t loss: 0.0853 \t train acc: 0.8438\n",
      "epoch: 2 \t batch_idx : 12099 \t loss: 0.0898 \t train acc: 0.9062\n",
      "epoch: 2 \t batch_idx : 12199 \t loss: 0.1623 \t train acc: 0.7188\n",
      "epoch: 2 \t batch_idx : 12299 \t loss: 0.1227 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 12399 \t loss: 0.0836 \t train acc: 0.9062\n",
      "epoch: 2 \t batch_idx : 12499 \t loss: 0.1255 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 12599 \t loss: 0.0618 \t train acc: 0.9062\n",
      "epoch: 2 \t batch_idx : 12699 \t loss: 0.1190 \t train acc: 0.8750\n",
      "epoch: 2 \t batch_idx : 12799 \t loss: 0.1261 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 12899 \t loss: 0.0590 \t train acc: 0.9375\n",
      "epoch: 2 \t batch_idx : 12999 \t loss: 0.1264 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 13099 \t loss: 0.0984 \t train acc: 0.8750\n",
      "epoch: 2 \t batch_idx : 13199 \t loss: 0.0903 \t train acc: 0.8438\n",
      "epoch: 2 \t batch_idx : 13299 \t loss: 0.1820 \t train acc: 0.6875\n",
      "epoch: 2 \t batch_idx : 13399 \t loss: 0.1812 \t train acc: 0.6562\n",
      "epoch: 2 \t batch_idx : 13499 \t loss: 0.0614 \t train acc: 0.9375\n",
      "epoch: 2 \t batch_idx : 13599 \t loss: 0.1561 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 13699 \t loss: 0.1980 \t train acc: 0.5938\n",
      "epoch: 2 \t batch_idx : 13799 \t loss: 0.0987 \t train acc: 0.8438\n",
      "epoch: 2 \t batch_idx : 13899 \t loss: 0.0534 \t train acc: 1.0000\n",
      "epoch: 2 \t batch_idx : 13999 \t loss: 0.1285 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 14099 \t loss: 0.1570 \t train acc: 0.6875\n",
      "epoch: 2 \t batch_idx : 14199 \t loss: 0.1121 \t train acc: 0.7812\n",
      "epoch: 2 \t batch_idx : 14299 \t loss: 0.1089 \t train acc: 0.8438\n",
      "epoch: 2 \t batch_idx : 14399 \t loss: 0.1148 \t train acc: 0.9062\n",
      "epoch: 2 \t batch_idx : 14499 \t loss: 0.0947 \t train acc: 0.9062\n",
      "epoch: 2 \t batch_idx : 14599 \t loss: 0.0628 \t train acc: 0.9062\n",
      "epoch: 2 \t batch_idx : 14699 \t loss: 0.0585 \t train acc: 0.9688\n",
      "epoch: 2 \t batch_idx : 14799 \t loss: 0.0827 \t train acc: 0.8438\n",
      "epoch: 2 \t batch_idx : 14899 \t loss: 0.0661 \t train acc: 0.9062\n",
      "epoch: 2 \t batch_idx : 14999 \t loss: 0.1131 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 15099 \t loss: 0.0836 \t train acc: 0.9375\n",
      "epoch: 2 \t batch_idx : 15199 \t loss: 0.1568 \t train acc: 0.7188\n",
      "epoch: 2 \t batch_idx : 15299 \t loss: 0.1195 \t train acc: 0.8438\n",
      "epoch: 2 \t batch_idx : 15399 \t loss: 0.1271 \t train acc: 0.7188\n",
      "epoch: 2 \t batch_idx : 15499 \t loss: 0.0663 \t train acc: 0.9062\n",
      "epoch: 2 \t batch_idx : 15599 \t loss: 0.1132 \t train acc: 0.8438\n",
      "epoch: 2 \t batch_idx : 15699 \t loss: 0.0747 \t train acc: 0.9062\n",
      "epoch: 2 \t batch_idx : 15799 \t loss: 0.1184 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 15899 \t loss: 0.1145 \t train acc: 0.7812\n",
      "epoch: 2 \t batch_idx : 15999 \t loss: 0.0741 \t train acc: 0.8750\n",
      "epoch: 2 \t batch_idx : 16099 \t loss: 0.1104 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 16199 \t loss: 0.0710 \t train acc: 0.9062\n",
      "epoch: 2 \t batch_idx : 16299 \t loss: 0.0462 \t train acc: 0.9375\n",
      "epoch: 2 \t batch_idx : 16399 \t loss: 0.1281 \t train acc: 0.7500\n",
      "epoch: 2 \t batch_idx : 16499 \t loss: 0.1478 \t train acc: 0.7188\n",
      "epoch: 2 \t batch_idx : 16599 \t loss: 0.0782 \t train acc: 0.9062\n",
      "epoch: 2 \t batch_idx : 16699 \t loss: 0.1048 \t train acc: 0.8750\n",
      "val acc : 0.8201 > 0.8179 saving model\n",
      "test acc: 0.8201\n",
      "epoch: 3 \t batch_idx : 99 \t loss: 0.1313 \t train acc: 0.7500\n",
      "epoch: 3 \t batch_idx : 199 \t loss: 0.0875 \t train acc: 0.8750\n",
      "epoch: 3 \t batch_idx : 299 \t loss: 0.1171 \t train acc: 0.7812\n",
      "epoch: 3 \t batch_idx : 399 \t loss: 0.1324 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 499 \t loss: 0.1204 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 599 \t loss: 0.1028 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 699 \t loss: 0.0963 \t train acc: 0.8750\n",
      "epoch: 3 \t batch_idx : 799 \t loss: 0.0506 \t train acc: 0.9375\n",
      "epoch: 3 \t batch_idx : 899 \t loss: 0.0972 \t train acc: 0.9062\n",
      "epoch: 3 \t batch_idx : 999 \t loss: 0.1178 \t train acc: 0.8750\n",
      "epoch: 3 \t batch_idx : 1099 \t loss: 0.0829 \t train acc: 0.8750\n",
      "epoch: 3 \t batch_idx : 1199 \t loss: 0.0570 \t train acc: 0.8750\n",
      "epoch: 3 \t batch_idx : 1299 \t loss: 0.1536 \t train acc: 0.7500\n",
      "epoch: 3 \t batch_idx : 1399 \t loss: 0.1211 \t train acc: 0.7500\n",
      "epoch: 3 \t batch_idx : 1499 \t loss: 0.0581 \t train acc: 0.9375\n",
      "epoch: 3 \t batch_idx : 1599 \t loss: 0.1990 \t train acc: 0.6875\n",
      "epoch: 3 \t batch_idx : 1699 \t loss: 0.1235 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 1799 \t loss: 0.0582 \t train acc: 0.9062\n",
      "epoch: 3 \t batch_idx : 1899 \t loss: 0.1180 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 1999 \t loss: 0.0678 \t train acc: 0.9062\n",
      "epoch: 3 \t batch_idx : 2099 \t loss: 0.0794 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 2199 \t loss: 0.1338 \t train acc: 0.7812\n",
      "epoch: 3 \t batch_idx : 2299 \t loss: 0.0660 \t train acc: 0.9688\n",
      "epoch: 3 \t batch_idx : 2399 \t loss: 0.0885 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 2499 \t loss: 0.1226 \t train acc: 0.7812\n",
      "epoch: 3 \t batch_idx : 2599 \t loss: 0.1026 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 2699 \t loss: 0.0864 \t train acc: 0.8750\n",
      "epoch: 3 \t batch_idx : 2799 \t loss: 0.0599 \t train acc: 0.8750\n",
      "epoch: 3 \t batch_idx : 2899 \t loss: 0.0870 \t train acc: 0.8750\n",
      "epoch: 3 \t batch_idx : 2999 \t loss: 0.1354 \t train acc: 0.7188\n",
      "epoch: 3 \t batch_idx : 3099 \t loss: 0.0926 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 3199 \t loss: 0.1157 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 3299 \t loss: 0.0769 \t train acc: 0.9062\n",
      "epoch: 3 \t batch_idx : 3399 \t loss: 0.1364 \t train acc: 0.7812\n",
      "epoch: 3 \t batch_idx : 3499 \t loss: 0.1257 \t train acc: 0.7812\n",
      "epoch: 3 \t batch_idx : 3599 \t loss: 0.1101 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 3699 \t loss: 0.1004 \t train acc: 0.8750\n",
      "epoch: 3 \t batch_idx : 3799 \t loss: 0.0909 \t train acc: 0.8750\n",
      "epoch: 3 \t batch_idx : 3899 \t loss: 0.1384 \t train acc: 0.7812\n",
      "epoch: 3 \t batch_idx : 3999 \t loss: 0.1271 \t train acc: 0.7812\n",
      "epoch: 3 \t batch_idx : 4099 \t loss: 0.1222 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 4199 \t loss: 0.1084 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 4299 \t loss: 0.1188 \t train acc: 0.7812\n",
      "epoch: 3 \t batch_idx : 4399 \t loss: 0.1050 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 4499 \t loss: 0.0969 \t train acc: 0.8750\n",
      "epoch: 3 \t batch_idx : 4599 \t loss: 0.0644 \t train acc: 0.9062\n",
      "epoch: 3 \t batch_idx : 4699 \t loss: 0.0582 \t train acc: 0.9688\n",
      "epoch: 3 \t batch_idx : 4799 \t loss: 0.1033 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 4899 \t loss: 0.1662 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 4999 \t loss: 0.1139 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 5099 \t loss: 0.1233 \t train acc: 0.7500\n",
      "epoch: 3 \t batch_idx : 5199 \t loss: 0.0803 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 5299 \t loss: 0.1439 \t train acc: 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 \t batch_idx : 5399 \t loss: 0.1071 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 5499 \t loss: 0.1167 \t train acc: 0.9062\n",
      "epoch: 3 \t batch_idx : 5599 \t loss: 0.1779 \t train acc: 0.6562\n",
      "epoch: 3 \t batch_idx : 5699 \t loss: 0.1169 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 5799 \t loss: 0.1242 \t train acc: 0.7812\n",
      "epoch: 3 \t batch_idx : 5899 \t loss: 0.0848 \t train acc: 0.8750\n",
      "epoch: 3 \t batch_idx : 5999 \t loss: 0.1095 \t train acc: 0.7812\n",
      "epoch: 3 \t batch_idx : 6099 \t loss: 0.0844 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 6199 \t loss: 0.1177 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 6299 \t loss: 0.1619 \t train acc: 0.6875\n",
      "epoch: 3 \t batch_idx : 6399 \t loss: 0.0920 \t train acc: 0.8750\n",
      "epoch: 3 \t batch_idx : 6499 \t loss: 0.1057 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 6599 \t loss: 0.1272 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 6699 \t loss: 0.0628 \t train acc: 0.9062\n",
      "epoch: 3 \t batch_idx : 6799 \t loss: 0.0799 \t train acc: 0.8750\n",
      "epoch: 3 \t batch_idx : 6899 \t loss: 0.0650 \t train acc: 0.9375\n",
      "epoch: 3 \t batch_idx : 6999 \t loss: 0.1084 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 7099 \t loss: 0.0979 \t train acc: 0.8750\n",
      "epoch: 3 \t batch_idx : 7199 \t loss: 0.1057 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 7299 \t loss: 0.1543 \t train acc: 0.7812\n",
      "epoch: 3 \t batch_idx : 7399 \t loss: 0.1143 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 7499 \t loss: 0.0850 \t train acc: 0.8750\n",
      "epoch: 3 \t batch_idx : 7599 \t loss: 0.1384 \t train acc: 0.7812\n",
      "epoch: 3 \t batch_idx : 7699 \t loss: 0.1718 \t train acc: 0.6562\n",
      "epoch: 3 \t batch_idx : 7799 \t loss: 0.1114 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 7899 \t loss: 0.0625 \t train acc: 0.9062\n",
      "epoch: 3 \t batch_idx : 7999 \t loss: 0.0703 \t train acc: 0.9375\n",
      "epoch: 3 \t batch_idx : 8099 \t loss: 0.0872 \t train acc: 0.8750\n",
      "epoch: 3 \t batch_idx : 8199 \t loss: 0.0403 \t train acc: 0.9375\n",
      "epoch: 3 \t batch_idx : 8299 \t loss: 0.0526 \t train acc: 0.9375\n",
      "epoch: 3 \t batch_idx : 8399 \t loss: 0.1179 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 8499 \t loss: 0.0759 \t train acc: 0.9062\n",
      "epoch: 3 \t batch_idx : 8599 \t loss: 0.1133 \t train acc: 0.8750\n",
      "epoch: 3 \t batch_idx : 8699 \t loss: 0.0820 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 8799 \t loss: 0.1051 \t train acc: 0.8750\n",
      "epoch: 3 \t batch_idx : 8899 \t loss: 0.0674 \t train acc: 0.9375\n",
      "epoch: 3 \t batch_idx : 8999 \t loss: 0.1548 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 9099 \t loss: 0.1124 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 9199 \t loss: 0.1051 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 9299 \t loss: 0.1221 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 9399 \t loss: 0.0734 \t train acc: 0.8750\n",
      "epoch: 3 \t batch_idx : 9499 \t loss: 0.1067 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 9599 \t loss: 0.0937 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 9699 \t loss: 0.1695 \t train acc: 0.6562\n",
      "epoch: 3 \t batch_idx : 9799 \t loss: 0.0964 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 9899 \t loss: 0.1322 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 9999 \t loss: 0.0858 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 10099 \t loss: 0.1423 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 10199 \t loss: 0.0575 \t train acc: 0.9688\n",
      "epoch: 3 \t batch_idx : 10299 \t loss: 0.0890 \t train acc: 0.9375\n",
      "epoch: 3 \t batch_idx : 10399 \t loss: 0.0786 \t train acc: 0.9375\n",
      "epoch: 3 \t batch_idx : 10499 \t loss: 0.1812 \t train acc: 0.7812\n",
      "epoch: 3 \t batch_idx : 10599 \t loss: 0.1004 \t train acc: 0.8750\n",
      "epoch: 3 \t batch_idx : 10699 \t loss: 0.0633 \t train acc: 0.9062\n",
      "epoch: 3 \t batch_idx : 10799 \t loss: 0.0741 \t train acc: 0.8750\n",
      "epoch: 3 \t batch_idx : 10899 \t loss: 0.1088 \t train acc: 0.8750\n",
      "epoch: 3 \t batch_idx : 10999 \t loss: 0.0912 \t train acc: 0.8750\n",
      "epoch: 3 \t batch_idx : 11099 \t loss: 0.0464 \t train acc: 0.9062\n",
      "epoch: 3 \t batch_idx : 11199 \t loss: 0.1286 \t train acc: 0.7500\n",
      "epoch: 3 \t batch_idx : 11299 \t loss: 0.1283 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 11399 \t loss: 0.0954 \t train acc: 0.7812\n",
      "epoch: 3 \t batch_idx : 11499 \t loss: 0.1202 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 11599 \t loss: 0.1251 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 11699 \t loss: 0.1129 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 11799 \t loss: 0.0550 \t train acc: 0.9688\n",
      "epoch: 3 \t batch_idx : 11899 \t loss: 0.1533 \t train acc: 0.7500\n",
      "epoch: 3 \t batch_idx : 11999 \t loss: 0.0679 \t train acc: 0.9375\n",
      "epoch: 3 \t batch_idx : 12099 \t loss: 0.1411 \t train acc: 0.7812\n",
      "epoch: 3 \t batch_idx : 12199 \t loss: 0.0760 \t train acc: 0.9062\n",
      "epoch: 3 \t batch_idx : 12299 \t loss: 0.1190 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 12399 \t loss: 0.1202 \t train acc: 0.7812\n",
      "epoch: 3 \t batch_idx : 12499 \t loss: 0.1019 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 12599 \t loss: 0.0697 \t train acc: 0.8750\n",
      "epoch: 3 \t batch_idx : 12699 \t loss: 0.0960 \t train acc: 0.9062\n",
      "epoch: 3 \t batch_idx : 12799 \t loss: 0.0756 \t train acc: 0.9375\n",
      "epoch: 3 \t batch_idx : 12899 \t loss: 0.0713 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 12999 \t loss: 0.1021 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 13099 \t loss: 0.1126 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 13199 \t loss: 0.1417 \t train acc: 0.7500\n",
      "epoch: 3 \t batch_idx : 13299 \t loss: 0.1149 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 13399 \t loss: 0.0567 \t train acc: 0.9375\n",
      "epoch: 3 \t batch_idx : 13499 \t loss: 0.1357 \t train acc: 0.7188\n",
      "epoch: 3 \t batch_idx : 13599 \t loss: 0.0518 \t train acc: 0.9375\n",
      "epoch: 3 \t batch_idx : 13699 \t loss: 0.0709 \t train acc: 0.9375\n",
      "epoch: 3 \t batch_idx : 13799 \t loss: 0.1016 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 13899 \t loss: 0.0997 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 13999 \t loss: 0.0890 \t train acc: 0.9062\n",
      "epoch: 3 \t batch_idx : 14099 \t loss: 0.1017 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 14199 \t loss: 0.0784 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 14299 \t loss: 0.1013 \t train acc: 0.8750\n",
      "epoch: 3 \t batch_idx : 14399 \t loss: 0.1265 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 14499 \t loss: 0.0573 \t train acc: 0.9688\n",
      "epoch: 3 \t batch_idx : 14599 \t loss: 0.1017 \t train acc: 0.7812\n",
      "epoch: 3 \t batch_idx : 14699 \t loss: 0.1480 \t train acc: 0.7500\n",
      "epoch: 3 \t batch_idx : 14799 \t loss: 0.1401 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 14899 \t loss: 0.0998 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 14999 \t loss: 0.0736 \t train acc: 0.9062\n",
      "epoch: 3 \t batch_idx : 15099 \t loss: 0.0975 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 15199 \t loss: 0.0960 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 15299 \t loss: 0.1033 \t train acc: 0.7812\n",
      "epoch: 3 \t batch_idx : 15399 \t loss: 0.0929 \t train acc: 0.8750\n",
      "epoch: 3 \t batch_idx : 15499 \t loss: 0.0682 \t train acc: 0.8750\n",
      "epoch: 3 \t batch_idx : 15599 \t loss: 0.0935 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 15699 \t loss: 0.0818 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 15799 \t loss: 0.0833 \t train acc: 0.9062\n",
      "epoch: 3 \t batch_idx : 15899 \t loss: 0.1343 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 15999 \t loss: 0.0861 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 16099 \t loss: 0.1472 \t train acc: 0.7812\n",
      "epoch: 3 \t batch_idx : 16199 \t loss: 0.1309 \t train acc: 0.7500\n",
      "epoch: 3 \t batch_idx : 16299 \t loss: 0.0951 \t train acc: 0.8750\n",
      "epoch: 3 \t batch_idx : 16399 \t loss: 0.0543 \t train acc: 0.9062\n",
      "epoch: 3 \t batch_idx : 16499 \t loss: 0.1303 \t train acc: 0.7500\n",
      "epoch: 3 \t batch_idx : 16599 \t loss: 0.1100 \t train acc: 0.8750\n",
      "epoch: 3 \t batch_idx : 16699 \t loss: 0.0436 \t train acc: 1.0000\n",
      "test acc: 0.8188\n",
      "epoch: 4 \t batch_idx : 99 \t loss: 0.1144 \t train acc: 0.7812\n",
      "epoch: 4 \t batch_idx : 199 \t loss: 0.1164 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 299 \t loss: 0.1250 \t train acc: 0.7812\n",
      "epoch: 4 \t batch_idx : 399 \t loss: 0.0996 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 499 \t loss: 0.0600 \t train acc: 0.9375\n",
      "epoch: 4 \t batch_idx : 599 \t loss: 0.1166 \t train acc: 0.7812\n",
      "epoch: 4 \t batch_idx : 699 \t loss: 0.1091 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 799 \t loss: 0.0988 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 899 \t loss: 0.0697 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 999 \t loss: 0.1159 \t train acc: 0.7812\n",
      "epoch: 4 \t batch_idx : 1099 \t loss: 0.1194 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 1199 \t loss: 0.0579 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 1299 \t loss: 0.0919 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 1399 \t loss: 0.0643 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 1499 \t loss: 0.0711 \t train acc: 0.8750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 \t batch_idx : 1599 \t loss: 0.0837 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 1699 \t loss: 0.0911 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 1799 \t loss: 0.0924 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 1899 \t loss: 0.0859 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 1999 \t loss: 0.0647 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 2099 \t loss: 0.0713 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 2199 \t loss: 0.0662 \t train acc: 0.9688\n",
      "epoch: 4 \t batch_idx : 2299 \t loss: 0.0960 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 2399 \t loss: 0.0985 \t train acc: 0.8125\n",
      "epoch: 4 \t batch_idx : 2499 \t loss: 0.0812 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 2599 \t loss: 0.0653 \t train acc: 0.9375\n",
      "epoch: 4 \t batch_idx : 2699 \t loss: 0.0605 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 2799 \t loss: 0.0825 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 2899 \t loss: 0.1073 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 2999 \t loss: 0.1509 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 3099 \t loss: 0.1253 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 3199 \t loss: 0.1087 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 3299 \t loss: 0.1400 \t train acc: 0.8125\n",
      "epoch: 4 \t batch_idx : 3399 \t loss: 0.0936 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 3499 \t loss: 0.1189 \t train acc: 0.7812\n",
      "epoch: 4 \t batch_idx : 3599 \t loss: 0.1060 \t train acc: 0.8125\n",
      "epoch: 4 \t batch_idx : 3699 \t loss: 0.0600 \t train acc: 0.9375\n",
      "epoch: 4 \t batch_idx : 3799 \t loss: 0.0743 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 3899 \t loss: 0.1398 \t train acc: 0.7188\n",
      "epoch: 4 \t batch_idx : 3999 \t loss: 0.0913 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 4099 \t loss: 0.0705 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 4199 \t loss: 0.0829 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 4299 \t loss: 0.0606 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 4399 \t loss: 0.0887 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 4499 \t loss: 0.0761 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 4599 \t loss: 0.0531 \t train acc: 0.9375\n",
      "epoch: 4 \t batch_idx : 4699 \t loss: 0.1247 \t train acc: 0.8125\n",
      "epoch: 4 \t batch_idx : 4799 \t loss: 0.0799 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 4899 \t loss: 0.0813 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 4999 \t loss: 0.0944 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 5099 \t loss: 0.1057 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 5199 \t loss: 0.0786 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 5299 \t loss: 0.0428 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 5399 \t loss: 0.0641 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 5499 \t loss: 0.0781 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 5599 \t loss: 0.0943 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 5699 \t loss: 0.1097 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 5799 \t loss: 0.0932 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 5899 \t loss: 0.1484 \t train acc: 0.6875\n",
      "epoch: 4 \t batch_idx : 5999 \t loss: 0.0687 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 6099 \t loss: 0.0876 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 6199 \t loss: 0.0919 \t train acc: 0.8125\n",
      "epoch: 4 \t batch_idx : 6299 \t loss: 0.1068 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 6399 \t loss: 0.1616 \t train acc: 0.7188\n",
      "epoch: 4 \t batch_idx : 6499 \t loss: 0.1097 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 6599 \t loss: 0.0618 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 6699 \t loss: 0.1208 \t train acc: 0.8125\n",
      "epoch: 4 \t batch_idx : 6799 \t loss: 0.0956 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 6899 \t loss: 0.1115 \t train acc: 0.8125\n",
      "epoch: 4 \t batch_idx : 6999 \t loss: 0.0452 \t train acc: 0.9688\n",
      "epoch: 4 \t batch_idx : 7099 \t loss: 0.1217 \t train acc: 0.8125\n",
      "epoch: 4 \t batch_idx : 7199 \t loss: 0.0672 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 7299 \t loss: 0.0592 \t train acc: 0.9375\n",
      "epoch: 4 \t batch_idx : 7399 \t loss: 0.0797 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 7499 \t loss: 0.1449 \t train acc: 0.7188\n",
      "epoch: 4 \t batch_idx : 7599 \t loss: 0.1091 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 7699 \t loss: 0.0384 \t train acc: 0.9688\n",
      "epoch: 4 \t batch_idx : 7799 \t loss: 0.0728 \t train acc: 0.9375\n",
      "epoch: 4 \t batch_idx : 7899 \t loss: 0.0698 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 7999 \t loss: 0.0972 \t train acc: 0.8125\n",
      "epoch: 4 \t batch_idx : 8099 \t loss: 0.0808 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 8199 \t loss: 0.0747 \t train acc: 0.9375\n",
      "epoch: 4 \t batch_idx : 8299 \t loss: 0.0815 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 8399 \t loss: 0.0989 \t train acc: 0.8125\n",
      "epoch: 4 \t batch_idx : 8499 \t loss: 0.0464 \t train acc: 0.9375\n",
      "epoch: 4 \t batch_idx : 8599 \t loss: 0.0893 \t train acc: 0.8125\n",
      "epoch: 4 \t batch_idx : 8699 \t loss: 0.1501 \t train acc: 0.7812\n",
      "epoch: 4 \t batch_idx : 8799 \t loss: 0.0396 \t train acc: 0.9375\n",
      "epoch: 4 \t batch_idx : 8899 \t loss: 0.0874 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 8999 \t loss: 0.1278 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 9099 \t loss: 0.1136 \t train acc: 0.8125\n",
      "epoch: 4 \t batch_idx : 9199 \t loss: 0.0676 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 9299 \t loss: 0.0878 \t train acc: 0.8125\n",
      "epoch: 4 \t batch_idx : 9399 \t loss: 0.1238 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 9499 \t loss: 0.1079 \t train acc: 0.7812\n",
      "epoch: 4 \t batch_idx : 9599 \t loss: 0.1619 \t train acc: 0.7188\n",
      "epoch: 4 \t batch_idx : 9699 \t loss: 0.1234 \t train acc: 0.8125\n",
      "epoch: 4 \t batch_idx : 9799 \t loss: 0.0851 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 9899 \t loss: 0.0872 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 9999 \t loss: 0.1159 \t train acc: 0.8125\n",
      "epoch: 4 \t batch_idx : 10099 \t loss: 0.1609 \t train acc: 0.7188\n",
      "epoch: 4 \t batch_idx : 10199 \t loss: 0.0546 \t train acc: 0.9375\n",
      "epoch: 4 \t batch_idx : 10299 \t loss: 0.1479 \t train acc: 0.7188\n",
      "epoch: 4 \t batch_idx : 10399 \t loss: 0.1178 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 10499 \t loss: 0.1159 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 10599 \t loss: 0.0842 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 10699 \t loss: 0.1132 \t train acc: 0.7812\n",
      "epoch: 4 \t batch_idx : 10799 \t loss: 0.0682 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 10899 \t loss: 0.1050 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 10999 \t loss: 0.0670 \t train acc: 0.9375\n",
      "epoch: 4 \t batch_idx : 11099 \t loss: 0.1080 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 11199 \t loss: 0.1137 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 11299 \t loss: 0.0949 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 11399 \t loss: 0.0665 \t train acc: 0.9375\n",
      "epoch: 4 \t batch_idx : 11499 \t loss: 0.1241 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 11599 \t loss: 0.0852 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 11699 \t loss: 0.0546 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 11799 \t loss: 0.0610 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 11899 \t loss: 0.0741 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 11999 \t loss: 0.1106 \t train acc: 0.8125\n",
      "epoch: 4 \t batch_idx : 12099 \t loss: 0.0916 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 12199 \t loss: 0.1039 \t train acc: 0.7812\n",
      "epoch: 4 \t batch_idx : 12299 \t loss: 0.1406 \t train acc: 0.7812\n",
      "epoch: 4 \t batch_idx : 12399 \t loss: 0.0869 \t train acc: 0.8125\n",
      "epoch: 4 \t batch_idx : 12499 \t loss: 0.1256 \t train acc: 0.7500\n",
      "epoch: 4 \t batch_idx : 12599 \t loss: 0.0382 \t train acc: 0.9688\n",
      "epoch: 4 \t batch_idx : 12699 \t loss: 0.0955 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 12799 \t loss: 0.1304 \t train acc: 0.8125\n",
      "epoch: 4 \t batch_idx : 12899 \t loss: 0.1625 \t train acc: 0.7500\n",
      "epoch: 4 \t batch_idx : 12999 \t loss: 0.1224 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 13099 \t loss: 0.1311 \t train acc: 0.7188\n",
      "epoch: 4 \t batch_idx : 13199 \t loss: 0.0416 \t train acc: 1.0000\n",
      "epoch: 4 \t batch_idx : 13299 \t loss: 0.1596 \t train acc: 0.6875\n",
      "epoch: 4 \t batch_idx : 13399 \t loss: 0.0653 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 13499 \t loss: 0.0902 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 13599 \t loss: 0.1584 \t train acc: 0.7500\n",
      "epoch: 4 \t batch_idx : 13699 \t loss: 0.0597 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 13799 \t loss: 0.1091 \t train acc: 0.7812\n",
      "epoch: 4 \t batch_idx : 13899 \t loss: 0.0832 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 13999 \t loss: 0.0876 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 14099 \t loss: 0.0544 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 14199 \t loss: 0.0619 \t train acc: 0.9375\n",
      "epoch: 4 \t batch_idx : 14299 \t loss: 0.0981 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 14399 \t loss: 0.0525 \t train acc: 0.9375\n",
      "epoch: 4 \t batch_idx : 14499 \t loss: 0.0694 \t train acc: 0.8750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 \t batch_idx : 14599 \t loss: 0.0615 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 14699 \t loss: 0.0736 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 14799 \t loss: 0.0424 \t train acc: 0.9375\n",
      "epoch: 4 \t batch_idx : 14899 \t loss: 0.0627 \t train acc: 0.9375\n",
      "epoch: 4 \t batch_idx : 14999 \t loss: 0.0435 \t train acc: 0.9688\n",
      "epoch: 4 \t batch_idx : 15099 \t loss: 0.1010 \t train acc: 0.7812\n",
      "epoch: 4 \t batch_idx : 15199 \t loss: 0.1243 \t train acc: 0.8125\n",
      "epoch: 4 \t batch_idx : 15299 \t loss: 0.1612 \t train acc: 0.7500\n",
      "epoch: 4 \t batch_idx : 15399 \t loss: 0.1116 \t train acc: 0.7812\n",
      "epoch: 4 \t batch_idx : 15499 \t loss: 0.1055 \t train acc: 0.7812\n",
      "epoch: 4 \t batch_idx : 15599 \t loss: 0.0928 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 15699 \t loss: 0.1167 \t train acc: 0.8125\n",
      "epoch: 4 \t batch_idx : 15799 \t loss: 0.1278 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 15899 \t loss: 0.0795 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 15999 \t loss: 0.0820 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 16099 \t loss: 0.0914 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 16199 \t loss: 0.0954 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 16299 \t loss: 0.1720 \t train acc: 0.6875\n",
      "epoch: 4 \t batch_idx : 16399 \t loss: 0.0643 \t train acc: 0.9375\n",
      "epoch: 4 \t batch_idx : 16499 \t loss: 0.0896 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 16599 \t loss: 0.0897 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 16699 \t loss: 0.0553 \t train acc: 0.9062\n",
      "test acc: 0.8171\n",
      "epoch: 5 \t batch_idx : 99 \t loss: 0.1213 \t train acc: 0.8438\n",
      "epoch: 5 \t batch_idx : 199 \t loss: 0.0567 \t train acc: 0.9375\n",
      "epoch: 5 \t batch_idx : 299 \t loss: 0.0922 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 399 \t loss: 0.0506 \t train acc: 0.9688\n",
      "epoch: 5 \t batch_idx : 499 \t loss: 0.0773 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 599 \t loss: 0.0902 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 699 \t loss: 0.1240 \t train acc: 0.8125\n",
      "epoch: 5 \t batch_idx : 799 \t loss: 0.1699 \t train acc: 0.7188\n",
      "epoch: 5 \t batch_idx : 899 \t loss: 0.0786 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 999 \t loss: 0.0820 \t train acc: 0.9688\n",
      "epoch: 5 \t batch_idx : 1099 \t loss: 0.0841 \t train acc: 0.8438\n",
      "epoch: 5 \t batch_idx : 1199 \t loss: 0.1084 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 1299 \t loss: 0.0484 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 1399 \t loss: 0.0961 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 1499 \t loss: 0.0519 \t train acc: 0.9688\n",
      "epoch: 5 \t batch_idx : 1599 \t loss: 0.0645 \t train acc: 0.9375\n",
      "epoch: 5 \t batch_idx : 1699 \t loss: 0.0585 \t train acc: 0.9375\n",
      "epoch: 5 \t batch_idx : 1799 \t loss: 0.0888 \t train acc: 0.8125\n",
      "epoch: 5 \t batch_idx : 1899 \t loss: 0.0939 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 1999 \t loss: 0.0525 \t train acc: 0.9375\n",
      "epoch: 5 \t batch_idx : 2099 \t loss: 0.0627 \t train acc: 0.9375\n",
      "epoch: 5 \t batch_idx : 2199 \t loss: 0.1030 \t train acc: 0.8438\n",
      "epoch: 5 \t batch_idx : 2299 \t loss: 0.0738 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 2399 \t loss: 0.0774 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 2499 \t loss: 0.0728 \t train acc: 0.9375\n",
      "epoch: 5 \t batch_idx : 2599 \t loss: 0.1030 \t train acc: 0.8125\n",
      "epoch: 5 \t batch_idx : 2699 \t loss: 0.0540 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 2799 \t loss: 0.0605 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 2899 \t loss: 0.0686 \t train acc: 0.9375\n",
      "epoch: 5 \t batch_idx : 2999 \t loss: 0.1230 \t train acc: 0.8438\n",
      "epoch: 5 \t batch_idx : 3099 \t loss: 0.0707 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 3199 \t loss: 0.1311 \t train acc: 0.7188\n",
      "epoch: 5 \t batch_idx : 3299 \t loss: 0.0878 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 3399 \t loss: 0.0987 \t train acc: 0.8438\n",
      "epoch: 5 \t batch_idx : 3499 \t loss: 0.0776 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 3599 \t loss: 0.0507 \t train acc: 0.9375\n",
      "epoch: 5 \t batch_idx : 3699 \t loss: 0.0379 \t train acc: 0.9688\n",
      "epoch: 5 \t batch_idx : 3799 \t loss: 0.1052 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 3899 \t loss: 0.0679 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 3999 \t loss: 0.0695 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 4099 \t loss: 0.0546 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 4199 \t loss: 0.1106 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 4299 \t loss: 0.0397 \t train acc: 0.9375\n",
      "epoch: 5 \t batch_idx : 4399 \t loss: 0.1126 \t train acc: 0.7812\n",
      "epoch: 5 \t batch_idx : 4499 \t loss: 0.0819 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 4599 \t loss: 0.0464 \t train acc: 0.9375\n",
      "epoch: 5 \t batch_idx : 4699 \t loss: 0.0722 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 4799 \t loss: 0.0965 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 4899 \t loss: 0.0254 \t train acc: 0.9688\n",
      "epoch: 5 \t batch_idx : 4999 \t loss: 0.0658 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 5099 \t loss: 0.0333 \t train acc: 0.9375\n",
      "epoch: 5 \t batch_idx : 5199 \t loss: 0.1260 \t train acc: 0.8438\n",
      "epoch: 5 \t batch_idx : 5299 \t loss: 0.1244 \t train acc: 0.8125\n",
      "epoch: 5 \t batch_idx : 5399 \t loss: 0.0696 \t train acc: 0.9375\n",
      "epoch: 5 \t batch_idx : 5499 \t loss: 0.0355 \t train acc: 0.9375\n",
      "epoch: 5 \t batch_idx : 5599 \t loss: 0.1005 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 5699 \t loss: 0.1410 \t train acc: 0.7500\n",
      "epoch: 5 \t batch_idx : 5799 \t loss: 0.0165 \t train acc: 1.0000\n",
      "epoch: 5 \t batch_idx : 5899 \t loss: 0.1012 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 5999 \t loss: 0.0884 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 6099 \t loss: 0.0935 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 6199 \t loss: 0.0891 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 6299 \t loss: 0.1211 \t train acc: 0.8438\n",
      "epoch: 5 \t batch_idx : 6399 \t loss: 0.0836 \t train acc: 0.8438\n",
      "epoch: 5 \t batch_idx : 6499 \t loss: 0.0690 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 6599 \t loss: 0.0689 \t train acc: 0.9375\n",
      "epoch: 5 \t batch_idx : 6699 \t loss: 0.0835 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 6799 \t loss: 0.0828 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 6899 \t loss: 0.0717 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 6999 \t loss: 0.0418 \t train acc: 1.0000\n",
      "epoch: 5 \t batch_idx : 7099 \t loss: 0.0906 \t train acc: 0.8125\n",
      "epoch: 5 \t batch_idx : 7199 \t loss: 0.0742 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 7299 \t loss: 0.0920 \t train acc: 0.8438\n",
      "epoch: 5 \t batch_idx : 7399 \t loss: 0.0474 \t train acc: 0.9688\n",
      "epoch: 5 \t batch_idx : 7499 \t loss: 0.0357 \t train acc: 0.9375\n",
      "epoch: 5 \t batch_idx : 7599 \t loss: 0.0975 \t train acc: 0.8438\n",
      "epoch: 5 \t batch_idx : 7699 \t loss: 0.0578 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 7799 \t loss: 0.0727 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 7899 \t loss: 0.0333 \t train acc: 0.9688\n",
      "epoch: 5 \t batch_idx : 7999 \t loss: 0.0908 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 8099 \t loss: 0.0976 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 8199 \t loss: 0.0844 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 8299 \t loss: 0.0694 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 8399 \t loss: 0.0752 \t train acc: 0.8125\n",
      "epoch: 5 \t batch_idx : 8499 \t loss: 0.0700 \t train acc: 0.9375\n",
      "epoch: 5 \t batch_idx : 8599 \t loss: 0.0911 \t train acc: 0.8438\n",
      "epoch: 5 \t batch_idx : 8699 \t loss: 0.0611 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 8799 \t loss: 0.0390 \t train acc: 0.9688\n",
      "epoch: 5 \t batch_idx : 8899 \t loss: 0.0708 \t train acc: 0.9688\n",
      "epoch: 5 \t batch_idx : 8999 \t loss: 0.0645 \t train acc: 0.9375\n",
      "epoch: 5 \t batch_idx : 9099 \t loss: 0.0631 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 9199 \t loss: 0.0755 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 9299 \t loss: 0.0702 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 9399 \t loss: 0.0922 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 9499 \t loss: 0.0940 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 9599 \t loss: 0.0471 \t train acc: 0.9375\n",
      "epoch: 5 \t batch_idx : 9699 \t loss: 0.0446 \t train acc: 0.9375\n",
      "epoch: 5 \t batch_idx : 9799 \t loss: 0.0720 \t train acc: 0.8438\n",
      "epoch: 5 \t batch_idx : 9899 \t loss: 0.1366 \t train acc: 0.7500\n",
      "epoch: 5 \t batch_idx : 9999 \t loss: 0.0594 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 10099 \t loss: 0.0457 \t train acc: 0.9375\n",
      "epoch: 5 \t batch_idx : 10199 \t loss: 0.0698 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 10299 \t loss: 0.0513 \t train acc: 0.9375\n",
      "epoch: 5 \t batch_idx : 10399 \t loss: 0.0724 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 10499 \t loss: 0.0498 \t train acc: 0.9375\n",
      "epoch: 5 \t batch_idx : 10599 \t loss: 0.0770 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 10699 \t loss: 0.0635 \t train acc: 0.9375\n",
      "epoch: 5 \t batch_idx : 10799 \t loss: 0.1345 \t train acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 \t batch_idx : 10899 \t loss: 0.1264 \t train acc: 0.8438\n",
      "epoch: 5 \t batch_idx : 10999 \t loss: 0.1193 \t train acc: 0.8438\n",
      "epoch: 5 \t batch_idx : 11099 \t loss: 0.0726 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 11199 \t loss: 0.0321 \t train acc: 0.9375\n",
      "epoch: 5 \t batch_idx : 11299 \t loss: 0.0950 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 11399 \t loss: 0.1213 \t train acc: 0.8125\n",
      "epoch: 5 \t batch_idx : 11499 \t loss: 0.0950 \t train acc: 0.8438\n",
      "epoch: 5 \t batch_idx : 11599 \t loss: 0.0744 \t train acc: 0.9375\n",
      "epoch: 5 \t batch_idx : 11699 \t loss: 0.0362 \t train acc: 0.9688\n",
      "epoch: 5 \t batch_idx : 11799 \t loss: 0.0689 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 11899 \t loss: 0.1113 \t train acc: 0.8438\n",
      "epoch: 5 \t batch_idx : 11999 \t loss: 0.0985 \t train acc: 0.8125\n",
      "epoch: 5 \t batch_idx : 12099 \t loss: 0.1422 \t train acc: 0.7500\n",
      "epoch: 5 \t batch_idx : 12199 \t loss: 0.0555 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 12299 \t loss: 0.0693 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 12399 \t loss: 0.1247 \t train acc: 0.7812\n",
      "epoch: 5 \t batch_idx : 12499 \t loss: 0.1039 \t train acc: 0.8438\n",
      "epoch: 5 \t batch_idx : 12599 \t loss: 0.0713 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 12699 \t loss: 0.1283 \t train acc: 0.7812\n",
      "epoch: 5 \t batch_idx : 12799 \t loss: 0.0397 \t train acc: 0.9688\n",
      "epoch: 5 \t batch_idx : 12899 \t loss: 0.0946 \t train acc: 0.8438\n",
      "epoch: 5 \t batch_idx : 12999 \t loss: 0.0524 \t train acc: 0.9688\n",
      "epoch: 5 \t batch_idx : 13099 \t loss: 0.0678 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 13199 \t loss: 0.0803 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 13299 \t loss: 0.1529 \t train acc: 0.7500\n",
      "epoch: 5 \t batch_idx : 13399 \t loss: 0.0611 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 13499 \t loss: 0.0189 \t train acc: 1.0000\n",
      "epoch: 5 \t batch_idx : 13599 \t loss: 0.0945 \t train acc: 0.8438\n",
      "epoch: 5 \t batch_idx : 13699 \t loss: 0.0805 \t train acc: 0.8438\n",
      "epoch: 5 \t batch_idx : 13799 \t loss: 0.0652 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 13899 \t loss: 0.0529 \t train acc: 0.9688\n",
      "epoch: 5 \t batch_idx : 13999 \t loss: 0.0586 \t train acc: 0.9375\n",
      "epoch: 5 \t batch_idx : 14099 \t loss: 0.0543 \t train acc: 0.9688\n",
      "epoch: 5 \t batch_idx : 14199 \t loss: 0.0523 \t train acc: 0.9688\n",
      "epoch: 5 \t batch_idx : 14299 \t loss: 0.0480 \t train acc: 0.9688\n",
      "epoch: 5 \t batch_idx : 14399 \t loss: 0.0909 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 14499 \t loss: 0.0415 \t train acc: 0.9688\n",
      "epoch: 5 \t batch_idx : 14599 \t loss: 0.0299 \t train acc: 1.0000\n",
      "epoch: 5 \t batch_idx : 14699 \t loss: 0.0264 \t train acc: 1.0000\n",
      "epoch: 5 \t batch_idx : 14799 \t loss: 0.0965 \t train acc: 0.8438\n",
      "epoch: 5 \t batch_idx : 14899 \t loss: 0.0447 \t train acc: 0.9375\n",
      "epoch: 5 \t batch_idx : 14999 \t loss: 0.0616 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 15099 \t loss: 0.0456 \t train acc: 1.0000\n",
      "epoch: 5 \t batch_idx : 15199 \t loss: 0.0450 \t train acc: 0.9375\n",
      "epoch: 5 \t batch_idx : 15299 \t loss: 0.1333 \t train acc: 0.8438\n",
      "epoch: 5 \t batch_idx : 15399 \t loss: 0.0634 \t train acc: 0.9375\n",
      "epoch: 5 \t batch_idx : 15499 \t loss: 0.1422 \t train acc: 0.7500\n",
      "epoch: 5 \t batch_idx : 15599 \t loss: 0.0252 \t train acc: 1.0000\n",
      "epoch: 5 \t batch_idx : 15699 \t loss: 0.0674 \t train acc: 0.9375\n",
      "epoch: 5 \t batch_idx : 15799 \t loss: 0.1311 \t train acc: 0.8438\n",
      "epoch: 5 \t batch_idx : 15899 \t loss: 0.0336 \t train acc: 1.0000\n",
      "epoch: 5 \t batch_idx : 15999 \t loss: 0.1439 \t train acc: 0.7812\n",
      "epoch: 5 \t batch_idx : 16099 \t loss: 0.0608 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 16199 \t loss: 0.0279 \t train acc: 1.0000\n",
      "epoch: 5 \t batch_idx : 16299 \t loss: 0.0643 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 16399 \t loss: 0.0490 \t train acc: 0.9375\n",
      "epoch: 5 \t batch_idx : 16499 \t loss: 0.1236 \t train acc: 0.8125\n",
      "epoch: 5 \t batch_idx : 16599 \t loss: 0.0716 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 16699 \t loss: 0.0418 \t train acc: 0.9688\n",
      "test acc: 0.8140\n",
      "epoch: 6 \t batch_idx : 99 \t loss: 0.0985 \t train acc: 0.8438\n",
      "epoch: 6 \t batch_idx : 199 \t loss: 0.0443 \t train acc: 0.9688\n",
      "epoch: 6 \t batch_idx : 299 \t loss: 0.0977 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 399 \t loss: 0.0941 \t train acc: 0.8438\n",
      "epoch: 6 \t batch_idx : 499 \t loss: 0.0483 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 599 \t loss: 0.0758 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 699 \t loss: 0.0650 \t train acc: 0.9688\n",
      "epoch: 6 \t batch_idx : 799 \t loss: 0.0566 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 899 \t loss: 0.0654 \t train acc: 0.9688\n",
      "epoch: 6 \t batch_idx : 999 \t loss: 0.0703 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 1099 \t loss: 0.0506 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 1199 \t loss: 0.0319 \t train acc: 0.9688\n",
      "epoch: 6 \t batch_idx : 1299 \t loss: 0.0616 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 1399 \t loss: 0.0702 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 1499 \t loss: 0.0279 \t train acc: 0.9688\n",
      "epoch: 6 \t batch_idx : 1599 \t loss: 0.0503 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 1699 \t loss: 0.0559 \t train acc: 0.9688\n",
      "epoch: 6 \t batch_idx : 1799 \t loss: 0.0448 \t train acc: 0.9688\n",
      "epoch: 6 \t batch_idx : 1899 \t loss: 0.0406 \t train acc: 0.9688\n",
      "epoch: 6 \t batch_idx : 1999 \t loss: 0.0721 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 2099 \t loss: 0.0230 \t train acc: 1.0000\n",
      "epoch: 6 \t batch_idx : 2199 \t loss: 0.0484 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 2299 \t loss: 0.0521 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 2399 \t loss: 0.0520 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 2499 \t loss: 0.0336 \t train acc: 0.9688\n",
      "epoch: 6 \t batch_idx : 2599 \t loss: 0.0501 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 2699 \t loss: 0.0326 \t train acc: 0.9688\n",
      "epoch: 6 \t batch_idx : 2799 \t loss: 0.0695 \t train acc: 0.8438\n",
      "epoch: 6 \t batch_idx : 2899 \t loss: 0.0798 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 2999 \t loss: 0.0881 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 3099 \t loss: 0.0954 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 3199 \t loss: 0.0758 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 3299 \t loss: 0.0591 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 3399 \t loss: 0.0421 \t train acc: 0.9688\n",
      "epoch: 6 \t batch_idx : 3499 \t loss: 0.0703 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 3599 \t loss: 0.0506 \t train acc: 0.9688\n",
      "epoch: 6 \t batch_idx : 3699 \t loss: 0.0863 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 3799 \t loss: 0.0327 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 3899 \t loss: 0.0441 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 3999 \t loss: 0.0789 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 4099 \t loss: 0.0541 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 4199 \t loss: 0.0666 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 4299 \t loss: 0.0726 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 4399 \t loss: 0.0724 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 4499 \t loss: 0.0661 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 4599 \t loss: 0.0195 \t train acc: 1.0000\n",
      "epoch: 6 \t batch_idx : 4699 \t loss: 0.0847 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 4799 \t loss: 0.0586 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 4899 \t loss: 0.0819 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 4999 \t loss: 0.0743 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 5099 \t loss: 0.0564 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 5199 \t loss: 0.1018 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 5299 \t loss: 0.0293 \t train acc: 1.0000\n",
      "epoch: 6 \t batch_idx : 5399 \t loss: 0.1395 \t train acc: 0.8125\n",
      "epoch: 6 \t batch_idx : 5499 \t loss: 0.1248 \t train acc: 0.8438\n",
      "epoch: 6 \t batch_idx : 5599 \t loss: 0.0641 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 5699 \t loss: 0.0707 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 5799 \t loss: 0.0483 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 5899 \t loss: 0.0945 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 5999 \t loss: 0.0422 \t train acc: 0.9688\n",
      "epoch: 6 \t batch_idx : 6099 \t loss: 0.0534 \t train acc: 0.9688\n",
      "epoch: 6 \t batch_idx : 6199 \t loss: 0.0555 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 6299 \t loss: 0.0483 \t train acc: 0.9688\n",
      "epoch: 6 \t batch_idx : 6399 \t loss: 0.0217 \t train acc: 0.9688\n",
      "epoch: 6 \t batch_idx : 6499 \t loss: 0.0939 \t train acc: 0.8125\n",
      "epoch: 6 \t batch_idx : 6599 \t loss: 0.0985 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 6699 \t loss: 0.0900 \t train acc: 0.8438\n",
      "epoch: 6 \t batch_idx : 6799 \t loss: 0.0531 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 6899 \t loss: 0.0619 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 6999 \t loss: 0.0788 \t train acc: 0.8125\n",
      "epoch: 6 \t batch_idx : 7099 \t loss: 0.0204 \t train acc: 0.9688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 \t batch_idx : 7199 \t loss: 0.0865 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 7299 \t loss: 0.0394 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 7399 \t loss: 0.0819 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 7499 \t loss: 0.0917 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 7599 \t loss: 0.0558 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 7699 \t loss: 0.0515 \t train acc: 0.9688\n",
      "epoch: 6 \t batch_idx : 7799 \t loss: 0.0635 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 7899 \t loss: 0.0827 \t train acc: 0.8125\n",
      "epoch: 6 \t batch_idx : 7999 \t loss: 0.0357 \t train acc: 0.9688\n",
      "epoch: 6 \t batch_idx : 8099 \t loss: 0.0639 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 8199 \t loss: 0.0972 \t train acc: 0.7812\n",
      "epoch: 6 \t batch_idx : 8299 \t loss: 0.0261 \t train acc: 1.0000\n",
      "epoch: 6 \t batch_idx : 8399 \t loss: 0.0417 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 8499 \t loss: 0.0285 \t train acc: 0.9688\n",
      "epoch: 6 \t batch_idx : 8599 \t loss: 0.1042 \t train acc: 0.8438\n",
      "epoch: 6 \t batch_idx : 8699 \t loss: 0.0940 \t train acc: 0.8438\n",
      "epoch: 6 \t batch_idx : 8799 \t loss: 0.0691 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 8899 \t loss: 0.0387 \t train acc: 0.9688\n",
      "epoch: 6 \t batch_idx : 8999 \t loss: 0.0806 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 9099 \t loss: 0.0695 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 9199 \t loss: 0.0524 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 9299 \t loss: 0.0687 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 9399 \t loss: 0.0827 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 9499 \t loss: 0.0298 \t train acc: 0.9688\n",
      "epoch: 6 \t batch_idx : 9599 \t loss: 0.0413 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 9699 \t loss: 0.0635 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 9799 \t loss: 0.0705 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 9899 \t loss: 0.0454 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 9999 \t loss: 0.1109 \t train acc: 0.8125\n",
      "epoch: 6 \t batch_idx : 10099 \t loss: 0.0593 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 10199 \t loss: 0.0387 \t train acc: 0.9688\n",
      "epoch: 6 \t batch_idx : 10299 \t loss: 0.0324 \t train acc: 0.9688\n",
      "epoch: 6 \t batch_idx : 10399 \t loss: 0.0403 \t train acc: 0.9688\n",
      "epoch: 6 \t batch_idx : 10499 \t loss: 0.0377 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 10599 \t loss: 0.0868 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 10699 \t loss: 0.0891 \t train acc: 0.8438\n",
      "epoch: 6 \t batch_idx : 10799 \t loss: 0.0453 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 10899 \t loss: 0.0719 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 10999 \t loss: 0.0734 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 11099 \t loss: 0.0801 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 11199 \t loss: 0.0993 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 11299 \t loss: 0.0370 \t train acc: 1.0000\n",
      "epoch: 6 \t batch_idx : 11399 \t loss: 0.0281 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 11499 \t loss: 0.0553 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 11599 \t loss: 0.0871 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 11699 \t loss: 0.1202 \t train acc: 0.7812\n",
      "epoch: 6 \t batch_idx : 11799 \t loss: 0.0411 \t train acc: 0.9688\n",
      "epoch: 6 \t batch_idx : 11899 \t loss: 0.0834 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 11999 \t loss: 0.0895 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 12099 \t loss: 0.1220 \t train acc: 0.8438\n",
      "epoch: 6 \t batch_idx : 12199 \t loss: 0.0470 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 12299 \t loss: 0.0891 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 12399 \t loss: 0.0632 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 12499 \t loss: 0.0508 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 12599 \t loss: 0.0732 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 12699 \t loss: 0.0334 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 12799 \t loss: 0.0675 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 12899 \t loss: 0.0740 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 12999 \t loss: 0.0746 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 13099 \t loss: 0.0325 \t train acc: 1.0000\n",
      "epoch: 6 \t batch_idx : 13199 \t loss: 0.0617 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 13299 \t loss: 0.0792 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 13399 \t loss: 0.0897 \t train acc: 0.8438\n",
      "epoch: 6 \t batch_idx : 13499 \t loss: 0.0635 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 13599 \t loss: 0.1002 \t train acc: 0.8125\n",
      "epoch: 6 \t batch_idx : 13699 \t loss: 0.0281 \t train acc: 0.9688\n",
      "epoch: 6 \t batch_idx : 13799 \t loss: 0.1011 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 13899 \t loss: 0.0567 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 13999 \t loss: 0.0706 \t train acc: 0.8438\n",
      "epoch: 6 \t batch_idx : 14099 \t loss: 0.0211 \t train acc: 1.0000\n",
      "epoch: 6 \t batch_idx : 14199 \t loss: 0.0336 \t train acc: 0.9688\n",
      "epoch: 6 \t batch_idx : 14299 \t loss: 0.0642 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 14399 \t loss: 0.0382 \t train acc: 0.9688\n",
      "epoch: 6 \t batch_idx : 14499 \t loss: 0.0275 \t train acc: 0.9688\n",
      "epoch: 6 \t batch_idx : 14599 \t loss: 0.0498 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 14699 \t loss: 0.1169 \t train acc: 0.8125\n",
      "epoch: 6 \t batch_idx : 14799 \t loss: 0.0474 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 14899 \t loss: 0.0210 \t train acc: 0.9688\n",
      "epoch: 6 \t batch_idx : 14999 \t loss: 0.1109 \t train acc: 0.8438\n",
      "epoch: 6 \t batch_idx : 15099 \t loss: 0.0695 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 15199 \t loss: 0.0827 \t train acc: 0.8438\n",
      "epoch: 6 \t batch_idx : 15299 \t loss: 0.0605 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 15399 \t loss: 0.0469 \t train acc: 0.9688\n",
      "epoch: 6 \t batch_idx : 15499 \t loss: 0.0855 \t train acc: 0.8438\n",
      "epoch: 6 \t batch_idx : 15599 \t loss: 0.0698 \t train acc: 0.8438\n",
      "epoch: 6 \t batch_idx : 15699 \t loss: 0.0588 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 15799 \t loss: 0.0468 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 15899 \t loss: 0.0846 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 15999 \t loss: 0.1215 \t train acc: 0.8438\n",
      "epoch: 6 \t batch_idx : 16099 \t loss: 0.0942 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 16199 \t loss: 0.0596 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 16299 \t loss: 0.1440 \t train acc: 0.7500\n",
      "epoch: 6 \t batch_idx : 16399 \t loss: 0.0499 \t train acc: 0.9688\n",
      "epoch: 6 \t batch_idx : 16499 \t loss: 0.0552 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 16599 \t loss: 0.0405 \t train acc: 0.9688\n",
      "epoch: 6 \t batch_idx : 16699 \t loss: 0.0277 \t train acc: 0.9688\n",
      "test acc: 0.8077\n",
      "epoch: 7 \t batch_idx : 99 \t loss: 0.0191 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 199 \t loss: 0.0173 \t train acc: 1.0000\n",
      "epoch: 7 \t batch_idx : 299 \t loss: 0.0133 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 399 \t loss: 0.0327 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 499 \t loss: 0.0179 \t train acc: 1.0000\n",
      "epoch: 7 \t batch_idx : 599 \t loss: 0.0197 \t train acc: 1.0000\n",
      "epoch: 7 \t batch_idx : 699 \t loss: 0.0860 \t train acc: 0.8438\n",
      "epoch: 7 \t batch_idx : 799 \t loss: 0.0675 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 899 \t loss: 0.0920 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 999 \t loss: 0.0415 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 1099 \t loss: 0.0628 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 1199 \t loss: 0.0303 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 1299 \t loss: 0.0537 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 1399 \t loss: 0.0267 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 1499 \t loss: 0.0203 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 1599 \t loss: 0.0238 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 1699 \t loss: 0.0990 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 1799 \t loss: 0.0535 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 1899 \t loss: 0.0530 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 1999 \t loss: 0.0312 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 2099 \t loss: 0.0172 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 2199 \t loss: 0.0265 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 2299 \t loss: 0.0590 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 2399 \t loss: 0.0187 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 2499 \t loss: 0.0429 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 2599 \t loss: 0.0409 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 2699 \t loss: 0.0540 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 2799 \t loss: 0.0264 \t train acc: 1.0000\n",
      "epoch: 7 \t batch_idx : 2899 \t loss: 0.0520 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 2999 \t loss: 0.0458 \t train acc: 0.8750\n",
      "epoch: 7 \t batch_idx : 3099 \t loss: 0.0718 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 3199 \t loss: 0.0500 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 3299 \t loss: 0.0272 \t train acc: 0.9688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 \t batch_idx : 3399 \t loss: 0.0352 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 3499 \t loss: 0.0320 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 3599 \t loss: 0.0714 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 3699 \t loss: 0.0295 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 3799 \t loss: 0.0296 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 3899 \t loss: 0.0315 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 3999 \t loss: 0.0475 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 4099 \t loss: 0.0248 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 4199 \t loss: 0.0264 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 4299 \t loss: 0.0164 \t train acc: 1.0000\n",
      "epoch: 7 \t batch_idx : 4399 \t loss: 0.0413 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 4499 \t loss: 0.0659 \t train acc: 0.8750\n",
      "epoch: 7 \t batch_idx : 4599 \t loss: 0.0215 \t train acc: 1.0000\n",
      "epoch: 7 \t batch_idx : 4699 \t loss: 0.0903 \t train acc: 0.8438\n",
      "epoch: 7 \t batch_idx : 4799 \t loss: 0.1099 \t train acc: 0.8438\n",
      "epoch: 7 \t batch_idx : 4899 \t loss: 0.0261 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 4999 \t loss: 0.0061 \t train acc: 1.0000\n",
      "epoch: 7 \t batch_idx : 5099 \t loss: 0.0086 \t train acc: 1.0000\n",
      "epoch: 7 \t batch_idx : 5199 \t loss: 0.0545 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 5299 \t loss: 0.1028 \t train acc: 0.8438\n",
      "epoch: 7 \t batch_idx : 5399 \t loss: 0.0505 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 5499 \t loss: 0.0593 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 5599 \t loss: 0.0505 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 5699 \t loss: 0.0482 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 5799 \t loss: 0.0540 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 5899 \t loss: 0.0706 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 5999 \t loss: 0.0309 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 6099 \t loss: 0.0151 \t train acc: 1.0000\n",
      "epoch: 7 \t batch_idx : 6199 \t loss: 0.0515 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 6299 \t loss: 0.0358 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 6399 \t loss: 0.0360 \t train acc: 1.0000\n",
      "epoch: 7 \t batch_idx : 6499 \t loss: 0.0361 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 6599 \t loss: 0.0770 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 6699 \t loss: 0.0124 \t train acc: 1.0000\n",
      "epoch: 7 \t batch_idx : 6799 \t loss: 0.0403 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 6899 \t loss: 0.0358 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 6999 \t loss: 0.0164 \t train acc: 1.0000\n",
      "epoch: 7 \t batch_idx : 7099 \t loss: 0.0522 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 7199 \t loss: 0.0515 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 7299 \t loss: 0.0371 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 7399 \t loss: 0.0609 \t train acc: 0.8750\n",
      "epoch: 7 \t batch_idx : 7499 \t loss: 0.0526 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 7599 \t loss: 0.0443 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 7699 \t loss: 0.0237 \t train acc: 1.0000\n",
      "epoch: 7 \t batch_idx : 7799 \t loss: 0.0336 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 7899 \t loss: 0.0431 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 7999 \t loss: 0.0550 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 8099 \t loss: 0.0439 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 8199 \t loss: 0.0147 \t train acc: 1.0000\n",
      "epoch: 7 \t batch_idx : 8299 \t loss: 0.0648 \t train acc: 0.8750\n",
      "epoch: 7 \t batch_idx : 8399 \t loss: 0.0875 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 8499 \t loss: 0.0417 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 8599 \t loss: 0.0525 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 8699 \t loss: 0.0593 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 8799 \t loss: 0.0592 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 8899 \t loss: 0.0379 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 8999 \t loss: 0.0464 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 9099 \t loss: 0.0548 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 9199 \t loss: 0.0592 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 9299 \t loss: 0.0274 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 9399 \t loss: 0.0401 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 9499 \t loss: 0.0591 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 9599 \t loss: 0.0511 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 9699 \t loss: 0.0629 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 9799 \t loss: 0.0548 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 9899 \t loss: 0.0749 \t train acc: 0.8750\n",
      "epoch: 7 \t batch_idx : 9999 \t loss: 0.0446 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 10099 \t loss: 0.0323 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 10199 \t loss: 0.0474 \t train acc: 0.8750\n",
      "epoch: 7 \t batch_idx : 10299 \t loss: 0.0507 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 10399 \t loss: 0.0538 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 10499 \t loss: 0.0858 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 10599 \t loss: 0.0317 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 10699 \t loss: 0.0803 \t train acc: 0.8750\n",
      "epoch: 7 \t batch_idx : 10799 \t loss: 0.0367 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 10899 \t loss: 0.0664 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 10999 \t loss: 0.0692 \t train acc: 0.8750\n",
      "epoch: 7 \t batch_idx : 11099 \t loss: 0.0447 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 11199 \t loss: 0.0609 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 11299 \t loss: 0.0739 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 11399 \t loss: 0.0531 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 11499 \t loss: 0.0671 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 11599 \t loss: 0.0553 \t train acc: 0.8438\n",
      "epoch: 7 \t batch_idx : 11699 \t loss: 0.0637 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 11799 \t loss: 0.0205 \t train acc: 1.0000\n",
      "epoch: 7 \t batch_idx : 11899 \t loss: 0.0689 \t train acc: 0.8750\n",
      "epoch: 7 \t batch_idx : 11999 \t loss: 0.0629 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 12099 \t loss: 0.0699 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 12199 \t loss: 0.0148 \t train acc: 1.0000\n",
      "epoch: 7 \t batch_idx : 12299 \t loss: 0.0524 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 12399 \t loss: 0.0653 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 12499 \t loss: 0.1032 \t train acc: 0.8750\n",
      "epoch: 7 \t batch_idx : 12599 \t loss: 0.0289 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 12699 \t loss: 0.0503 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 12799 \t loss: 0.0576 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 12899 \t loss: 0.0559 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 12999 \t loss: 0.0736 \t train acc: 0.8750\n",
      "epoch: 7 \t batch_idx : 13099 \t loss: 0.0636 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 13199 \t loss: 0.0386 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 13299 \t loss: 0.0264 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 13399 \t loss: 0.0470 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 13499 \t loss: 0.0692 \t train acc: 0.8750\n",
      "epoch: 7 \t batch_idx : 13599 \t loss: 0.0777 \t train acc: 0.8750\n",
      "epoch: 7 \t batch_idx : 13699 \t loss: 0.0836 \t train acc: 0.8750\n",
      "epoch: 7 \t batch_idx : 13799 \t loss: 0.0318 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 13899 \t loss: 0.0546 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 13999 \t loss: 0.0725 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 14099 \t loss: 0.0606 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 14199 \t loss: 0.0843 \t train acc: 0.7812\n",
      "epoch: 7 \t batch_idx : 14299 \t loss: 0.0725 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 14399 \t loss: 0.0768 \t train acc: 0.8750\n",
      "epoch: 7 \t batch_idx : 14499 \t loss: 0.0236 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 14599 \t loss: 0.0597 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 14699 \t loss: 0.0764 \t train acc: 0.8438\n",
      "epoch: 7 \t batch_idx : 14799 \t loss: 0.1037 \t train acc: 0.8438\n",
      "epoch: 7 \t batch_idx : 14899 \t loss: 0.0129 \t train acc: 1.0000\n",
      "epoch: 7 \t batch_idx : 14999 \t loss: 0.0341 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 15099 \t loss: 0.0761 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 15199 \t loss: 0.0702 \t train acc: 0.8750\n",
      "epoch: 7 \t batch_idx : 15299 \t loss: 0.0261 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 15399 \t loss: 0.0558 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 15499 \t loss: 0.0391 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 15599 \t loss: 0.0551 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 15699 \t loss: 0.0536 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 15799 \t loss: 0.0504 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 15899 \t loss: 0.0687 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 15999 \t loss: 0.0977 \t train acc: 0.8438\n",
      "epoch: 7 \t batch_idx : 16099 \t loss: 0.0852 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 16199 \t loss: 0.0417 \t train acc: 1.0000\n",
      "epoch: 7 \t batch_idx : 16299 \t loss: 0.0545 \t train acc: 0.9688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 \t batch_idx : 16399 \t loss: 0.0849 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 16499 \t loss: 0.0697 \t train acc: 0.8750\n",
      "epoch: 7 \t batch_idx : 16599 \t loss: 0.0717 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 16699 \t loss: 0.0474 \t train acc: 0.9375\n",
      "test acc: 0.8072\n",
      "epoch: 8 \t batch_idx : 99 \t loss: 0.0291 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 199 \t loss: 0.0592 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 299 \t loss: 0.0244 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 399 \t loss: 0.0427 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 499 \t loss: 0.0106 \t train acc: 1.0000\n",
      "epoch: 8 \t batch_idx : 599 \t loss: 0.0190 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 699 \t loss: 0.0318 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 799 \t loss: 0.0759 \t train acc: 0.9062\n",
      "epoch: 8 \t batch_idx : 899 \t loss: 0.0360 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 999 \t loss: 0.0351 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 1099 \t loss: 0.0235 \t train acc: 1.0000\n",
      "epoch: 8 \t batch_idx : 1199 \t loss: 0.0677 \t train acc: 0.8438\n",
      "epoch: 8 \t batch_idx : 1299 \t loss: 0.0234 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 1399 \t loss: 0.0228 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 1499 \t loss: 0.0090 \t train acc: 1.0000\n",
      "epoch: 8 \t batch_idx : 1599 \t loss: 0.0146 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 1699 \t loss: 0.0839 \t train acc: 0.9062\n",
      "epoch: 8 \t batch_idx : 1799 \t loss: 0.0199 \t train acc: 1.0000\n",
      "epoch: 8 \t batch_idx : 1899 \t loss: 0.0521 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 1999 \t loss: 0.0435 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 2099 \t loss: 0.0482 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 2199 \t loss: 0.0646 \t train acc: 0.9062\n",
      "epoch: 8 \t batch_idx : 2299 \t loss: 0.0119 \t train acc: 1.0000\n",
      "epoch: 8 \t batch_idx : 2399 \t loss: 0.0747 \t train acc: 0.9062\n",
      "epoch: 8 \t batch_idx : 2499 \t loss: 0.0345 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 2599 \t loss: 0.0458 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 2699 \t loss: 0.0145 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 2799 \t loss: 0.0473 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 2899 \t loss: 0.0168 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 2999 \t loss: 0.0236 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 3099 \t loss: 0.0468 \t train acc: 0.9062\n",
      "epoch: 8 \t batch_idx : 3199 \t loss: 0.0084 \t train acc: 1.0000\n",
      "epoch: 8 \t batch_idx : 3299 \t loss: 0.0111 \t train acc: 1.0000\n",
      "epoch: 8 \t batch_idx : 3399 \t loss: 0.0134 \t train acc: 1.0000\n",
      "epoch: 8 \t batch_idx : 3499 \t loss: 0.0346 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 3599 \t loss: 0.0608 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 3699 \t loss: 0.0625 \t train acc: 0.9062\n",
      "epoch: 8 \t batch_idx : 3799 \t loss: 0.0605 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 3899 \t loss: 0.0358 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 3999 \t loss: 0.0630 \t train acc: 0.9062\n",
      "epoch: 8 \t batch_idx : 4099 \t loss: 0.0412 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 4199 \t loss: 0.0162 \t train acc: 1.0000\n",
      "epoch: 8 \t batch_idx : 4299 \t loss: 0.0164 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 4399 \t loss: 0.0406 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 4499 \t loss: 0.0712 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 4599 \t loss: 0.0320 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 4699 \t loss: 0.0195 \t train acc: 1.0000\n",
      "epoch: 8 \t batch_idx : 4799 \t loss: 0.0213 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 4899 \t loss: 0.0271 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 4999 \t loss: 0.0138 \t train acc: 1.0000\n",
      "epoch: 8 \t batch_idx : 5099 \t loss: 0.0565 \t train acc: 0.9062\n",
      "epoch: 8 \t batch_idx : 5199 \t loss: 0.0718 \t train acc: 0.8750\n",
      "epoch: 8 \t batch_idx : 5299 \t loss: 0.0484 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 5399 \t loss: 0.0147 \t train acc: 1.0000\n",
      "epoch: 8 \t batch_idx : 5499 \t loss: 0.0149 \t train acc: 1.0000\n",
      "epoch: 8 \t batch_idx : 5599 \t loss: 0.0123 \t train acc: 1.0000\n",
      "epoch: 8 \t batch_idx : 5699 \t loss: 0.0196 \t train acc: 1.0000\n",
      "epoch: 8 \t batch_idx : 5799 \t loss: 0.0118 \t train acc: 1.0000\n",
      "epoch: 8 \t batch_idx : 5899 \t loss: 0.0373 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 5999 \t loss: 0.0626 \t train acc: 0.9062\n",
      "epoch: 8 \t batch_idx : 6099 \t loss: 0.0291 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 6199 \t loss: 0.0655 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 6299 \t loss: 0.0789 \t train acc: 0.8750\n",
      "epoch: 8 \t batch_idx : 6399 \t loss: 0.0398 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 6499 \t loss: 0.0570 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 6599 \t loss: 0.0466 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 6699 \t loss: 0.0530 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 6799 \t loss: 0.0125 \t train acc: 1.0000\n",
      "epoch: 8 \t batch_idx : 6899 \t loss: 0.0512 \t train acc: 0.9062\n",
      "epoch: 8 \t batch_idx : 6999 \t loss: 0.0400 \t train acc: 0.9062\n",
      "epoch: 8 \t batch_idx : 7099 \t loss: 0.0224 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 7199 \t loss: 0.0150 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 7299 \t loss: 0.0172 \t train acc: 1.0000\n",
      "epoch: 8 \t batch_idx : 7399 \t loss: 0.0100 \t train acc: 1.0000\n",
      "epoch: 8 \t batch_idx : 7499 \t loss: 0.0094 \t train acc: 1.0000\n",
      "epoch: 8 \t batch_idx : 7599 \t loss: 0.0547 \t train acc: 0.9062\n",
      "epoch: 8 \t batch_idx : 7699 \t loss: 0.0364 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 7799 \t loss: 0.0355 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 7899 \t loss: 0.0295 \t train acc: 1.0000\n",
      "epoch: 8 \t batch_idx : 7999 \t loss: 0.0275 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 8099 \t loss: 0.0399 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 8199 \t loss: 0.0401 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 8299 \t loss: 0.0264 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 8399 \t loss: 0.0350 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 8499 \t loss: 0.0751 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 8599 \t loss: 0.0402 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 8699 \t loss: 0.0144 \t train acc: 1.0000\n",
      "epoch: 8 \t batch_idx : 8799 \t loss: 0.0442 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 8899 \t loss: 0.0212 \t train acc: 1.0000\n",
      "epoch: 8 \t batch_idx : 8999 \t loss: 0.0577 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 9099 \t loss: 0.0172 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 9199 \t loss: 0.0365 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 9299 \t loss: 0.0584 \t train acc: 0.9062\n",
      "epoch: 8 \t batch_idx : 9399 \t loss: 0.0569 \t train acc: 0.8750\n",
      "epoch: 8 \t batch_idx : 9499 \t loss: 0.0594 \t train acc: 0.9062\n",
      "epoch: 8 \t batch_idx : 9599 \t loss: 0.0272 \t train acc: 1.0000\n",
      "epoch: 8 \t batch_idx : 9699 \t loss: 0.0303 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 9799 \t loss: 0.0541 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 9899 \t loss: 0.0204 \t train acc: 1.0000\n",
      "epoch: 8 \t batch_idx : 9999 \t loss: 0.1087 \t train acc: 0.8750\n",
      "epoch: 8 \t batch_idx : 10099 \t loss: 0.0241 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 10199 \t loss: 0.0087 \t train acc: 1.0000\n",
      "epoch: 8 \t batch_idx : 10299 \t loss: 0.0363 \t train acc: 0.9062\n",
      "epoch: 8 \t batch_idx : 10399 \t loss: 0.0083 \t train acc: 1.0000\n",
      "epoch: 8 \t batch_idx : 10499 \t loss: 0.0168 \t train acc: 1.0000\n",
      "epoch: 8 \t batch_idx : 10599 \t loss: 0.0360 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 10699 \t loss: 0.0337 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 10799 \t loss: 0.0250 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 10899 \t loss: 0.0080 \t train acc: 1.0000\n",
      "epoch: 8 \t batch_idx : 10999 \t loss: 0.0557 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 11099 \t loss: 0.0428 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 11199 \t loss: 0.0642 \t train acc: 0.9062\n",
      "epoch: 8 \t batch_idx : 11299 \t loss: 0.0667 \t train acc: 0.9062\n",
      "epoch: 8 \t batch_idx : 11399 \t loss: 0.0263 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 11499 \t loss: 0.0374 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 11599 \t loss: 0.0486 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 11699 \t loss: 0.0591 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 11799 \t loss: 0.0093 \t train acc: 1.0000\n",
      "epoch: 8 \t batch_idx : 11899 \t loss: 0.0259 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 11999 \t loss: 0.0660 \t train acc: 0.9062\n",
      "epoch: 8 \t batch_idx : 12099 \t loss: 0.0272 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 12199 \t loss: 0.0260 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 12299 \t loss: 0.0418 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 12399 \t loss: 0.0519 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 12499 \t loss: 0.0576 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 12599 \t loss: 0.0618 \t train acc: 0.8750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 \t batch_idx : 12699 \t loss: 0.0174 \t train acc: 1.0000\n",
      "epoch: 8 \t batch_idx : 12799 \t loss: 0.0703 \t train acc: 0.8750\n",
      "epoch: 8 \t batch_idx : 12899 \t loss: 0.0229 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 12999 \t loss: 0.0638 \t train acc: 0.8438\n",
      "epoch: 8 \t batch_idx : 13099 \t loss: 0.0513 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 13199 \t loss: 0.0332 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 13299 \t loss: 0.0431 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 13399 \t loss: 0.0422 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 13499 \t loss: 0.0322 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 13599 \t loss: 0.0413 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 13699 \t loss: 0.0381 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 13799 \t loss: 0.0690 \t train acc: 0.9062\n",
      "epoch: 8 \t batch_idx : 13899 \t loss: 0.0879 \t train acc: 0.9062\n",
      "epoch: 8 \t batch_idx : 13999 \t loss: 0.0254 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 14099 \t loss: 0.0565 \t train acc: 0.9062\n",
      "epoch: 8 \t batch_idx : 14199 \t loss: 0.0221 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 14299 \t loss: 0.0161 \t train acc: 1.0000\n",
      "epoch: 8 \t batch_idx : 14399 \t loss: 0.0252 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 14499 \t loss: 0.0778 \t train acc: 0.9062\n",
      "epoch: 8 \t batch_idx : 14599 \t loss: 0.0368 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 14699 \t loss: 0.0865 \t train acc: 0.8750\n",
      "epoch: 8 \t batch_idx : 14799 \t loss: 0.0905 \t train acc: 0.8750\n",
      "epoch: 8 \t batch_idx : 14899 \t loss: 0.0595 \t train acc: 0.9062\n",
      "epoch: 8 \t batch_idx : 14999 \t loss: 0.0529 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 15099 \t loss: 0.0287 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 15199 \t loss: 0.0501 \t train acc: 0.9062\n",
      "epoch: 8 \t batch_idx : 15299 \t loss: 0.0506 \t train acc: 0.8750\n",
      "epoch: 8 \t batch_idx : 15399 \t loss: 0.0947 \t train acc: 0.8438\n",
      "epoch: 8 \t batch_idx : 15499 \t loss: 0.0270 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 15599 \t loss: 0.0516 \t train acc: 0.9062\n",
      "epoch: 8 \t batch_idx : 15699 \t loss: 0.0703 \t train acc: 0.8750\n",
      "epoch: 8 \t batch_idx : 15799 \t loss: 0.0770 \t train acc: 0.9062\n",
      "epoch: 8 \t batch_idx : 15899 \t loss: 0.0813 \t train acc: 0.8750\n",
      "epoch: 8 \t batch_idx : 15999 \t loss: 0.0559 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 16099 \t loss: 0.0493 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 16199 \t loss: 0.0606 \t train acc: 0.9062\n",
      "epoch: 8 \t batch_idx : 16299 \t loss: 0.0323 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 16399 \t loss: 0.0805 \t train acc: 0.8750\n",
      "epoch: 8 \t batch_idx : 16499 \t loss: 0.0500 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 16599 \t loss: 0.0199 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 16699 \t loss: 0.0143 \t train acc: 1.0000\n",
      "test acc: 0.8024\n",
      "epoch: 9 \t batch_idx : 99 \t loss: 0.0367 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 199 \t loss: 0.0714 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 299 \t loss: 0.0092 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 399 \t loss: 0.0429 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 499 \t loss: 0.0192 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 599 \t loss: 0.0350 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 699 \t loss: 0.0173 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 799 \t loss: 0.0243 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 899 \t loss: 0.0189 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 999 \t loss: 0.0259 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 1099 \t loss: 0.0485 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 1199 \t loss: 0.0454 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 1299 \t loss: 0.0098 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 1399 \t loss: 0.0298 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 1499 \t loss: 0.0412 \t train acc: 0.9062\n",
      "epoch: 9 \t batch_idx : 1599 \t loss: 0.0157 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 1699 \t loss: 0.0112 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 1799 \t loss: 0.0179 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 1899 \t loss: 0.0119 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 1999 \t loss: 0.0299 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 2099 \t loss: 0.0205 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 2199 \t loss: 0.0319 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 2299 \t loss: 0.0255 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 2399 \t loss: 0.0088 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 2499 \t loss: 0.0255 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 2599 \t loss: 0.0344 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 2699 \t loss: 0.0379 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 2799 \t loss: 0.0369 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 2899 \t loss: 0.0523 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 2999 \t loss: 0.0221 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 3099 \t loss: 0.0128 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 3199 \t loss: 0.0693 \t train acc: 0.9062\n",
      "epoch: 9 \t batch_idx : 3299 \t loss: 0.0566 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 3399 \t loss: 0.0303 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 3499 \t loss: 0.0416 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 3599 \t loss: 0.0176 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 3699 \t loss: 0.0231 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 3799 \t loss: 0.0361 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 3899 \t loss: 0.0195 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 3999 \t loss: 0.0755 \t train acc: 0.8750\n",
      "epoch: 9 \t batch_idx : 4099 \t loss: 0.0082 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 4199 \t loss: 0.0315 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 4299 \t loss: 0.0443 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 4399 \t loss: 0.0282 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 4499 \t loss: 0.0184 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 4599 \t loss: 0.0272 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 4699 \t loss: 0.1021 \t train acc: 0.9062\n",
      "epoch: 9 \t batch_idx : 4799 \t loss: 0.0328 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 4899 \t loss: 0.0468 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 4999 \t loss: 0.0482 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 5099 \t loss: 0.0371 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 5199 \t loss: 0.0063 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 5299 \t loss: 0.0195 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 5399 \t loss: 0.0414 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 5499 \t loss: 0.0151 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 5599 \t loss: 0.0436 \t train acc: 0.9062\n",
      "epoch: 9 \t batch_idx : 5699 \t loss: 0.0296 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 5799 \t loss: 0.0258 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 5899 \t loss: 0.0254 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 5999 \t loss: 0.0845 \t train acc: 0.9062\n",
      "epoch: 9 \t batch_idx : 6099 \t loss: 0.0130 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 6199 \t loss: 0.0313 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 6299 \t loss: 0.0229 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 6399 \t loss: 0.0123 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 6499 \t loss: 0.0330 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 6599 \t loss: 0.0136 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 6699 \t loss: 0.0309 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 6799 \t loss: 0.0320 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 6899 \t loss: 0.0158 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 6999 \t loss: 0.0504 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 7099 \t loss: 0.0166 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 7199 \t loss: 0.0659 \t train acc: 0.9062\n",
      "epoch: 9 \t batch_idx : 7299 \t loss: 0.0096 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 7399 \t loss: 0.0044 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 7499 \t loss: 0.0032 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 7599 \t loss: 0.0497 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 7699 \t loss: 0.0359 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 7799 \t loss: 0.0432 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 7899 \t loss: 0.0292 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 7999 \t loss: 0.0678 \t train acc: 0.8750\n",
      "epoch: 9 \t batch_idx : 8099 \t loss: 0.0076 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 8199 \t loss: 0.0640 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 8299 \t loss: 0.0475 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 8399 \t loss: 0.0370 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 8499 \t loss: 0.0116 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 8599 \t loss: 0.0583 \t train acc: 0.9062\n",
      "epoch: 9 \t batch_idx : 8699 \t loss: 0.1322 \t train acc: 0.8438\n",
      "epoch: 9 \t batch_idx : 8799 \t loss: 0.0155 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 8899 \t loss: 0.0547 \t train acc: 0.9062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 \t batch_idx : 8999 \t loss: 0.0345 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 9099 \t loss: 0.0394 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 9199 \t loss: 0.0146 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 9299 \t loss: 0.0313 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 9399 \t loss: 0.0496 \t train acc: 0.9062\n",
      "epoch: 9 \t batch_idx : 9499 \t loss: 0.0315 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 9599 \t loss: 0.0370 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 9699 \t loss: 0.0367 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 9799 \t loss: 0.0285 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 9899 \t loss: 0.0179 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 9999 \t loss: 0.0356 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 10099 \t loss: 0.0703 \t train acc: 0.8750\n",
      "epoch: 9 \t batch_idx : 10199 \t loss: 0.0659 \t train acc: 0.9062\n",
      "epoch: 9 \t batch_idx : 10299 \t loss: 0.0092 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 10399 \t loss: 0.0152 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 10499 \t loss: 0.0323 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 10599 \t loss: 0.0583 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 10699 \t loss: 0.0653 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 10799 \t loss: 0.0333 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 10899 \t loss: 0.0661 \t train acc: 0.8750\n",
      "epoch: 9 \t batch_idx : 10999 \t loss: 0.0474 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 11099 \t loss: 0.0579 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 11199 \t loss: 0.0191 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 11299 \t loss: 0.0107 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 11399 \t loss: 0.0095 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 11499 \t loss: 0.0538 \t train acc: 0.9062\n",
      "epoch: 9 \t batch_idx : 11599 \t loss: 0.0259 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 11699 \t loss: 0.0280 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 11799 \t loss: 0.0527 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 11899 \t loss: 0.0795 \t train acc: 0.8750\n",
      "epoch: 9 \t batch_idx : 11999 \t loss: 0.0510 \t train acc: 0.9062\n",
      "epoch: 9 \t batch_idx : 12099 \t loss: 0.0539 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 12199 \t loss: 0.0355 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 12299 \t loss: 0.0302 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 12399 \t loss: 0.0301 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 12499 \t loss: 0.0703 \t train acc: 0.9062\n",
      "epoch: 9 \t batch_idx : 12599 \t loss: 0.0558 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 12699 \t loss: 0.0255 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 12799 \t loss: 0.0312 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 12899 \t loss: 0.0279 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 12999 \t loss: 0.0080 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 13099 \t loss: 0.0387 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 13199 \t loss: 0.0805 \t train acc: 0.8750\n",
      "epoch: 9 \t batch_idx : 13299 \t loss: 0.0239 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 13399 \t loss: 0.0580 \t train acc: 0.9062\n",
      "epoch: 9 \t batch_idx : 13499 \t loss: 0.0723 \t train acc: 0.8750\n",
      "epoch: 9 \t batch_idx : 13599 \t loss: 0.0536 \t train acc: 0.9062\n",
      "epoch: 9 \t batch_idx : 13699 \t loss: 0.0746 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 13799 \t loss: 0.0192 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 13899 \t loss: 0.0087 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 13999 \t loss: 0.0490 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 14099 \t loss: 0.0510 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 14199 \t loss: 0.0209 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 14299 \t loss: 0.0187 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 14399 \t loss: 0.0115 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 14499 \t loss: 0.0212 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 14599 \t loss: 0.0383 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 14699 \t loss: 0.0185 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 14799 \t loss: 0.0683 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 14899 \t loss: 0.0959 \t train acc: 0.8438\n",
      "epoch: 9 \t batch_idx : 14999 \t loss: 0.0623 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 15099 \t loss: 0.0276 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 15199 \t loss: 0.0052 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 15299 \t loss: 0.0090 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 15399 \t loss: 0.0598 \t train acc: 0.9062\n",
      "epoch: 9 \t batch_idx : 15499 \t loss: 0.0490 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 15599 \t loss: 0.0227 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 15699 \t loss: 0.0459 \t train acc: 0.9062\n",
      "epoch: 9 \t batch_idx : 15799 \t loss: 0.0160 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 15899 \t loss: 0.0351 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 15999 \t loss: 0.0189 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 16099 \t loss: 0.0129 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 16199 \t loss: 0.0394 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 16299 \t loss: 0.0107 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 16399 \t loss: 0.0195 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 16499 \t loss: 0.0163 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 16599 \t loss: 0.0953 \t train acc: 0.8750\n",
      "epoch: 9 \t batch_idx : 16699 \t loss: 0.0830 \t train acc: 0.8438\n",
      "test acc: 0.8025\n",
      "epoch: 10 \t batch_idx : 99 \t loss: 0.0017 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 199 \t loss: 0.0181 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 299 \t loss: 0.0469 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 399 \t loss: 0.0629 \t train acc: 0.8750\n",
      "epoch: 10 \t batch_idx : 499 \t loss: 0.0084 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 599 \t loss: 0.0072 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 699 \t loss: 0.0083 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 799 \t loss: 0.0418 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 899 \t loss: 0.0098 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 999 \t loss: 0.0192 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 1099 \t loss: 0.0134 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 1199 \t loss: 0.0030 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 1299 \t loss: 0.0093 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 1399 \t loss: 0.0223 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 1499 \t loss: 0.0149 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 1599 \t loss: 0.0334 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 1699 \t loss: 0.0265 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 1799 \t loss: 0.0111 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 1899 \t loss: 0.0106 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 1999 \t loss: 0.0147 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 2099 \t loss: 0.0175 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 2199 \t loss: 0.0103 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 2299 \t loss: 0.0347 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 2399 \t loss: 0.0110 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 2499 \t loss: 0.0481 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 2599 \t loss: 0.0628 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 2699 \t loss: 0.0049 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 2799 \t loss: 0.0094 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 2899 \t loss: 0.0070 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 2999 \t loss: 0.0011 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 3099 \t loss: 0.0311 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 3199 \t loss: 0.0260 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 3299 \t loss: 0.0433 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 3399 \t loss: 0.0302 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 3499 \t loss: 0.0476 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 3599 \t loss: 0.0301 \t train acc: 0.9062\n",
      "epoch: 10 \t batch_idx : 3699 \t loss: 0.0274 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 3799 \t loss: 0.0009 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 3899 \t loss: 0.0447 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 3999 \t loss: 0.0140 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 4099 \t loss: 0.0082 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 4199 \t loss: 0.0098 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 4299 \t loss: 0.0233 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 4399 \t loss: 0.0205 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 4499 \t loss: 0.0393 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 4599 \t loss: 0.0138 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 4699 \t loss: 0.0251 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 4799 \t loss: 0.0067 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 4899 \t loss: 0.0333 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 4999 \t loss: 0.0469 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 5099 \t loss: 0.0172 \t train acc: 0.9688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 \t batch_idx : 5199 \t loss: 0.0088 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 5299 \t loss: 0.0384 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 5399 \t loss: 0.0058 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 5499 \t loss: 0.0447 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 5599 \t loss: 0.0597 \t train acc: 0.9062\n",
      "epoch: 10 \t batch_idx : 5699 \t loss: 0.0121 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 5799 \t loss: 0.0276 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 5899 \t loss: 0.0022 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 5999 \t loss: 0.0094 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 6099 \t loss: 0.0118 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 6199 \t loss: 0.0913 \t train acc: 0.8750\n",
      "epoch: 10 \t batch_idx : 6299 \t loss: 0.0066 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 6399 \t loss: 0.0143 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 6499 \t loss: 0.0113 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 6599 \t loss: 0.0146 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 6699 \t loss: 0.0164 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 6799 \t loss: 0.0379 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 6899 \t loss: 0.0372 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 6999 \t loss: 0.0411 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 7099 \t loss: 0.0104 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 7199 \t loss: 0.0159 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 7299 \t loss: 0.0245 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 7399 \t loss: 0.0287 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 7499 \t loss: 0.0097 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 7599 \t loss: 0.0062 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 7699 \t loss: 0.0636 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 7799 \t loss: 0.0216 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 7899 \t loss: 0.0079 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 7999 \t loss: 0.0183 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 8099 \t loss: 0.0387 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 8199 \t loss: 0.0108 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 8299 \t loss: 0.0362 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 8399 \t loss: 0.0189 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 8499 \t loss: 0.0123 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 8599 \t loss: 0.0125 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 8699 \t loss: 0.0264 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 8799 \t loss: 0.0178 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 8899 \t loss: 0.0061 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 8999 \t loss: 0.0500 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 9099 \t loss: 0.0280 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 9199 \t loss: 0.0117 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 9299 \t loss: 0.0165 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 9399 \t loss: 0.0328 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 9499 \t loss: 0.0154 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 9599 \t loss: 0.0429 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 9699 \t loss: 0.0335 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 9799 \t loss: 0.0182 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 9899 \t loss: 0.0119 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 9999 \t loss: 0.0079 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 10099 \t loss: 0.0024 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 10199 \t loss: 0.0480 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 10299 \t loss: 0.0185 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 10399 \t loss: 0.0636 \t train acc: 0.9062\n",
      "epoch: 10 \t batch_idx : 10499 \t loss: 0.0053 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 10599 \t loss: 0.0180 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 10699 \t loss: 0.0202 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 10799 \t loss: 0.0424 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 10899 \t loss: 0.0510 \t train acc: 0.8750\n",
      "epoch: 10 \t batch_idx : 10999 \t loss: 0.0422 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 11099 \t loss: 0.0047 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 11199 \t loss: 0.0382 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 11299 \t loss: 0.0237 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 11399 \t loss: 0.0053 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 11499 \t loss: 0.0277 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 11599 \t loss: 0.0430 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 11699 \t loss: 0.0215 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 11799 \t loss: 0.0683 \t train acc: 0.8750\n",
      "epoch: 10 \t batch_idx : 11899 \t loss: 0.0807 \t train acc: 0.9062\n",
      "epoch: 10 \t batch_idx : 11999 \t loss: 0.0191 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 12099 \t loss: 0.0132 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 12199 \t loss: 0.0044 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 12299 \t loss: 0.0434 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 12399 \t loss: 0.0365 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 12499 \t loss: 0.0054 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 12599 \t loss: 0.0529 \t train acc: 0.8750\n",
      "epoch: 10 \t batch_idx : 12699 \t loss: 0.0606 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 12799 \t loss: 0.0411 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 12899 \t loss: 0.0036 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 12999 \t loss: 0.0526 \t train acc: 0.9062\n",
      "epoch: 10 \t batch_idx : 13099 \t loss: 0.0314 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 13199 \t loss: 0.0406 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 13299 \t loss: 0.0554 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 13399 \t loss: 0.0574 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 13499 \t loss: 0.0128 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 13599 \t loss: 0.0202 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 13699 \t loss: 0.0139 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 13799 \t loss: 0.0569 \t train acc: 0.9062\n",
      "epoch: 10 \t batch_idx : 13899 \t loss: 0.0134 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 13999 \t loss: 0.0020 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 14099 \t loss: 0.0235 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 14199 \t loss: 0.0163 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 14299 \t loss: 0.0468 \t train acc: 0.9062\n",
      "epoch: 10 \t batch_idx : 14399 \t loss: 0.0235 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 14499 \t loss: 0.0254 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 14599 \t loss: 0.0116 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 14699 \t loss: 0.0132 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 14799 \t loss: 0.0489 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 14899 \t loss: 0.0370 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 14999 \t loss: 0.0388 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 15099 \t loss: 0.0423 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 15199 \t loss: 0.0315 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 15299 \t loss: 0.0256 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 15399 \t loss: 0.0337 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 15499 \t loss: 0.0048 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 15599 \t loss: 0.0946 \t train acc: 0.8750\n",
      "epoch: 10 \t batch_idx : 15699 \t loss: 0.0056 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 15799 \t loss: 0.0407 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 15899 \t loss: 0.0126 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 15999 \t loss: 0.0753 \t train acc: 0.9062\n",
      "epoch: 10 \t batch_idx : 16099 \t loss: 0.0320 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 16199 \t loss: 0.0430 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 16299 \t loss: 0.0648 \t train acc: 0.8750\n",
      "epoch: 10 \t batch_idx : 16399 \t loss: 0.0133 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 16499 \t loss: 0.0326 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 16599 \t loss: 0.0351 \t train acc: 0.9062\n",
      "epoch: 10 \t batch_idx : 16699 \t loss: 0.0134 \t train acc: 1.0000\n",
      "test acc: 0.7989\n",
      "epoch: 11 \t batch_idx : 99 \t loss: 0.0088 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 199 \t loss: 0.0151 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 299 \t loss: 0.0439 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 399 \t loss: 0.0174 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 499 \t loss: 0.0022 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 599 \t loss: 0.0047 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 699 \t loss: 0.0310 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 799 \t loss: 0.0195 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 899 \t loss: 0.0401 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 999 \t loss: 0.0138 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 1099 \t loss: 0.0536 \t train acc: 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11 \t batch_idx : 1199 \t loss: 0.0037 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 1299 \t loss: 0.0104 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 1399 \t loss: 0.0059 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 1499 \t loss: 0.0077 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 1599 \t loss: 0.0154 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 1699 \t loss: 0.0558 \t train acc: 0.9062\n",
      "epoch: 11 \t batch_idx : 1799 \t loss: 0.0281 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 1899 \t loss: 0.0322 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 1999 \t loss: 0.0083 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 2099 \t loss: 0.0132 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 2199 \t loss: 0.0139 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 2299 \t loss: 0.0231 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 2399 \t loss: 0.0082 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 2499 \t loss: 0.0152 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 2599 \t loss: 0.0039 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 2699 \t loss: 0.0604 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 2799 \t loss: 0.0282 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 2899 \t loss: 0.0461 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 2999 \t loss: 0.0430 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 3099 \t loss: 0.0228 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 3199 \t loss: 0.0136 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 3299 \t loss: 0.0163 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 3399 \t loss: 0.0165 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 3499 \t loss: 0.0099 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 3599 \t loss: 0.0126 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 3699 \t loss: 0.0209 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 3799 \t loss: 0.0109 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 3899 \t loss: 0.0312 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 3999 \t loss: 0.0157 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 4099 \t loss: 0.0320 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 4199 \t loss: 0.0310 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 4299 \t loss: 0.0466 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 4399 \t loss: 0.0104 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 4499 \t loss: 0.0070 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 4599 \t loss: 0.0654 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 4699 \t loss: 0.0529 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 4799 \t loss: 0.0439 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 4899 \t loss: 0.0106 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 4999 \t loss: 0.0095 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 5099 \t loss: 0.0423 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 5199 \t loss: 0.0160 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 5299 \t loss: 0.0620 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 5399 \t loss: 0.0292 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 5499 \t loss: 0.0105 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 5599 \t loss: 0.0042 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 5699 \t loss: 0.0166 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 5799 \t loss: 0.0218 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 5899 \t loss: 0.0405 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 5999 \t loss: 0.0173 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 6099 \t loss: 0.0128 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 6199 \t loss: 0.0147 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 6299 \t loss: 0.0040 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 6399 \t loss: 0.0406 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 6499 \t loss: 0.0377 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 6599 \t loss: 0.0578 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 6699 \t loss: 0.0599 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 6799 \t loss: 0.0041 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 6899 \t loss: 0.0042 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 6999 \t loss: 0.0319 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 7099 \t loss: 0.0112 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 7199 \t loss: 0.0520 \t train acc: 0.9062\n",
      "epoch: 11 \t batch_idx : 7299 \t loss: 0.0052 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 7399 \t loss: 0.0289 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 7499 \t loss: 0.0237 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 7599 \t loss: 0.0202 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 7699 \t loss: 0.0094 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 7799 \t loss: 0.0682 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 7899 \t loss: 0.0324 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 7999 \t loss: 0.0100 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 8099 \t loss: 0.0322 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 8199 \t loss: 0.0012 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 8299 \t loss: 0.0054 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 8399 \t loss: 0.0156 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 8499 \t loss: 0.0291 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 8599 \t loss: 0.0044 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 8699 \t loss: 0.0448 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 8799 \t loss: 0.0282 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 8899 \t loss: 0.0385 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 8999 \t loss: 0.0216 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 9099 \t loss: 0.0235 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 9199 \t loss: 0.0102 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 9299 \t loss: 0.0603 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 9399 \t loss: 0.0246 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 9499 \t loss: 0.0089 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 9599 \t loss: 0.0296 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 9699 \t loss: 0.0219 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 9799 \t loss: 0.0374 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 9899 \t loss: 0.0305 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 9999 \t loss: 0.0424 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 10099 \t loss: 0.0005 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 10199 \t loss: 0.0529 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 10299 \t loss: 0.0553 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 10399 \t loss: 0.0086 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 10499 \t loss: 0.0029 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 10599 \t loss: 0.0108 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 10699 \t loss: 0.0072 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 10799 \t loss: 0.0037 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 10899 \t loss: 0.0154 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 10999 \t loss: 0.0252 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 11099 \t loss: 0.0092 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 11199 \t loss: 0.0452 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 11299 \t loss: 0.0272 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 11399 \t loss: 0.0246 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 11499 \t loss: 0.0288 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 11599 \t loss: 0.0158 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 11699 \t loss: 0.0304 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 11799 \t loss: 0.0615 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 11899 \t loss: 0.0009 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 11999 \t loss: 0.0432 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 12099 \t loss: 0.0448 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 12199 \t loss: 0.0096 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 12299 \t loss: 0.0392 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 12399 \t loss: 0.0296 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 12499 \t loss: 0.0100 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 12599 \t loss: 0.0166 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 12699 \t loss: 0.0569 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 12799 \t loss: 0.0104 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 12899 \t loss: 0.0554 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 12999 \t loss: 0.0252 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 13099 \t loss: 0.0139 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 13199 \t loss: 0.0028 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 13299 \t loss: 0.0355 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 13399 \t loss: 0.0332 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 13499 \t loss: 0.0374 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 13599 \t loss: 0.0129 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 13699 \t loss: 0.0122 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 13799 \t loss: 0.0313 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 13899 \t loss: 0.0253 \t train acc: 0.9688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11 \t batch_idx : 13999 \t loss: 0.0778 \t train acc: 0.9062\n",
      "epoch: 11 \t batch_idx : 14099 \t loss: 0.0347 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 14199 \t loss: 0.0149 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 14299 \t loss: 0.0355 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 14399 \t loss: 0.0329 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 14499 \t loss: 0.0083 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 14599 \t loss: 0.0680 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 14699 \t loss: 0.0379 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 14799 \t loss: 0.0054 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 14899 \t loss: 0.0119 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 14999 \t loss: 0.0226 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 15099 \t loss: 0.0939 \t train acc: 0.8750\n",
      "epoch: 11 \t batch_idx : 15199 \t loss: 0.0471 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 15299 \t loss: 0.0029 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 15399 \t loss: 0.0319 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 15499 \t loss: 0.0127 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 15599 \t loss: 0.0713 \t train acc: 0.9062\n",
      "epoch: 11 \t batch_idx : 15699 \t loss: 0.0464 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 15799 \t loss: 0.0039 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 15899 \t loss: 0.0611 \t train acc: 0.9062\n",
      "epoch: 11 \t batch_idx : 15999 \t loss: 0.0321 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 16099 \t loss: 0.0236 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 16199 \t loss: 0.0220 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 16299 \t loss: 0.0265 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 16399 \t loss: 0.0252 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 16499 \t loss: 0.0259 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 16599 \t loss: 0.0254 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 16699 \t loss: 0.0379 \t train acc: 0.9375\n",
      "test acc: 0.7988\n"
     ]
    }
   ],
   "source": [
    "#train_iter\n",
    "for epoch in range(n_epoch):\n",
    "    for batch_idx, batch in enumerate(train_iter):\n",
    "        data = batch.Seqs\n",
    "        target = batch.Label\n",
    "        target = torch.sparse.torch.eye(n_class).index_select(dim=0, index=target.cpu().data)\n",
    "        target = target.to(DEVICE)\n",
    "        data = data.permute(1,0)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        # loss = -target*torch.log(out)-(1-target)*torch.log(1-out)\n",
    "        # loss = loss.sum(-1).mean()\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "        output = torch.sqrt(torch.sum(output*output, 2))\n",
    "        loss1 = target*F.relu(0.9-output)**2 + 0.5*(1-target)*F.relu(output-0.1)**2\n",
    "        loss1 = loss1.sum(dim=1).mean()\n",
    "        #loss2 = ((data-pred_img)**2).mean()\n",
    "        loss = loss1#+0.0005*loss2\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if (batch_idx+1) %100 == 0:\n",
    "            _,y_pre = torch.max(output,-1)\n",
    "            acc = torch.mean((torch.tensor(y_pre == batch.Label,dtype=torch.float)))\n",
    "            print('epoch: %d \\t batch_idx : %d \\t loss: %.4f \\t train acc: %.4f'\n",
    "                  %(epoch,batch_idx,loss,acc))\n",
    "    \n",
    "    val_accs = []\n",
    "    #val_iter\n",
    "    for batch_idx, batch in enumerate(val_iter):\n",
    "        data = batch.Seqs\n",
    "        target = batch.Label\n",
    "        target = torch.sparse.torch.eye(n_class).index_select(dim=0, index=target.cpu().data)\n",
    "        target = target.to(DEVICE)\n",
    "        data = data.permute(1,0)\n",
    "        out = model(data)\n",
    "        out = torch.sqrt(torch.sum(out*out, 2))\n",
    "        _,y_pre = torch.max(out,-1)\n",
    "        acc = torch.mean((torch.tensor(y_pre == batch.Label,dtype=torch.float)))\n",
    "        val_accs.append(acc)\n",
    "    acc =torch.mean(torch.stack(val_accs))\n",
    "    #acc = np.array(val_accs).mean()\n",
    "    if acc > best_val_acc:\n",
    "        print('val acc : %.4f > %.4f saving model'%(acc,best_val_acc))\n",
    "        torch.save(model.state_dict(), 'params.pkl')\n",
    "        best_val_acc = acc\n",
    "    print('test acc: %.4f'%(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HPP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-24T11:22:10.709351Z",
     "start_time": "2022-01-24T11:22:10.686317Z"
    }
   },
   "outputs": [],
   "source": [
    "df_hpp = pd.read_csv('../2019Bioinformatics_DeepMSPeptide/Datasets/HPP_validation_peptides.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-24T11:22:10.754130Z",
     "start_time": "2022-01-24T11:22:10.712255Z"
    }
   },
   "outputs": [],
   "source": [
    "aa_set = set(RE.keys())\n",
    "\n",
    "check_idx = []\n",
    "for idx, seq in enumerate(df_hpp.Peptide_seq.values):\n",
    "    for aa in seq:\n",
    "        if aa not in aa_set:\n",
    "            check_idx.append(idx)\n",
    "\n",
    "df_hpp = df_hpp.drop(check_idx, axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-24T11:22:10.785688Z",
     "start_time": "2022-01-24T11:22:10.757508Z"
    }
   },
   "outputs": [],
   "source": [
    "pe1 = df_hpp.loc[df_hpp['Class']=='PE1'].rename({'Peptide_seq':'Seqs', 'Class':'Label'}, axis=1).reset_index(drop=True)\n",
    "pe1 = pe1.loc[pe1.Seqs.apply(lambda x: len(x)<=45)].reset_index(drop=True)\n",
    "dmp = df_hpp.loc[df_hpp['Class']=='Detected_MPs'].rename({'Peptide_seq':'Seqs', 'Class':'Label'}, axis=1).reset_index(drop=True)\n",
    "dmp = dmp.loc[dmp.Seqs.apply(lambda x: len(x)<=45)].reset_index(drop=True)\n",
    "mp = df_hpp.loc[df_hpp['Class']=='MPs'].rename({'Peptide_seq':'Seqs', 'Class':'Label'}, axis=1).reset_index(drop=True)\n",
    "mp = mp.loc[mp.Seqs.apply(lambda x: len(x)<=45)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-24T11:22:10.790762Z",
     "start_time": "2022-01-24T11:22:10.787009Z"
    }
   },
   "outputs": [],
   "source": [
    "pe1['Label']=1\n",
    "dmp['Label']=1\n",
    "mp['Label']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-24T11:22:10.849722Z",
     "start_time": "2022-01-24T11:22:10.792075Z"
    }
   },
   "outputs": [],
   "source": [
    "pe1.to_csv('pe1.csv', index=False)\n",
    "dmp.to_csv('dmp.csv', index=False)\n",
    "mp.to_csv('mp.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-24T11:22:11.231051Z",
     "start_time": "2022-01-24T11:22:10.851059Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchtext import data, datasets  # .legacy\n",
    "\n",
    "# LABEL = data.Field(sequential=False, use_vocab=False)\n",
    "# TEXT = data.Field(sequential=True, tokenize=tokenizer,fix_length=45)\n",
    "\n",
    "train1,val1,test1 = data.TabularDataset.splits(\n",
    "        path='.', train='pe1.csv',validation='pe1.csv',test='pe1.csv', format='csv',skip_header=True,\n",
    "        fields=[('Seqs', TEXT), ('Label', LABEL)])\n",
    "# TEXT.build_vocab(train1,val1,test1)\n",
    "test_iter1 = data.BucketIterator(val1, batch_size=32, sort_key=lambda x: len(x.Seqs), \n",
    "                                 shuffle=True,device=DEVICE)\n",
    "\n",
    "train2,val2,test2 = data.TabularDataset.splits(\n",
    "        path='.', train='dmp.csv',validation='dmp.csv',test='dmp.csv', format='csv',skip_header=True,\n",
    "        fields=[('Seqs', TEXT), ('Label', LABEL)])\n",
    "# TEXT.build_vocab(train2,val2,test2)\n",
    "test_iter2 = data.BucketIterator(val2, batch_size=32, sort_key=lambda x: len(x.Seqs), \n",
    "                                 shuffle=True,device=DEVICE)\n",
    "\n",
    "\n",
    "train3,val3,test3 = data.TabularDataset.splits(\n",
    "        path='.', train='mp.csv',validation='mp.csv',test='mp.csv', format='csv',skip_header=True,\n",
    "        fields=[('Seqs', TEXT), ('Label', LABEL)])\n",
    "# TEXT.build_vocab(train3, val3,test3)\n",
    "test_iter3 = data.BucketIterator(val3, batch_size=32, sort_key=lambda x: len(x.Seqs), \n",
    "                                 shuffle=True,device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-24T11:22:11.270876Z",
     "start_time": "2022-01-24T11:22:11.232344Z"
    },
    "id": "zMKE8VD9Nc34"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CapsuleNet(\n",
       "  (embedding): Embedding(22, 20)\n",
       "  (lstm): LSTM(20, 40, batch_first=True)\n",
       "  (conv1): Conv2d(1, 256, kernel_size=(9, 9), stride=(1, 1))\n",
       "  (conv3): Conv2d(1, 256, kernel_size=(8, 8), stride=(1, 1))\n",
       "  (cbamBlock): CBAMBlock(\n",
       "    (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "    (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
       "    (channel_excitation): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=16, bias=False)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Linear(in_features=16, out_features=256, bias=False)\n",
       "    )\n",
       "    (sigmoid): Sigmoid()\n",
       "    (spatial_excitation): Sequential(\n",
       "      (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (conv2): Conv2d(256, 256, kernel_size=(9, 9), stride=(2, 2))\n",
       "  (conv4): Conv2d(256, 256, kernel_size=(8, 8), stride=(2, 2))\n",
       "  (capsule): Capsule()\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.load_state_dict(torch.load('params.pkl'))#, map_location='cpu'))\n",
    "model.eval()\n",
    "model\n",
    "\n",
    "# a = metrics.confusion_matrix(\n",
    "#     all_true,   # array, Gound true (correct) target values\n",
    "#     all_pred,  # array, Estimated targets as returned by a classifier\n",
    "#     labels=None,  # array, List of labels to index the matrix.\n",
    "#     sample_weight=None  # array-like of shape = [n_samples], Optional sample weights\n",
    "# )\n",
    "# print(a)\n",
    "# print(metrics.accuracy_score(all_true,all_pred))\n",
    "# print(metrics.roc_auc_score(all_true,all_p))\n",
    "# print(metrics.recall_score(all_true,all_pred))\n",
    "# print(metrics.f1_score(all_true,all_pred))\n",
    "# FN = a[1][0]\n",
    "# FP = a[0][1]\n",
    "# TN = a[0][0]\n",
    "# TP = a[1][1]\n",
    "# print(FN)\n",
    "# print(FP)\n",
    "# print(TN)\n",
    "# print(TP)\n",
    "# print((FN+TP)/(FN+FP+TN+TP))\n",
    "# print(\"se\",TP/ (TP+ FN))\n",
    "# print(\"sp\",TN / (FP + TN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-24T11:23:06.840734Z",
     "start_time": "2022-01-24T11:22:11.271945Z"
    },
    "id": "kPqXuJZcNhAG",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9482909728308502\n"
     ]
    }
   ],
   "source": [
    "all_pred1 = []\n",
    "all_true1 = []\n",
    "all_p1 = []\n",
    "for batch_idx, batch in enumerate(test_iter1):\n",
    "        data = batch.Seqs\n",
    "        target = batch.Label\n",
    "        target = torch.sparse.torch.eye(n_class).index_select(dim=0, index=target.cpu().data)\n",
    "        target = target.to(DEVICE)\n",
    "        data = data.permute(1,0)\n",
    "        try:\n",
    "            out = model(data)\n",
    "        except:\n",
    "            print(data)\n",
    "        out = torch.sqrt(torch.sum(out*out, 2))\n",
    "        #out1 = F.softmax(out,-1)\n",
    "        # print(out)\n",
    "        #print(out1)\n",
    "        out1 = out[:,1]\n",
    "        # print(out)\n",
    "        _,y_pre = torch.max(out,-1)\n",
    "        \n",
    "        all_p1.extend(list(out1.cpu().detach().numpy()))\n",
    "        all_pred1.extend(list(y_pre.cpu().detach().numpy()))\n",
    "        all_true1.extend(list(batch.Label.cpu().detach().numpy()))\n",
    "print(metrics.accuracy_score(all_true1,all_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-24T11:24:05.374592Z",
     "start_time": "2022-01-24T11:23:06.842669Z"
    },
    "id": "-l9ml1kYNhtC"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bis/miniconda3/envs/torch/lib/python3.6/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9612160640560491\n"
     ]
    }
   ],
   "source": [
    "all_pred2 = []\n",
    "all_true2 = []\n",
    "all_p2 = []\n",
    "for batch_idx, batch in enumerate(test_iter2):\n",
    "        data = batch.Seqs\n",
    "        target = batch.Label\n",
    "        target = torch.sparse.torch.eye(n_class).index_select(dim=0, index=target.cpu().data)\n",
    "        target = target.to(DEVICE)\n",
    "        data = data.permute(1,0)\n",
    "        out = model(data)\n",
    "        out = torch.sqrt(torch.sum(out*out, 2))\n",
    "        #out1 = F.softmax(out,-1)\n",
    "        # print(out)\n",
    "        out1 = out[:,1]\n",
    "        #print(out1)\n",
    "        # print(out)\n",
    "        _,y_pre = torch.max(out,-1)\n",
    "        \n",
    "        all_p2.extend(list(out1.cpu().detach().numpy()))\n",
    "        all_pred2.extend(list(y_pre.cpu().detach().numpy()))\n",
    "        all_true2.extend(list(batch.Label.cpu().detach().numpy()))\n",
    "print(metrics.accuracy_score(all_true2,all_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-24T11:24:56.530490Z",
     "start_time": "2022-01-24T11:24:05.376134Z"
    },
    "id": "f-7C0a_FNlU8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bis/miniconda3/envs/torch/lib/python3.6/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5587710876422988\n"
     ]
    }
   ],
   "source": [
    "all_pred3 = []\n",
    "all_true3 = []\n",
    "all_p3 = []\n",
    "for batch_idx, batch in enumerate(test_iter3):\n",
    "        data = batch.Seqs\n",
    "        target = batch.Label\n",
    "        target = torch.sparse.torch.eye(n_class).index_select(dim=0, index=target.cpu().data)\n",
    "        target = target.to(DEVICE)\n",
    "        data = data.permute(1,0)\n",
    "        out = model(data)\n",
    "        out = torch.sqrt(torch.sum(out*out, 2))\n",
    "        #out1 = F.softmax(out,-1)\n",
    "        # print(out)\n",
    "        out1 = out[:,1]\n",
    "        # print(out)\n",
    "        _,y_pre = torch.max(out,-1)\n",
    "        \n",
    "        all_p3.extend(list(out1.cpu().detach().numpy()))\n",
    "        all_pred3.extend(list(y_pre.cpu().detach().numpy()))\n",
    "        all_true3.extend(list(batch.Label.cpu().detach().numpy()))\n",
    "print(metrics.accuracy_score(all_true3,all_pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-24T11:24:56.873012Z",
     "start_time": "2022-01-24T11:24:56.532812Z"
    },
    "id": "5piTcRiANrup"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(15, 8))\n",
    "\n",
    "#all_data = [np.random.normal(0, std, 100) for std in range(6, 10)]\n",
    "#\n",
    "# print(all_p1)\n",
    "axes.violinplot([all_p1,all_p2,all_p3],\n",
    "                   showmeans=True,\n",
    "                   showmedians=True\n",
    "                   )\n",
    "\n",
    "axes.set_title('violin plot')\n",
    "\n",
    "# adding horizontal grid lines\n",
    "\n",
    "axes.yaxis.grid(True)\n",
    "#axes.set_xticks([y + 1 for y in range(len(all_data))], )\n",
    "axes.set_xlabel('xlabel')\n",
    "axes.set_ylabel('ylabel')\n",
    "\n",
    "plt.setp(axes, xticks=[y + 1 for y in range(len([all_p1,all_p2,all_p3]))],\n",
    "         xticklabels=['PF1', 'Detected MPs', 'Current MPs'],\n",
    "         )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-24T11:24:56.890202Z",
     "start_time": "2022-01-24T11:24:56.874337Z"
    }
   },
   "outputs": [],
   "source": [
    "df_pe1 = pd.DataFrame([['PE1', _] for _ in all_p1],\n",
    "                     columns=['class', 'prob'])\n",
    "df_dmp = pd.DataFrame([['Detected_MPs', _] for _ in all_p2],\n",
    "                     columns=['class', 'prob'])\n",
    "df_mp = pd.DataFrame([['MPs', _] for _ in all_p3],\n",
    "                     columns=['class', 'prob'])\n",
    "df_sns = pd.concat([df_pe1, df_dmp, df_mp], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-24T11:24:56.912654Z",
     "start_time": "2022-01-24T11:24:56.891342Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-24T11:24:57.104285Z",
     "start_time": "2022-01-24T11:24:56.913690Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Peptide Detectability')\n",
    "plt.ylabel('Detectability')\n",
    "plt.xlabel('Type')\n",
    "\n",
    "sns.violinplot(data=df_sns, x='class', y='prob', order=['PE1', 'Detected_MPs', 'MPs'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "test.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
