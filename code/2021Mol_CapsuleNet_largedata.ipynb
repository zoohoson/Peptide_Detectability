{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-23T16:01:46.423568Z",
     "start_time": "2022-01-23T16:01:45.628523Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-23T16:03:40.749747Z",
     "start_time": "2022-01-23T16:03:37.788783Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19, 21)\n"
     ]
    }
   ],
   "source": [
    "df_aaindex = pd.read_csv('data/aaindex/df_aaindex19.csv')\n",
    "print(df_aaindex.shape)\n",
    "df_aaindex.head(1)\n",
    "tmp = df_aaindex.drop('Unnamed: 0',axis=1).T\n",
    "aa2val = dict()\n",
    "for aa, val in zip(tmp.index, tmp.values):\n",
    "    aa2val[aa]=val\n",
    "\n",
    "train = pd.read_csv('compareModel/2021MDPI_CapsNet/train.csv')\n",
    "val = pd.read_csv('compareModel/2021MDPI_CapsNet/val.csv')\n",
    "test = pd.read_csv('compareModel/2021MDPI_CapsNet/test.csv')\n",
    "\n",
    "# to_csv format\n",
    "train_save = train[['PEP', 'ID']].rename({'PEP':'Seqs',\n",
    "                                 'ID':'Label'}, axis=1)\n",
    "train_save.to_csv('compareModel/2021MDPI_CapsNet/train_.csv', index=False)\n",
    "\n",
    "val_save = val[['PEP', 'ID']].rename({'PEP':'Seqs',\n",
    "                                 'ID':'Label'}, axis=1)\n",
    "val_save.to_csv('compareModel/2021MDPI_CapsNet/val_.csv', index=False)\n",
    "\n",
    "test_save = test[['PEP', 'ID']].rename({'PEP':'Seqs',\n",
    "                                 'ID':'Label'}, axis=1)\n",
    "test_save.to_csv('compareModel/2021MDPI_CapsNet/test_.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-16T09:41:33.458639Z",
     "start_time": "2022-01-16T09:41:32.654569Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torch\n",
    "import sys\n",
    "from torchtext import data, datasets  # .legacy\n",
    "from torchtext.vocab import Vectors\n",
    "from torch.nn import init\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from torch.autograd import Variable\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-16T09:41:33.628243Z",
     "start_time": "2022-01-16T09:41:33.626328Z"
    }
   },
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser(description='test')\n",
    "\n",
    "# parser.add_argument('--test',  default=\"test_sequence.csv\", help='Location of test data')\n",
    "# parser.add_argument('--model', default=\"params.pkl\", help='Location of model')\n",
    "# parser.add_argument('--result', default=\"result.txt\", help='Location of result')\n",
    "# args = parser.parse_args()\n",
    "# print(\"data: \"+args.test)\n",
    "# print(\"model: \"+args.model)\n",
    "# print(\"result: \"+args.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-16T09:41:34.906875Z",
     "start_time": "2022-01-16T09:41:34.869692Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-16T09:41:36.032099Z",
     "start_time": "2022-01-16T09:41:36.027594Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bis/miniconda3/envs/torch/lib/python3.6/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenizer(text): # create a tokenizer function\n",
    "    \"\"\"\n",
    "    定义分词操作\n",
    "    \"\"\"\n",
    "    return list(text)\n",
    "\n",
    "\"\"\"\n",
    "field在默认的情况下都期望一个输入是一组单词的序列，并且将单词映射成整数。\n",
    "这个映射被称为vocab。如果一个field已经被数字化了并且不需要被序列化，\n",
    "可以将参数设置为use_vocab=False以及sequential=False。\n",
    "\"\"\"\n",
    "LABEL = data.Field(sequential=False, use_vocab=False)\n",
    "TEXT = data.Field(sequential=True, tokenize=tokenizer ,fix_length=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-16T09:41:45.136210Z",
     "start_time": "2022-01-16T09:41:39.071478Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bis/miniconda3/envs/torch/lib/python3.6/site-packages/torchtext/data/example.py:68: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
      "/home/bis/miniconda3/envs/torch/lib/python3.6/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "train,val,test = data.TabularDataset.splits(\n",
    "        path='compareModel/2021MDPI_CapsNet/', \n",
    "    train='train.csv',\n",
    "    validation='val.csv',\n",
    "    test='test.csv',\n",
    "    format='csv',\n",
    "    skip_header=True,\n",
    "    fields=[('Seqs', TEXT), ('Label', LABEL)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-16T09:41:46.129907Z",
     "start_time": "2022-01-16T09:41:46.126521Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchtext.data.example.Example object at 0x7f2e83f947b8>\n",
      "dict_keys(['Seqs', 'Label'])\n",
      "['H', 'H', 'C', 'A', 'R', 'Q', 'R', 'L', 'R'] 0\n"
     ]
    }
   ],
   "source": [
    "print(train[5])\n",
    "print(train[5].__dict__.keys())\n",
    "print(train[5].Seqs,train[5].Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-16T09:41:48.738473Z",
     "start_time": "2022-01-16T09:41:46.514939Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bis/miniconda3/envs/torch/lib/python3.6/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(train,val,test)\n",
    "train_iter = data.BucketIterator(train, batch_size=32, sort_key=lambda x: len(x.Seqs), \n",
    "                                 shuffle=True,device=DEVICE)\n",
    "\n",
    "val_iter = data.BucketIterator(val, batch_size=32, sort_key=lambda x: len(x.Seqs), \n",
    "                                 shuffle=True,device=DEVICE)\n",
    "\n",
    "test_iter = data.BucketIterator(val, batch_size=32, sort_key=lambda x: len(x.Seqs), \n",
    "                                 shuffle=True,device=DEVICE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-16T09:41:48.819146Z",
     "start_time": "2022-01-16T09:41:48.740241Z"
    }
   },
   "outputs": [],
   "source": [
    "epsilon = 0.00000001\n",
    "def squash(x):\n",
    "    # not concern batch_size, maybe rewrite\n",
    "    s_squared_norm = torch.sum(x*x,1,keepdim=True) + epsilon\n",
    "    scale = torch.sqrt(s_squared_norm)/(1. + s_squared_norm)\n",
    "    # out = (batch_size,1,10)*(batch_size,16,10) = (batch_size,16,10)\n",
    "    out = scale * x\n",
    "    return out\n",
    "class Capsule(nn.Module):\n",
    "\n",
    "    def __init__(self, in_units,in_channels, num_capsule, dim_capsule, routings=3, **kwargs):\n",
    "        super(Capsule, self).__init__(**kwargs)\n",
    "        self.in_units = in_units\n",
    "        self.in_channels = in_channels\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        # (in_units,10,128,16)\n",
    "        self.W = nn.Parameter((torch.randn(self.in_units,self.num_capsule,self.in_channels, self.dim_capsule)))\n",
    "\n",
    "    def forward(self, u_vecs):\n",
    "        u_vecs = u_vecs.permute(0,2,1)\n",
    "        u_vecs = u_vecs.unsqueeze(2)\n",
    "        u_vecs = u_vecs.unsqueeze(2)\n",
    "\n",
    "        \n",
    "        # (batch_size,in_units,1,1,in_channels)*(in_units,10,in_channels,16) = (batch_size,in_units,10,1,16)\n",
    "        u_hat_vecs = torch.matmul(u_vecs,self.W)\n",
    "        # (batch_size,in_units,10,16)\n",
    "        u_hat_vecs = u_hat_vecs.permute(0,1,2,4,3).squeeze(4)\n",
    "        \n",
    "        # (batch_size,10,in_units,16)\n",
    "        u_hat_vecs2 = u_hat_vecs.permute(0,2,1,3)\n",
    "    \n",
    "        # (batch_size,10,1,in_units)\n",
    "        b = torch.zeros(u_hat_vecs.size(0),self.num_capsule,1,self.in_units,device=DEVICE)\n",
    "        for i in range(self.routings):\n",
    "            # (batch_size,10,1,in_units)\n",
    "            c = F.softmax(b,-1)\n",
    "            # s = (batch_size,10,1,in_units)*(batch_size,10,in_units,16) = (batch_size,10,1,16)\n",
    "            s = torch.matmul(c,u_hat_vecs2)\n",
    "            # (batch_size,16,10)\n",
    "            s = s.permute(0,3,1,2).squeeze(3)\n",
    "            # (batch_size,16,10)\n",
    "            v = squash(s)\n",
    "            # here\n",
    "            # (batch_size,10,16,1)\n",
    "            v = v.permute(0,2,1).unsqueeze(3)\n",
    "            # (batch_size,10,in_units,16)*(batch_size,10,16,1) = (batch_size,10,in_units,1)\n",
    "            sim = torch.matmul(u_hat_vecs2,v)\n",
    "            # (batch_size,10,1,in_units)\n",
    "            sim = sim.permute(0,1,3,2)\n",
    "            b = b+sim\n",
    "        # (batch_size,16,10)\n",
    "        return v.permute(0,2,1,3).squeeze(3)\n",
    "\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "    \n",
    "len_vocab = len(TEXT.vocab)\n",
    "embed_size=20\n",
    "n_class=2\n",
    "n_hidden =32\n",
    "\n",
    "energy = [\n",
    "[-1.65,  -2.83, 1.16,\t1.80,\t-3.73,\t-0.41,\t1.90,\t-3.69,\t0.49,\t-3.01,\t-2.08,\t0.66,\t1.54,\t1.20,\t0.98, -0.08,  0.46, -2.31,\t0.32,\t-4.62],\n",
    "[-2.83,\t-39.58,\t-0.82,\t-0.53,\t-3.07,\t-2.96,\t-4.98,\t0.34,\t-1.38,\t-2.15,\t1.43,\t-4.18,\t-2.13,\t-2.91,\t-0.41,\t-2.33,\t-1.84,\t-0.16,\t4.26,\t-4.46],\n",
    "[1.16,\t-0.82,\t0.84,\t1.97,\t-0.92,\t0.88,\t-1.07,\t0.68,\t-1.93,\t0.23,\t0.61,\t0.32,\t3.31,\t2.67,\t-2.02,\t0.91,\t-0.65,\t0.94,\t-0.71,\t0.90],\n",
    "[1.80,\t-0.53,\t1.97,\t1.45,\t0.94,\t1.31,\t0.61,\t1.30,\t-2.51,\t1.14,\t2.53,\t0.20,\t1.44,\t0.10,\t-3.13,\t0.81,\t1.54,\t0.12,\t-1.07,\t1.29],\n",
    "[-3.73,\t-3.07,\t-0.92,\t0.94,\t-11.25,\t0.35,\t-3.57,\t-5.88,\t-0.82,\t-8.59,\t-5.34,\t0.73,\t0.32,\t0.77,\t-0.40,\t-2.22,\t0.11,\t-7.05,\t-7.09,\t-8.80],\n",
    "[-0.41,\t-2.96,\t0.88,\t1.31,\t0.35,\t-0.20,\t1.09,\t-0.65,\t-0.16,\t-0.55,\t-0.52,\t-0.32,\t2.25,\t1.11,\t0.84,\t0.71,\t0.59,\t-0.38,\t1.69,\t-1.90],\n",
    "[1.90,\t-4.98,\t-1.07,\t0.61,\t-3.57,\t1.09,\t1.97,\t-0.71,\t2.89,\t-0.86,\t-0.75,\t1.84,\t0.35,\t2.64,\t2.05,\t0.82,\t-0.01,\t0.27,\t-7.58,\t-3.20],\n",
    "[-3.69,\t0.34,\t0.68,\t1.30,\t-5.88,\t-0.65,\t-0.71,\t-6.74,\t-0.01,\t-9.01,\t-3.62,\t-0.07,\t0.12,\t-0.18,\t0.19,\t-0.15,\t0.63,\t-6.54,\t-3.78,\t-5.26],\n",
    "[0.49,\t-1.38,\t-1.93,\t-2.51,\t-0.82,\t-0.16,\t2.89,\t-0.01,\t1.24,\t0.49,\t1.61,\t1.12,\t0.51,\t0.43,\t2.34,\t0.19,\t-1.11,\t0.19,\t0.02,\t-1.19],\n",
    "[-3.01,\t-2.15,\t0.23,\t1.14,\t-8.59,\t-0.55,\t-0.86,\t-9.01,\t0.49,\t-6.37,\t-2.88,\t0.97,\t1.81,\t-0.58,\t-0.60,\t-0.41,\t0.72,\t-5.43,\t-8.31,\t-4.90],\n",
    "[-2.08,\t1.43,\t0.61,\t2.53,\t-5.34,\t-0.52,\t-0.75,\t-3.62,\t1.61,\t-2.88,\t-6.49,\t0.21,\t0.75,\t1.90,\t2.09,\t1.39,\t0.63,\t-2.59,\t-6.88,\t-9.73],\n",
    "[0.66,\t-4.18,\t0.32,\t0.20,\t0.73,\t-0.32,\t1.84,\t-0.07,\t1.12,\t0.97,\t0.21,\t0.61,\t1.15,\t1.28,\t1.08,\t0.29,\t0.46,\t0.93,\t-0.74,\t0.93],\n",
    "[1.54,\t-2.13,\t3.31,\t1.44,\t0.32,\t2.25,\t0.35,\t0.12,\t0.51,\t1.81,\t0.75,\t1.15,\t-0.42,\t2.97,\t1.06,\t1.12,\t1.65,\t0.38,\t-2.06,\t-2.09],\n",
    "[1.20,\t-2.91,\t2.67,\t0.10,\t0.77,\t1.11,\t2.64,\t-0.18,\t0.43,\t-0.58,\t1.90,\t1.28,\t2.97,\t-1.54,\t0.91,\t0.85,\t-0.07,\t-1.91,\t-0.76,\t0.01],\n",
    "[0.98,\t-0.41,\t-2.02,\t-3.13,\t-0.40,\t0.84,\t2.05,\t0.19,\t2.34,\t-0.60,\t2.09,\t1.08,\t1.06,\t0.91,\t0.21,\t0.95,\t0.98,\t0.08,\t-5.89,\t0.36],\n",
    "[-0.08,\t-2.33,\t0.91,\t0.81,\t-2.22,\t0.71,\t0.82,\t-0.15,\t0.19,\t-0.41,\t1.39,\t0.29,\t1.12,\t0.85,\t0.95,\t-0.48,\t-0.06,\t0.13,\t-3.03,\t-0.82],\n",
    "[0.46,\t-1.84,\t-0.65,\t1.54,\t0.11,\t0.59,\t-0.01,\t0.63,\t-1.11,\t0.72,\t0.63,\t0.46,\t1.65,\t-0.07,\t0.98,\t-0.06,\t-0.96,\t1.14,\t-0.65,\t-0.37],\n",
    "[-2.31,\t-0.16,\t0.94,\t0.12,\t-7.05,\t-0.38,\t0.27,\t-6.54,\t0.19,\t-5.43,\t-2.59,\t0.93,\t0.38,\t-1.91,\t0.08,\t0.13,\t1.14,\t-4.82,\t-2.13,\t-3.59],\n",
    "[0.32,\t4.26,\t-0.71,\t-1.07,\t-7.09,\t1.69,\t-7.58,\t-3.78,\t0.02,\t-8.31,\t-6.88,\t-0.74,\t-2.06,\t-0.76,\t-5.89,\t-3.03,\t-0.65,\t-2.13,\t-1.73,\t-12.39],\n",
    "[-4.62,\t-4.46,\t0.90,\t1.29,\t-8.80,\t-1.90,\t-3.20,\t-5.26,\t-1.19,\t-4.90,\t-9.73,\t0.93,\t-2.09,\t0.01,\t0.36,\t-0.82,\t-0.37,\t-3.59,\t-12.39,\t-2.68],\n",
    "]\n",
    "\n",
    "physicochemical = [\n",
    "    [-0.4, -0.5, 15, 8.1, 0.046, 0.67, 1.28, 0.3, 0, 0.687, 115, 0.28, 154.330012, 27.5, 1.181, 0.0072,0,0,0,0],\n",
    "    [0.17, -1, 47, 5.5, 0.128, 0.38, 1.77, 0.9, 2.75, 0.263, 135, 0.28, 219.789, 44.6, 1.461, -0.037,0,0,0,0],\n",
    "    [-1.31, 3.0, 59, 13.0, 0.105, -1.2, 1.6, -0.6, 1.38, 0.632, 150, 0.21, 194.910002, 40.0, 1.587, 0.0238,0,0,0,0],\n",
    "    [-1.22, 3.0, 73, 12.3, 0.151, -0.76, 1.56, -0.7, 0.92, 0.669, 190, 0.33, 223.160, 62, 1.862, 0.0068,0,0,0,0],\n",
    "    [1.92, -2.5, 91, 5.2, 0.29, 2.3, 2.94, 0.5, 0, 0.577, 210, 2.18, 204.7, 115.5, 2.228, 0.0376,0,0,0,0],\n",
    "    [-0.67, 0, 1, 9, 0, 0, 0, 0.3, 0.74, 0.67, 75, 0.18, 127.9, 0, 0.881, 0.179,0,0,0,0],\n",
    "    [-0.64, -0.5, 82, 10.4, 0.23, 0.64, 2.99, -0.1, 0.58, 0.594, 195, 0.21, 242.539, 79, 2.025, -0.011,0,0,0,0],\n",
    "    [1.25, -1.5, 57, 5.2, 0.186, 1.9, 4.19, 0.7, 0, 0.564, 175, 0.82, 233.210, 93.5, 1.81, 0.0216,0,0,0,0],\n",
    "    [-0.67, 3, 73, 11.3, 0.219, -0.57, 1.89, -1.8, 0.33, 0.407, 200, 0.09, 300.459, 100, 2.258, 0.0177,0,0,0,0],\n",
    "    [1.22, -1.8, 57, 4.9, 0.186, 1.9, 2.59, 0.5, 0, 0.541, 170, 1, 232.3, 93.5, 1.931, 0.0517,0,0,0,0],\n",
    "    [1.02, -1.3, 75, 5.7, 0.0221, 2.4, 2.35, 0.4, 0, 0.328, 185, 0.74, 202.699, 94.1, 2.034, 0.0027,0,0,0,0],\n",
    "    [-0.92, 0.2, 58, 11.6, 0.134, -0.61, 1.6, -0.5, 1.33, 0.489, 160, 0.25, 207.899, 58.7, 1.655, 0.0054,0,0,0,0],\n",
    "    [-0.49, 0, 42, 8.0, 0.131, 102, 2.67, -0.3, 0.39, 0.600, 145, 0.39, 179.929, 41.9, 1.468, 0.239,0,0,0,0],\n",
    "    [-0.91, 0.2, 72, 10.5, 0.180, -0.22, 1.56, -0.7, 0.9, 0.527, 183, 0.35, 235.509, 80.7, 1.932, 0.0692,0,0,0,0],\n",
    "    [-0.59, 3, 101, 10.5, 0.291, -2.10, 2.34, -1.4, 0.64, 0.591, 225, 0.1, 341.0, 105, 2.56, 0.0436,0,0,0,0],\n",
    "    [-0.55, 0.3, 31, 9.2, 0.062, 0.01, 1.31, -0.1, 1.41, 0.693, 116, 0.12, 174.059, 29.3, 1.298, 0.0043,0,0,0,0],\n",
    "    [-0.28, -0.4, 45, 8.6, 0.108, 0.52, 3.03, -0.2, 0.71, 0.713, 142, 0.21, 205.5, 51.3, 1.525, 0.034,0,0,0,0],\n",
    "    [0.91, -1.5, 43, 5.9, 0.14, 1.5, 3.67, 0.6, 0, 0.529, 157, 0.6, 207, 71.5, 1.645, 0.057,0,0,0,0],\n",
    "    [0.5, -3.4, 130, 5.4, 0.409, 2.6, 3.21, 0.3, 0.12, 0.632, 258, 5.7, 237, 145.5, 2.663, 0.058,0,0,0,0],\n",
    "    [1.67, -2.3, 107, 6.2, 0.298, 1.6, 2.94, -0.4, 0.21, 0.493, 234, 1.26, 229.14, 117.3, 2.368, 0.0236,0,0,0,0]\n",
    "]\n",
    "\n",
    "RE = {'A':0,'C':1,'D':2,'E':3,'F':4,'G':5,'H':6,'I':7,'K':8,'L':9,'M':10,'N':11,'P':12,'Q':13,'R':14,'S':15,'T':16,'V':17,'W':18,'Y':19}\n",
    "def RECMEncoding(inpStr):\n",
    "  RECMT=[]\n",
    "  for x in inpStr:\n",
    "    if x in RE:\n",
    "      oneTi = energy[RE.get(x)]\n",
    "      RECMT.append(oneTi)\n",
    "  return RECMT\n",
    "    \n",
    "def RECMcompositionEncoding(inpStr):\n",
    "  RECMcomposition=[]\n",
    "  countNum = {'A':0,'C':0,'D':0,'E':0,'F':0,'G':0,'H':0,'I':0,'K':0,'L':0,'M':0,'N':0,'P':0,'Q':0,'R':0,'S':0,'T':0,'V':0,'W':0,'Y':0}\n",
    "  for i in inpStr:\n",
    "    if i in countNum:\n",
    "      value = countNum.get(i)+1\n",
    "      countNum[i] = value\n",
    "  for i in countNum:\n",
    "    oneTi = np.array(energy[RE.get(i)])*int(countNum.get(i))\n",
    "    # a = GetPseRECM(RECMEncoding(i))\n",
    "    # i = np.concatenate((oneTi,a),0)\n",
    "    RECMcomposition.append(oneTi)\n",
    "  RECMcomposition = np.array(RECMcomposition)\n",
    "  return RECMcomposition\n",
    "      \n",
    "def GetPseRECM(RECMT):\n",
    "  feature =[]\n",
    "  legth = 0\n",
    "  r = 3\n",
    "  legth = 20+20*(r-1)\n",
    "  # 取平均特征\n",
    "  for j in range(20):\n",
    "    averageColumn = 0\n",
    "    for i in range(len(RECMT)):\n",
    "      averageColumn = averageColumn + RECMT[i][j]\n",
    "    averageColumn = averageColumn/len(RECMT)\n",
    "    feature.append(averageColumn)\n",
    "  for k in range(1,r):\n",
    "    for j in range(20):\n",
    "      dist = 0\n",
    "      for i in range(len(RECMT)-k):\n",
    "        dist = dist +pow((RECMT[i][j]-RECMT[i+k][j]),2)\n",
    "      dist = dist/(len(RECMT)-k)\n",
    "      feature.append(dist)\n",
    "  feature = np.array(feature)\n",
    "  return feature\n",
    "\n",
    "def residueRatio(inpStr):\n",
    "    feature = []\n",
    "    countNum = {'A':0,'C':0,'D':0,'E':0,'F':0,'G':0,'H':0,'I':0,'K':0,'L':0,'M':0,'N':0,'P':0,'Q':0,'R':0,'S':0,'T':0,'V':0,'W':0,'Y':0}\n",
    "    total = 0\n",
    "    for i in inpStr:\n",
    "        total = total+1\n",
    "        if i in countNum:\n",
    "            value = countNum.get(i)+1\n",
    "            countNum[i] = value\n",
    "    for i in countNum:\n",
    "        oneResidueRatio = countNum.get(i)#/total\n",
    "        feature.append(oneResidueRatio)\n",
    "    feature = np.array(feature)\n",
    "    return feature\n",
    "\n",
    "\n",
    "\n",
    "def dipeptideRatio(inpStr):\n",
    "    # print(inpStr)\n",
    "    dipeptideFeature = np.zeros((20,20))\n",
    "    total = 0\n",
    "    for i in range(len(inpStr)-1):\n",
    "        total = total+1\n",
    "        x = RE.get(inpStr[i])\n",
    "        y = RE.get(inpStr[i+1])\n",
    "        # print(x)\n",
    "        # print(y)\n",
    "        dipeptideFeature[x][y] = dipeptideFeature[x][y] +1\n",
    "    # dipeptideFeature = dipeptideFeature/total\n",
    "    return dipeptideFeature\n",
    "\n",
    "\n",
    "def physicochemicalFeature(inpStr,fixlength):\n",
    "    pfeature=[]\n",
    "    Slength = 0\n",
    "    fix = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "    for x in inpStr:\n",
    "        if x in RE:\n",
    "            Slength = Slength+1\n",
    "            oneTi = physicochemical[RE.get(x)]\n",
    "            pfeature.append(oneTi)\n",
    "    fixlength = fixlength-Slength\n",
    "    for i in range(fixlength):\n",
    "        pfeature.append(fix)\n",
    "    pfeature = np.array(pfeature)\n",
    "    return pfeature\n",
    "\n",
    "def featureGenera(t):\n",
    "  flag =0 \n",
    "  for i in t:\n",
    "    protein = ''   \n",
    "    flag = flag+1\n",
    "    for j in i:\n",
    "      if j!=1:\n",
    "        a = TEXT.vocab.itos[j]\n",
    "        protein = protein+a\n",
    "    # print(protein)\n",
    "    featureOne = RECMcompositionEncoding(protein)\n",
    "    # print(featureOne.shape)\n",
    "    featureTwo = GetPseRECM(RECMEncoding(protein))\n",
    "    featureThree = dipeptideRatio(protein)\n",
    "    featureFour = residueRatio(protein)\n",
    "    featureFive = physicochemicalFeature(protein,45)\n",
    "    # print(featureTwo.shape)\n",
    "    featureTwo =featureTwo.reshape(3,20)\n",
    "    featureFour = featureFour.reshape(1,20)\n",
    "    featureOne = torch.from_numpy(featureOne)\n",
    "    featureTwo = torch.from_numpy(featureTwo)\n",
    "    featureThree = torch.from_numpy(featureThree)\n",
    "    featureFour = torch.from_numpy(featureFour)\n",
    "    featureFive = torch.from_numpy(featureFive)\n",
    "    featureThree.type_as(featureTwo)\n",
    "    featureFour.type_as(featureTwo)\n",
    "    featureFive.type_as(featureTwo)\n",
    "    # print(featureOne.shape)\n",
    "    # print(featureTwo.shape)\n",
    "    # print(featureThree.shape)\n",
    "    # print(featureFour.shape)\n",
    "    feature1 = torch.cat((featureOne,featureTwo),0)\n",
    "    # print(feature1.shape)\n",
    "    feature2 = torch.cat((featureThree,featureFour.type_as(featureThree)),0)\n",
    "    # # print(feature2.shape)\n",
    "    feature3 = torch.cat((feature1,feature2),0)\n",
    "    feature = torch.cat((feature3,featureFive),0)\n",
    "\n",
    "    # print(feature.shape)\n",
    "    # print(feature.shape)\n",
    "    if(flag==1):\n",
    "      # print(feature.shape)\n",
    "      feature = feature.unsqueeze(0)\n",
    "      temp = feature\n",
    "    if(flag!=1):\n",
    "      # print(feature.shape)\n",
    "      feature = feature.unsqueeze(0)\n",
    "      temp = torch.cat((temp,feature),0)\n",
    "  # print(temp.shape)\n",
    "  return temp\n",
    "\n",
    "\n",
    "class Enet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Enet, self).__init__()\n",
    "        self.embedding = nn.Embedding(len_vocab,embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size,64,batch_first=True)#,bidirectional=True)\n",
    "        self.conv = nn.Conv1d(embed_size,32,3)\n",
    "        self.pool = nn.MaxPool1d(32)\n",
    "        self.linear = nn.Linear(64,n_class)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size,seq_num = x.shape\n",
    "        y = featureGenera(x)\n",
    "        # print(x.shape)\n",
    "        vec = self.embedding(x)\n",
    "        # print(vec.shape)\n",
    "        # vec = torch.cat((vec,y.type_as(vec)),1) \n",
    "        out, (hn, cn) = self.lstm(vec)\n",
    "        # print(out.shape)\n",
    "        #out = self.conv(vec.permute(0,2,1))\n",
    "        out = F.relu(out)\n",
    "        out = self.linear(out[:,-1,:])\n",
    "        # out = self.linear(out)\n",
    "        out = F.softmax(out,-1)\n",
    "        return out\n",
    "\n",
    "class CBAMBlock(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(CBAMBlock, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.channel_excitation = nn.Sequential(nn.Linear(channel,int(channel//reduction),bias=False),\n",
    "                                                nn.ReLU(inplace=True),\n",
    "                                                nn.Linear(int(channel//reduction),channel,bias=False),\n",
    "                                                )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        self.spatial_excitation = nn.Sequential(nn.Conv2d(2, 1, kernel_size=7,\n",
    "                                                 stride=1, padding=3, bias=False),\n",
    "                                               )\n",
    "\n",
    "    def forward(self, x):\n",
    "        bahs, chs, _, _ = x.size()\n",
    "\n",
    "        # Returns a new tensor with the same data as the self tensor but of a different size.\n",
    "        chn_avg = self.avg_pool(x).view(bahs, chs)\n",
    "        chn_avg = self.channel_excitation(chn_avg).view(bahs, chs, 1, 1)\n",
    "        chn_max = self.max_pool(x).view(bahs, chs)\n",
    "        chn_max = self.channel_excitation(chn_max).view(bahs, chs, 1, 1)\n",
    "        chn_add=chn_avg+chn_max\n",
    "        chn_add=self.sigmoid(chn_add)\n",
    "\n",
    "        chn_cbam = torch.mul(x, chn_add)\n",
    "\n",
    "        avg_out = torch.mean(chn_cbam, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(chn_cbam, dim=1, keepdim=True)\n",
    "        cat = torch.cat([avg_out, max_out], dim=1)\n",
    "        spa_add = self.spatial_excitation(cat)\n",
    "        spa_add=self.sigmoid(spa_add)\n",
    "\n",
    "        spa_cbam = torch.mul(chn_cbam, spa_add)\n",
    "        return spa_cbam\n",
    "\n",
    "\n",
    "class CapsuleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 1,28x28\n",
    "        self.embedding = nn.Embedding(len_vocab,embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size,40,batch_first=True)#,bidirectional=True)\n",
    "        self.conv1=nn.Conv2d(1,256,9)\n",
    "        self.conv3=nn.Conv2d(1,256,8)\n",
    "        self.cbamBlock = CBAMBlock(256) \n",
    "        self.conv2=nn.Conv2d(256,32*8,9,2)\n",
    "        self.conv4=nn.Conv2d(256,32*8,8,2) \n",
    "        self.capsule = Capsule(2304,16,2,32)\n",
    "        # self.Decoder = Decoder()\n",
    "   \n",
    "    def forward(self,x):\n",
    "        batch_size = x.size(0)\n",
    "        # Conv1\n",
    "        # print(x)\n",
    "        y = featureGenera(x)\n",
    "        # print(y.shape)\n",
    "        out = self.embedding(x)\n",
    "        # print(out.shape)\n",
    "        #out, (hn, cn) = self.lstm(out)\n",
    "        # print(out.shape)\n",
    "        #out = out[:,-1,:].reshape(batch_size,2,20)\n",
    "        y = y.type_as(out)\n",
    "        # out = y\n",
    "        #out = torch.cat((out,y.type_as(out)),1)\n",
    "        # print(out)\n",
    "        out = out.unsqueeze(1)\n",
    "        y = y.unsqueeze(1)\n",
    "        #（16,1,25,20）\n",
    "        out = self.conv1(out)\n",
    "        out = self.cbamBlock(out)\n",
    "        y = self.conv3(y)\n",
    "        y = self.cbamBlock(y)\n",
    "        #（16,256,17,12）\n",
    "        out = F.relu(out)\n",
    "        y = F.relu(y)\n",
    "\n",
    "        #out = self.seLayer(out)\n",
    "        # PrimaryCaps\n",
    "        out = self.conv2(out)\n",
    "        y = self.conv4(y)\n",
    "        #(16,256,5,2)\n",
    "        out = F.relu(out)\n",
    "        y = F.relu(y)\n",
    "        out = out.view(batch_size,16,-1)\n",
    "        y = y.view(batch_size,16,-1)\n",
    "        # print(y.shape)\n",
    "        # print(out.shape)\n",
    "        out = torch.cat((out,y),2)\n",
    "        #(16,8,320)\n",
    "        out = squash(out)\n",
    "        # wj(batch_size,8,1152)\n",
    "        out = out.view(out.size(0),out.size(1),-1)\n",
    "        #(16,8,320)\n",
    "        # Capsule\n",
    "        # wj(batch_size,16,10)\n",
    "        out = self.capsule(out)\n",
    "        #(16,16,2)\n",
    "        # wj(batch_size,10,16)\n",
    "        out = out.permute(0,2,1)\n",
    "        # (16,2,16)\n",
    "        # decoder = self.Decoder(out,label)\n",
    "        return out#,decoder\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-16T09:41:51.519713Z",
     "start_time": "2022-01-16T09:41:48.820388Z"
    }
   },
   "outputs": [],
   "source": [
    "model = CapsuleNet()\n",
    "\"\"\"\n",
    "将前面生成的词向量矩阵拷贝到模型的embedding层\n",
    "这样就自动的可以将输入的word index转为词向量\n",
    "\"\"\"\n",
    "\n",
    "# 训练\n",
    "model.to(DEVICE)\n",
    "\n",
    "# 训练\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
    "\n",
    "n_epoch = 12\n",
    "\n",
    "best_val_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-16T09:41:51.534707Z",
     "start_time": "2022-01-16T09:41:51.521162Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+------------+\n",
      "|                Modules                | Parameters |\n",
      "+---------------------------------------+------------+\n",
      "|            embedding.weight           |    440     |\n",
      "|           lstm.weight_ih_l0           |    3200    |\n",
      "|           lstm.weight_hh_l0           |    6400    |\n",
      "|            lstm.bias_ih_l0            |    160     |\n",
      "|            lstm.bias_hh_l0            |    160     |\n",
      "|              conv1.weight             |   20736    |\n",
      "|               conv1.bias              |    256     |\n",
      "|              conv3.weight             |   16384    |\n",
      "|               conv3.bias              |    256     |\n",
      "| cbamBlock.channel_excitation.0.weight |    4096    |\n",
      "| cbamBlock.channel_excitation.2.weight |    4096    |\n",
      "| cbamBlock.spatial_excitation.0.weight |     98     |\n",
      "|              conv2.weight             |  5308416   |\n",
      "|               conv2.bias              |    256     |\n",
      "|              conv4.weight             |  4194304   |\n",
      "|               conv4.bias              |    256     |\n",
      "|               capsule.W               |  2359296   |\n",
      "+---------------------------------------+------------+\n",
      "Total Trainable Params: 11918810\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11918810"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        param = parameter.numel()\n",
    "        table.add_row([name, param])\n",
    "        total_params+=param\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "11,918,810"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-16T17:25:02.700516Z",
     "start_time": "2022-01-16T09:41:51.535765Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bis/miniconda3/envs/torch/lib/python3.6/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/home/bis/miniconda3/envs/torch/lib/python3.6/site-packages/ipykernel_launcher.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 \t batch_idx : 99 \t loss: 0.2165 \t train acc: 0.6250\n",
      "epoch: 0 \t batch_idx : 199 \t loss: 0.2562 \t train acc: 0.5312\n",
      "epoch: 0 \t batch_idx : 299 \t loss: 0.2073 \t train acc: 0.5938\n",
      "epoch: 0 \t batch_idx : 399 \t loss: 0.0992 \t train acc: 0.9375\n",
      "epoch: 0 \t batch_idx : 499 \t loss: 0.1590 \t train acc: 0.6562\n",
      "epoch: 0 \t batch_idx : 599 \t loss: 0.2056 \t train acc: 0.6250\n",
      "epoch: 0 \t batch_idx : 699 \t loss: 0.1194 \t train acc: 0.8750\n",
      "epoch: 0 \t batch_idx : 799 \t loss: 0.1335 \t train acc: 0.7812\n",
      "epoch: 0 \t batch_idx : 899 \t loss: 0.1259 \t train acc: 0.7500\n",
      "epoch: 0 \t batch_idx : 999 \t loss: 0.1332 \t train acc: 0.8438\n",
      "epoch: 0 \t batch_idx : 1099 \t loss: 0.1291 \t train acc: 0.7500\n",
      "epoch: 0 \t batch_idx : 1199 \t loss: 0.1113 \t train acc: 0.7812\n",
      "epoch: 0 \t batch_idx : 1299 \t loss: 0.1549 \t train acc: 0.8125\n",
      "epoch: 0 \t batch_idx : 1399 \t loss: 0.0893 \t train acc: 0.9375\n",
      "epoch: 0 \t batch_idx : 1499 \t loss: 0.1889 \t train acc: 0.6562\n",
      "epoch: 0 \t batch_idx : 1599 \t loss: 0.2323 \t train acc: 0.6250\n",
      "epoch: 0 \t batch_idx : 1699 \t loss: 0.1377 \t train acc: 0.8125\n",
      "epoch: 0 \t batch_idx : 1799 \t loss: 0.1348 \t train acc: 0.7500\n",
      "epoch: 0 \t batch_idx : 1899 \t loss: 0.1658 \t train acc: 0.6875\n",
      "epoch: 0 \t batch_idx : 1999 \t loss: 0.1052 \t train acc: 0.8750\n",
      "epoch: 0 \t batch_idx : 2099 \t loss: 0.1388 \t train acc: 0.7812\n",
      "epoch: 0 \t batch_idx : 2199 \t loss: 0.1258 \t train acc: 0.7812\n",
      "epoch: 0 \t batch_idx : 2299 \t loss: 0.1213 \t train acc: 0.8750\n",
      "epoch: 0 \t batch_idx : 2399 \t loss: 0.1695 \t train acc: 0.6875\n",
      "epoch: 0 \t batch_idx : 2499 \t loss: 0.0925 \t train acc: 0.8750\n",
      "epoch: 0 \t batch_idx : 2599 \t loss: 0.0975 \t train acc: 0.8125\n",
      "epoch: 0 \t batch_idx : 2699 \t loss: 0.1673 \t train acc: 0.6250\n",
      "epoch: 0 \t batch_idx : 2799 \t loss: 0.1673 \t train acc: 0.6562\n",
      "epoch: 0 \t batch_idx : 2899 \t loss: 0.0909 \t train acc: 0.8125\n",
      "epoch: 0 \t batch_idx : 2999 \t loss: 0.1621 \t train acc: 0.7500\n",
      "epoch: 0 \t batch_idx : 3099 \t loss: 0.1605 \t train acc: 0.7500\n",
      "epoch: 0 \t batch_idx : 3199 \t loss: 0.0633 \t train acc: 0.9375\n",
      "epoch: 0 \t batch_idx : 3299 \t loss: 0.1742 \t train acc: 0.7188\n",
      "epoch: 0 \t batch_idx : 3399 \t loss: 0.1173 \t train acc: 0.7500\n",
      "epoch: 0 \t batch_idx : 3499 \t loss: 0.1399 \t train acc: 0.8125\n",
      "epoch: 0 \t batch_idx : 3599 \t loss: 0.1432 \t train acc: 0.7188\n",
      "epoch: 0 \t batch_idx : 3699 \t loss: 0.0992 \t train acc: 0.8125\n",
      "epoch: 0 \t batch_idx : 3799 \t loss: 0.0914 \t train acc: 0.9062\n",
      "epoch: 0 \t batch_idx : 3899 \t loss: 0.0824 \t train acc: 0.9062\n",
      "epoch: 0 \t batch_idx : 3999 \t loss: 0.1029 \t train acc: 0.8438\n",
      "epoch: 0 \t batch_idx : 4099 \t loss: 0.1089 \t train acc: 0.8125\n",
      "epoch: 0 \t batch_idx : 4199 \t loss: 0.0986 \t train acc: 0.8125\n",
      "epoch: 0 \t batch_idx : 4299 \t loss: 0.0926 \t train acc: 0.8438\n",
      "epoch: 0 \t batch_idx : 4399 \t loss: 0.0845 \t train acc: 0.9062\n",
      "epoch: 0 \t batch_idx : 4499 \t loss: 0.0875 \t train acc: 0.9062\n",
      "epoch: 0 \t batch_idx : 4599 \t loss: 0.1154 \t train acc: 0.7812\n",
      "epoch: 0 \t batch_idx : 4699 \t loss: 0.0972 \t train acc: 0.8438\n",
      "epoch: 0 \t batch_idx : 4799 \t loss: 0.1192 \t train acc: 0.7812\n",
      "epoch: 0 \t batch_idx : 4899 \t loss: 0.1829 \t train acc: 0.7500\n",
      "epoch: 0 \t batch_idx : 4999 \t loss: 0.1546 \t train acc: 0.7812\n",
      "epoch: 0 \t batch_idx : 5099 \t loss: 0.1074 \t train acc: 0.8125\n",
      "epoch: 0 \t batch_idx : 5199 \t loss: 0.1513 \t train acc: 0.7812\n",
      "epoch: 0 \t batch_idx : 5299 \t loss: 0.1069 \t train acc: 0.8125\n",
      "epoch: 0 \t batch_idx : 5399 \t loss: 0.0751 \t train acc: 0.8438\n",
      "epoch: 0 \t batch_idx : 5499 \t loss: 0.1162 \t train acc: 0.8125\n",
      "epoch: 0 \t batch_idx : 5599 \t loss: 0.1690 \t train acc: 0.6562\n",
      "epoch: 0 \t batch_idx : 5699 \t loss: 0.0924 \t train acc: 0.8750\n",
      "epoch: 0 \t batch_idx : 5799 \t loss: 0.1850 \t train acc: 0.5938\n",
      "epoch: 0 \t batch_idx : 5899 \t loss: 0.1203 \t train acc: 0.7812\n",
      "epoch: 0 \t batch_idx : 5999 \t loss: 0.0798 \t train acc: 0.9062\n",
      "epoch: 0 \t batch_idx : 6099 \t loss: 0.1355 \t train acc: 0.7500\n",
      "epoch: 0 \t batch_idx : 6199 \t loss: 0.1267 \t train acc: 0.8438\n",
      "epoch: 0 \t batch_idx : 6299 \t loss: 0.1091 \t train acc: 0.8750\n",
      "epoch: 0 \t batch_idx : 6399 \t loss: 0.0681 \t train acc: 0.9688\n",
      "epoch: 0 \t batch_idx : 6499 \t loss: 0.0985 \t train acc: 0.8438\n",
      "epoch: 0 \t batch_idx : 6599 \t loss: 0.1082 \t train acc: 0.8750\n",
      "epoch: 0 \t batch_idx : 6699 \t loss: 0.1499 \t train acc: 0.7188\n",
      "epoch: 0 \t batch_idx : 6799 \t loss: 0.1212 \t train acc: 0.8438\n",
      "epoch: 0 \t batch_idx : 6899 \t loss: 0.1399 \t train acc: 0.7500\n",
      "epoch: 0 \t batch_idx : 6999 \t loss: 0.0951 \t train acc: 0.8438\n",
      "epoch: 0 \t batch_idx : 7099 \t loss: 0.1420 \t train acc: 0.7188\n",
      "epoch: 0 \t batch_idx : 7199 \t loss: 0.1410 \t train acc: 0.7188\n",
      "epoch: 0 \t batch_idx : 7299 \t loss: 0.1407 \t train acc: 0.8125\n",
      "epoch: 0 \t batch_idx : 7399 \t loss: 0.1028 \t train acc: 0.8750\n",
      "epoch: 0 \t batch_idx : 7499 \t loss: 0.1394 \t train acc: 0.7812\n",
      "epoch: 0 \t batch_idx : 7599 \t loss: 0.0880 \t train acc: 0.9062\n",
      "epoch: 0 \t batch_idx : 7699 \t loss: 0.1574 \t train acc: 0.8125\n",
      "epoch: 0 \t batch_idx : 7799 \t loss: 0.1574 \t train acc: 0.7188\n",
      "epoch: 0 \t batch_idx : 7899 \t loss: 0.1273 \t train acc: 0.8125\n",
      "epoch: 0 \t batch_idx : 7999 \t loss: 0.1367 \t train acc: 0.8125\n",
      "epoch: 0 \t batch_idx : 8099 \t loss: 0.0940 \t train acc: 0.8438\n",
      "epoch: 0 \t batch_idx : 8199 \t loss: 0.1176 \t train acc: 0.8125\n",
      "epoch: 0 \t batch_idx : 8299 \t loss: 0.1282 \t train acc: 0.8438\n",
      "epoch: 0 \t batch_idx : 8399 \t loss: 0.0982 \t train acc: 0.8438\n",
      "epoch: 0 \t batch_idx : 8499 \t loss: 0.1013 \t train acc: 0.8438\n",
      "epoch: 0 \t batch_idx : 8599 \t loss: 0.0923 \t train acc: 0.8438\n",
      "epoch: 0 \t batch_idx : 8699 \t loss: 0.1294 \t train acc: 0.7812\n",
      "epoch: 0 \t batch_idx : 8799 \t loss: 0.0861 \t train acc: 0.8750\n",
      "epoch: 0 \t batch_idx : 8899 \t loss: 0.0984 \t train acc: 0.8750\n",
      "epoch: 0 \t batch_idx : 8999 \t loss: 0.1096 \t train acc: 0.7812\n",
      "epoch: 0 \t batch_idx : 9099 \t loss: 0.1171 \t train acc: 0.8125\n",
      "epoch: 0 \t batch_idx : 9199 \t loss: 0.1184 \t train acc: 0.8125\n",
      "epoch: 0 \t batch_idx : 9299 \t loss: 0.1142 \t train acc: 0.8125\n",
      "epoch: 0 \t batch_idx : 9399 \t loss: 0.1188 \t train acc: 0.7500\n",
      "epoch: 0 \t batch_idx : 9499 \t loss: 0.1114 \t train acc: 0.8750\n",
      "epoch: 0 \t batch_idx : 9599 \t loss: 0.1045 \t train acc: 0.8125\n",
      "epoch: 0 \t batch_idx : 9699 \t loss: 0.1333 \t train acc: 0.7500\n",
      "epoch: 0 \t batch_idx : 9799 \t loss: 0.1492 \t train acc: 0.7188\n",
      "epoch: 0 \t batch_idx : 9899 \t loss: 0.1374 \t train acc: 0.8125\n",
      "epoch: 0 \t batch_idx : 9999 \t loss: 0.0989 \t train acc: 0.8750\n",
      "epoch: 0 \t batch_idx : 10099 \t loss: 0.1232 \t train acc: 0.8438\n",
      "epoch: 0 \t batch_idx : 10199 \t loss: 0.0570 \t train acc: 0.9688\n",
      "epoch: 0 \t batch_idx : 10299 \t loss: 0.0957 \t train acc: 0.9062\n",
      "epoch: 0 \t batch_idx : 10399 \t loss: 0.2153 \t train acc: 0.6250\n",
      "epoch: 0 \t batch_idx : 10499 \t loss: 0.1268 \t train acc: 0.8125\n",
      "epoch: 0 \t batch_idx : 10599 \t loss: 0.0672 \t train acc: 0.9375\n",
      "epoch: 0 \t batch_idx : 10699 \t loss: 0.0987 \t train acc: 0.8438\n",
      "epoch: 0 \t batch_idx : 10799 \t loss: 0.1186 \t train acc: 0.7812\n",
      "epoch: 0 \t batch_idx : 10899 \t loss: 0.1078 \t train acc: 0.8750\n",
      "epoch: 0 \t batch_idx : 10999 \t loss: 0.1472 \t train acc: 0.7812\n",
      "epoch: 0 \t batch_idx : 11099 \t loss: 0.1430 \t train acc: 0.6875\n",
      "epoch: 0 \t batch_idx : 11199 \t loss: 0.1554 \t train acc: 0.7812\n",
      "epoch: 0 \t batch_idx : 11299 \t loss: 0.1074 \t train acc: 0.7812\n",
      "epoch: 0 \t batch_idx : 11399 \t loss: 0.1787 \t train acc: 0.7188\n",
      "epoch: 0 \t batch_idx : 11499 \t loss: 0.1456 \t train acc: 0.7812\n",
      "epoch: 0 \t batch_idx : 11599 \t loss: 0.0968 \t train acc: 0.8750\n",
      "epoch: 0 \t batch_idx : 11699 \t loss: 0.1371 \t train acc: 0.7188\n",
      "epoch: 0 \t batch_idx : 11799 \t loss: 0.1491 \t train acc: 0.7500\n",
      "epoch: 0 \t batch_idx : 11899 \t loss: 0.1281 \t train acc: 0.7812\n",
      "epoch: 0 \t batch_idx : 11999 \t loss: 0.1113 \t train acc: 0.7812\n",
      "epoch: 0 \t batch_idx : 12099 \t loss: 0.1308 \t train acc: 0.8125\n",
      "epoch: 0 \t batch_idx : 12199 \t loss: 0.1063 \t train acc: 0.7812\n",
      "epoch: 0 \t batch_idx : 12299 \t loss: 0.1522 \t train acc: 0.7812\n",
      "epoch: 0 \t batch_idx : 12399 \t loss: 0.1712 \t train acc: 0.7188\n",
      "epoch: 0 \t batch_idx : 12499 \t loss: 0.0926 \t train acc: 0.8438\n",
      "epoch: 0 \t batch_idx : 12599 \t loss: 0.1306 \t train acc: 0.7500\n",
      "epoch: 0 \t batch_idx : 12699 \t loss: 0.0934 \t train acc: 0.8125\n",
      "epoch: 0 \t batch_idx : 12799 \t loss: 0.0808 \t train acc: 0.8750\n",
      "epoch: 0 \t batch_idx : 12899 \t loss: 0.1066 \t train acc: 0.8438\n",
      "epoch: 0 \t batch_idx : 12999 \t loss: 0.1092 \t train acc: 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 \t batch_idx : 13099 \t loss: 0.0626 \t train acc: 0.9062\n",
      "epoch: 0 \t batch_idx : 13199 \t loss: 0.0796 \t train acc: 0.9062\n",
      "epoch: 0 \t batch_idx : 13299 \t loss: 0.1225 \t train acc: 0.8125\n",
      "epoch: 0 \t batch_idx : 13399 \t loss: 0.1446 \t train acc: 0.7812\n",
      "epoch: 0 \t batch_idx : 13499 \t loss: 0.1386 \t train acc: 0.7812\n",
      "epoch: 0 \t batch_idx : 13599 \t loss: 0.0856 \t train acc: 0.8750\n",
      "epoch: 0 \t batch_idx : 13699 \t loss: 0.1058 \t train acc: 0.7812\n",
      "epoch: 0 \t batch_idx : 13799 \t loss: 0.1389 \t train acc: 0.8125\n",
      "epoch: 0 \t batch_idx : 13899 \t loss: 0.1058 \t train acc: 0.8750\n",
      "epoch: 0 \t batch_idx : 13999 \t loss: 0.1329 \t train acc: 0.7188\n",
      "epoch: 0 \t batch_idx : 14099 \t loss: 0.1668 \t train acc: 0.6562\n",
      "epoch: 0 \t batch_idx : 14199 \t loss: 0.0787 \t train acc: 0.9062\n",
      "epoch: 0 \t batch_idx : 14299 \t loss: 0.0729 \t train acc: 0.9062\n",
      "epoch: 0 \t batch_idx : 14399 \t loss: 0.0847 \t train acc: 0.8438\n",
      "epoch: 0 \t batch_idx : 14499 \t loss: 0.0563 \t train acc: 0.9062\n",
      "epoch: 0 \t batch_idx : 14599 \t loss: 0.0964 \t train acc: 0.8438\n",
      "epoch: 0 \t batch_idx : 14699 \t loss: 0.1308 \t train acc: 0.7812\n",
      "epoch: 0 \t batch_idx : 14799 \t loss: 0.1370 \t train acc: 0.7812\n",
      "epoch: 0 \t batch_idx : 14899 \t loss: 0.0988 \t train acc: 0.7812\n",
      "epoch: 0 \t batch_idx : 14999 \t loss: 0.1604 \t train acc: 0.6875\n",
      "epoch: 0 \t batch_idx : 15099 \t loss: 0.1047 \t train acc: 0.8125\n",
      "epoch: 0 \t batch_idx : 15199 \t loss: 0.0976 \t train acc: 0.8438\n",
      "epoch: 0 \t batch_idx : 15299 \t loss: 0.2034 \t train acc: 0.6250\n",
      "epoch: 0 \t batch_idx : 15399 \t loss: 0.1013 \t train acc: 0.8438\n",
      "epoch: 0 \t batch_idx : 15499 \t loss: 0.0958 \t train acc: 0.8438\n",
      "epoch: 0 \t batch_idx : 15599 \t loss: 0.1484 \t train acc: 0.7188\n",
      "epoch: 0 \t batch_idx : 15699 \t loss: 0.0922 \t train acc: 0.7812\n",
      "epoch: 0 \t batch_idx : 15799 \t loss: 0.1108 \t train acc: 0.8438\n",
      "epoch: 0 \t batch_idx : 15899 \t loss: 0.1101 \t train acc: 0.8125\n",
      "epoch: 0 \t batch_idx : 15999 \t loss: 0.0811 \t train acc: 0.9375\n",
      "epoch: 0 \t batch_idx : 16099 \t loss: 0.1305 \t train acc: 0.7812\n",
      "epoch: 0 \t batch_idx : 16199 \t loss: 0.0741 \t train acc: 0.8750\n",
      "epoch: 0 \t batch_idx : 16299 \t loss: 0.1889 \t train acc: 0.6562\n",
      "epoch: 0 \t batch_idx : 16399 \t loss: 0.1010 \t train acc: 0.8438\n",
      "epoch: 0 \t batch_idx : 16499 \t loss: 0.1233 \t train acc: 0.7812\n",
      "epoch: 0 \t batch_idx : 16599 \t loss: 0.0669 \t train acc: 0.9062\n",
      "epoch: 0 \t batch_idx : 16699 \t loss: 0.1217 \t train acc: 0.8438\n",
      "epoch: 0 \t batch_idx : 16799 \t loss: 0.1063 \t train acc: 0.8438\n",
      "epoch: 0 \t batch_idx : 16899 \t loss: 0.1366 \t train acc: 0.7812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bis/miniconda3/envs/torch/lib/python3.6/site-packages/ipykernel_launcher.py:41: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc : 0.8135 > 0.0000 saving model\n",
      "test acc: 0.8135\n",
      "epoch: 1 \t batch_idx : 99 \t loss: 0.1131 \t train acc: 0.7500\n",
      "epoch: 1 \t batch_idx : 199 \t loss: 0.1067 \t train acc: 0.7500\n",
      "epoch: 1 \t batch_idx : 299 \t loss: 0.1526 \t train acc: 0.7188\n",
      "epoch: 1 \t batch_idx : 399 \t loss: 0.1362 \t train acc: 0.7500\n",
      "epoch: 1 \t batch_idx : 499 \t loss: 0.1295 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 599 \t loss: 0.1247 \t train acc: 0.7812\n",
      "epoch: 1 \t batch_idx : 699 \t loss: 0.1090 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 799 \t loss: 0.1652 \t train acc: 0.7500\n",
      "epoch: 1 \t batch_idx : 899 \t loss: 0.1154 \t train acc: 0.7812\n",
      "epoch: 1 \t batch_idx : 999 \t loss: 0.1305 \t train acc: 0.7812\n",
      "epoch: 1 \t batch_idx : 1099 \t loss: 0.0929 \t train acc: 0.8438\n",
      "epoch: 1 \t batch_idx : 1199 \t loss: 0.1389 \t train acc: 0.8438\n",
      "epoch: 1 \t batch_idx : 1299 \t loss: 0.1470 \t train acc: 0.7500\n",
      "epoch: 1 \t batch_idx : 1399 \t loss: 0.1320 \t train acc: 0.7812\n",
      "epoch: 1 \t batch_idx : 1499 \t loss: 0.1434 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 1599 \t loss: 0.0717 \t train acc: 0.9375\n",
      "epoch: 1 \t batch_idx : 1699 \t loss: 0.1069 \t train acc: 0.7812\n",
      "epoch: 1 \t batch_idx : 1799 \t loss: 0.1261 \t train acc: 0.7812\n",
      "epoch: 1 \t batch_idx : 1899 \t loss: 0.1687 \t train acc: 0.7500\n",
      "epoch: 1 \t batch_idx : 1999 \t loss: 0.0558 \t train acc: 0.8750\n",
      "epoch: 1 \t batch_idx : 2099 \t loss: 0.0696 \t train acc: 0.8750\n",
      "epoch: 1 \t batch_idx : 2199 \t loss: 0.1414 \t train acc: 0.7188\n",
      "epoch: 1 \t batch_idx : 2299 \t loss: 0.1074 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 2399 \t loss: 0.0435 \t train acc: 0.9375\n",
      "epoch: 1 \t batch_idx : 2499 \t loss: 0.1900 \t train acc: 0.6562\n",
      "epoch: 1 \t batch_idx : 2599 \t loss: 0.0768 \t train acc: 0.8750\n",
      "epoch: 1 \t batch_idx : 2699 \t loss: 0.1102 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 2799 \t loss: 0.1244 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 2899 \t loss: 0.1253 \t train acc: 0.7812\n",
      "epoch: 1 \t batch_idx : 2999 \t loss: 0.1216 \t train acc: 0.7500\n",
      "epoch: 1 \t batch_idx : 3099 \t loss: 0.1297 \t train acc: 0.7188\n",
      "epoch: 1 \t batch_idx : 3199 \t loss: 0.0955 \t train acc: 0.9062\n",
      "epoch: 1 \t batch_idx : 3299 \t loss: 0.1231 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 3399 \t loss: 0.0999 \t train acc: 0.7812\n",
      "epoch: 1 \t batch_idx : 3499 \t loss: 0.1453 \t train acc: 0.8438\n",
      "epoch: 1 \t batch_idx : 3599 \t loss: 0.1043 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 3699 \t loss: 0.0804 \t train acc: 0.8750\n",
      "epoch: 1 \t batch_idx : 3799 \t loss: 0.1644 \t train acc: 0.7500\n",
      "epoch: 1 \t batch_idx : 3899 \t loss: 0.1181 \t train acc: 0.7812\n",
      "epoch: 1 \t batch_idx : 3999 \t loss: 0.0781 \t train acc: 0.9375\n",
      "epoch: 1 \t batch_idx : 4099 \t loss: 0.1056 \t train acc: 0.7500\n",
      "epoch: 1 \t batch_idx : 4199 \t loss: 0.1139 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 4299 \t loss: 0.1124 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 4399 \t loss: 0.1594 \t train acc: 0.7812\n",
      "epoch: 1 \t batch_idx : 4499 \t loss: 0.1207 \t train acc: 0.7500\n",
      "epoch: 1 \t batch_idx : 4599 \t loss: 0.1144 \t train acc: 0.7812\n",
      "epoch: 1 \t batch_idx : 4699 \t loss: 0.0957 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 4799 \t loss: 0.1288 \t train acc: 0.7812\n",
      "epoch: 1 \t batch_idx : 4899 \t loss: 0.0887 \t train acc: 0.9062\n",
      "epoch: 1 \t batch_idx : 4999 \t loss: 0.1101 \t train acc: 0.8438\n",
      "epoch: 1 \t batch_idx : 5099 \t loss: 0.1229 \t train acc: 0.7812\n",
      "epoch: 1 \t batch_idx : 5199 \t loss: 0.0806 \t train acc: 0.9062\n",
      "epoch: 1 \t batch_idx : 5299 \t loss: 0.0939 \t train acc: 0.8438\n",
      "epoch: 1 \t batch_idx : 5399 \t loss: 0.0872 \t train acc: 0.9375\n",
      "epoch: 1 \t batch_idx : 5499 \t loss: 0.1035 \t train acc: 0.8750\n",
      "epoch: 1 \t batch_idx : 5599 \t loss: 0.1014 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 5699 \t loss: 0.0527 \t train acc: 0.9062\n",
      "epoch: 1 \t batch_idx : 5799 \t loss: 0.1068 \t train acc: 0.8438\n",
      "epoch: 1 \t batch_idx : 5899 \t loss: 0.1760 \t train acc: 0.7500\n",
      "epoch: 1 \t batch_idx : 5999 \t loss: 0.1640 \t train acc: 0.6875\n",
      "epoch: 1 \t batch_idx : 6099 \t loss: 0.1520 \t train acc: 0.7500\n",
      "epoch: 1 \t batch_idx : 6199 \t loss: 0.1263 \t train acc: 0.8438\n",
      "epoch: 1 \t batch_idx : 6299 \t loss: 0.1468 \t train acc: 0.7500\n",
      "epoch: 1 \t batch_idx : 6399 \t loss: 0.0798 \t train acc: 0.9062\n",
      "epoch: 1 \t batch_idx : 6499 \t loss: 0.1035 \t train acc: 0.8438\n",
      "epoch: 1 \t batch_idx : 6599 \t loss: 0.0841 \t train acc: 0.8438\n",
      "epoch: 1 \t batch_idx : 6699 \t loss: 0.1325 \t train acc: 0.7500\n",
      "epoch: 1 \t batch_idx : 6799 \t loss: 0.0881 \t train acc: 0.8750\n",
      "epoch: 1 \t batch_idx : 6899 \t loss: 0.0941 \t train acc: 0.9375\n",
      "epoch: 1 \t batch_idx : 6999 \t loss: 0.1425 \t train acc: 0.7500\n",
      "epoch: 1 \t batch_idx : 7099 \t loss: 0.0648 \t train acc: 0.9688\n",
      "epoch: 1 \t batch_idx : 7199 \t loss: 0.1011 \t train acc: 0.8438\n",
      "epoch: 1 \t batch_idx : 7299 \t loss: 0.0966 \t train acc: 0.8750\n",
      "epoch: 1 \t batch_idx : 7399 \t loss: 0.1536 \t train acc: 0.7500\n",
      "epoch: 1 \t batch_idx : 7499 \t loss: 0.1407 \t train acc: 0.8438\n",
      "epoch: 1 \t batch_idx : 7599 \t loss: 0.0937 \t train acc: 0.8438\n",
      "epoch: 1 \t batch_idx : 7699 \t loss: 0.1078 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 7799 \t loss: 0.1560 \t train acc: 0.7812\n",
      "epoch: 1 \t batch_idx : 7899 \t loss: 0.0919 \t train acc: 0.8750\n",
      "epoch: 1 \t batch_idx : 7999 \t loss: 0.1488 \t train acc: 0.7500\n",
      "epoch: 1 \t batch_idx : 8099 \t loss: 0.0797 \t train acc: 0.8750\n",
      "epoch: 1 \t batch_idx : 8199 \t loss: 0.1312 \t train acc: 0.8438\n",
      "epoch: 1 \t batch_idx : 8299 \t loss: 0.1347 \t train acc: 0.7188\n",
      "epoch: 1 \t batch_idx : 8399 \t loss: 0.1221 \t train acc: 0.7500\n",
      "epoch: 1 \t batch_idx : 8499 \t loss: 0.0716 \t train acc: 0.8750\n",
      "epoch: 1 \t batch_idx : 8599 \t loss: 0.1426 \t train acc: 0.7500\n",
      "epoch: 1 \t batch_idx : 8699 \t loss: 0.1114 \t train acc: 0.8438\n",
      "epoch: 1 \t batch_idx : 8799 \t loss: 0.1904 \t train acc: 0.6250\n",
      "epoch: 1 \t batch_idx : 8899 \t loss: 0.0900 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 8999 \t loss: 0.0883 \t train acc: 0.8438\n",
      "epoch: 1 \t batch_idx : 9099 \t loss: 0.1428 \t train acc: 0.6250\n",
      "epoch: 1 \t batch_idx : 9199 \t loss: 0.1188 \t train acc: 0.7812\n",
      "epoch: 1 \t batch_idx : 9299 \t loss: 0.1170 \t train acc: 0.8438\n",
      "epoch: 1 \t batch_idx : 9399 \t loss: 0.0983 \t train acc: 0.8438\n",
      "epoch: 1 \t batch_idx : 9499 \t loss: 0.0588 \t train acc: 0.9062\n",
      "epoch: 1 \t batch_idx : 9599 \t loss: 0.1161 \t train acc: 0.8438\n",
      "epoch: 1 \t batch_idx : 9699 \t loss: 0.1098 \t train acc: 0.8750\n",
      "epoch: 1 \t batch_idx : 9799 \t loss: 0.1335 \t train acc: 0.6875\n",
      "epoch: 1 \t batch_idx : 9899 \t loss: 0.1199 \t train acc: 0.7812\n",
      "epoch: 1 \t batch_idx : 9999 \t loss: 0.1459 \t train acc: 0.7812\n",
      "epoch: 1 \t batch_idx : 10099 \t loss: 0.0922 \t train acc: 0.7812\n",
      "epoch: 1 \t batch_idx : 10199 \t loss: 0.1116 \t train acc: 0.7812\n",
      "epoch: 1 \t batch_idx : 10299 \t loss: 0.0855 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 10399 \t loss: 0.1656 \t train acc: 0.7188\n",
      "epoch: 1 \t batch_idx : 10499 \t loss: 0.1136 \t train acc: 0.8438\n",
      "epoch: 1 \t batch_idx : 10599 \t loss: 0.1427 \t train acc: 0.7500\n",
      "epoch: 1 \t batch_idx : 10699 \t loss: 0.1591 \t train acc: 0.7500\n",
      "epoch: 1 \t batch_idx : 10799 \t loss: 0.1191 \t train acc: 0.7812\n",
      "epoch: 1 \t batch_idx : 10899 \t loss: 0.0550 \t train acc: 0.9375\n",
      "epoch: 1 \t batch_idx : 10999 \t loss: 0.1247 \t train acc: 0.7188\n",
      "epoch: 1 \t batch_idx : 11099 \t loss: 0.0979 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 11199 \t loss: 0.0955 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 11299 \t loss: 0.1161 \t train acc: 0.8438\n",
      "epoch: 1 \t batch_idx : 11399 \t loss: 0.1104 \t train acc: 0.8438\n",
      "epoch: 1 \t batch_idx : 11499 \t loss: 0.1118 \t train acc: 0.8438\n",
      "epoch: 1 \t batch_idx : 11599 \t loss: 0.0627 \t train acc: 0.9062\n",
      "epoch: 1 \t batch_idx : 11699 \t loss: 0.0839 \t train acc: 0.8750\n",
      "epoch: 1 \t batch_idx : 11799 \t loss: 0.1281 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 11899 \t loss: 0.0672 \t train acc: 0.9375\n",
      "epoch: 1 \t batch_idx : 11999 \t loss: 0.1570 \t train acc: 0.7188\n",
      "epoch: 1 \t batch_idx : 12099 \t loss: 0.1262 \t train acc: 0.7812\n",
      "epoch: 1 \t batch_idx : 12199 \t loss: 0.1307 \t train acc: 0.7812\n",
      "epoch: 1 \t batch_idx : 12299 \t loss: 0.1208 \t train acc: 0.7188\n",
      "epoch: 1 \t batch_idx : 12399 \t loss: 0.0945 \t train acc: 0.8438\n",
      "epoch: 1 \t batch_idx : 12499 \t loss: 0.1128 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 12599 \t loss: 0.1222 \t train acc: 0.8438\n",
      "epoch: 1 \t batch_idx : 12699 \t loss: 0.1257 \t train acc: 0.7812\n",
      "epoch: 1 \t batch_idx : 12799 \t loss: 0.1273 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 12899 \t loss: 0.1204 \t train acc: 0.7812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 \t batch_idx : 12999 \t loss: 0.0997 \t train acc: 0.8438\n",
      "epoch: 1 \t batch_idx : 13099 \t loss: 0.1488 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 13199 \t loss: 0.0942 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 13299 \t loss: 0.0560 \t train acc: 0.9062\n",
      "epoch: 1 \t batch_idx : 13399 \t loss: 0.0786 \t train acc: 0.9062\n",
      "epoch: 1 \t batch_idx : 13499 \t loss: 0.1282 \t train acc: 0.7812\n",
      "epoch: 1 \t batch_idx : 13599 \t loss: 0.1005 \t train acc: 0.8438\n",
      "epoch: 1 \t batch_idx : 13699 \t loss: 0.1183 \t train acc: 0.8438\n",
      "epoch: 1 \t batch_idx : 13799 \t loss: 0.1089 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 13899 \t loss: 0.1225 \t train acc: 0.7500\n",
      "epoch: 1 \t batch_idx : 13999 \t loss: 0.1385 \t train acc: 0.7812\n",
      "epoch: 1 \t batch_idx : 14099 \t loss: 0.1317 \t train acc: 0.7500\n",
      "epoch: 1 \t batch_idx : 14199 \t loss: 0.1100 \t train acc: 0.8438\n",
      "epoch: 1 \t batch_idx : 14299 \t loss: 0.1634 \t train acc: 0.7500\n",
      "epoch: 1 \t batch_idx : 14399 \t loss: 0.1153 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 14499 \t loss: 0.0805 \t train acc: 0.9062\n",
      "epoch: 1 \t batch_idx : 14599 \t loss: 0.1096 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 14699 \t loss: 0.0739 \t train acc: 0.8750\n",
      "epoch: 1 \t batch_idx : 14799 \t loss: 0.0999 \t train acc: 0.8438\n",
      "epoch: 1 \t batch_idx : 14899 \t loss: 0.1003 \t train acc: 0.8750\n",
      "epoch: 1 \t batch_idx : 14999 \t loss: 0.0882 \t train acc: 0.8750\n",
      "epoch: 1 \t batch_idx : 15099 \t loss: 0.1061 \t train acc: 0.8750\n",
      "epoch: 1 \t batch_idx : 15199 \t loss: 0.1032 \t train acc: 0.7500\n",
      "epoch: 1 \t batch_idx : 15299 \t loss: 0.0919 \t train acc: 0.8750\n",
      "epoch: 1 \t batch_idx : 15399 \t loss: 0.1520 \t train acc: 0.7500\n",
      "epoch: 1 \t batch_idx : 15499 \t loss: 0.0674 \t train acc: 0.9375\n",
      "epoch: 1 \t batch_idx : 15599 \t loss: 0.1010 \t train acc: 0.7500\n",
      "epoch: 1 \t batch_idx : 15699 \t loss: 0.1329 \t train acc: 0.8750\n",
      "epoch: 1 \t batch_idx : 15799 \t loss: 0.1090 \t train acc: 0.7188\n",
      "epoch: 1 \t batch_idx : 15899 \t loss: 0.1129 \t train acc: 0.7812\n",
      "epoch: 1 \t batch_idx : 15999 \t loss: 0.0587 \t train acc: 0.9375\n",
      "epoch: 1 \t batch_idx : 16099 \t loss: 0.1025 \t train acc: 0.8125\n",
      "epoch: 1 \t batch_idx : 16199 \t loss: 0.0992 \t train acc: 0.8750\n",
      "epoch: 1 \t batch_idx : 16299 \t loss: 0.0996 \t train acc: 0.7500\n",
      "epoch: 1 \t batch_idx : 16399 \t loss: 0.0720 \t train acc: 0.9062\n",
      "epoch: 1 \t batch_idx : 16499 \t loss: 0.1284 \t train acc: 0.7812\n",
      "epoch: 1 \t batch_idx : 16599 \t loss: 0.1573 \t train acc: 0.7500\n",
      "epoch: 1 \t batch_idx : 16699 \t loss: 0.1023 \t train acc: 0.8750\n",
      "epoch: 1 \t batch_idx : 16799 \t loss: 0.1029 \t train acc: 0.8750\n",
      "epoch: 1 \t batch_idx : 16899 \t loss: 0.1704 \t train acc: 0.6875\n",
      "val acc : 0.8171 > 0.8135 saving model\n",
      "test acc: 0.8171\n",
      "epoch: 2 \t batch_idx : 99 \t loss: 0.1272 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 199 \t loss: 0.1193 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 299 \t loss: 0.1176 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 399 \t loss: 0.0742 \t train acc: 0.9375\n",
      "epoch: 2 \t batch_idx : 499 \t loss: 0.0938 \t train acc: 0.8438\n",
      "epoch: 2 \t batch_idx : 599 \t loss: 0.1680 \t train acc: 0.7188\n",
      "epoch: 2 \t batch_idx : 699 \t loss: 0.1101 \t train acc: 0.8750\n",
      "epoch: 2 \t batch_idx : 799 \t loss: 0.1430 \t train acc: 0.6562\n",
      "epoch: 2 \t batch_idx : 899 \t loss: 0.1053 \t train acc: 0.8438\n",
      "epoch: 2 \t batch_idx : 999 \t loss: 0.0920 \t train acc: 0.8438\n",
      "epoch: 2 \t batch_idx : 1099 \t loss: 0.0877 \t train acc: 0.9062\n",
      "epoch: 2 \t batch_idx : 1199 \t loss: 0.0822 \t train acc: 0.9062\n",
      "epoch: 2 \t batch_idx : 1299 \t loss: 0.0605 \t train acc: 0.9375\n",
      "epoch: 2 \t batch_idx : 1399 \t loss: 0.1422 \t train acc: 0.7500\n",
      "epoch: 2 \t batch_idx : 1499 \t loss: 0.1064 \t train acc: 0.7812\n",
      "epoch: 2 \t batch_idx : 1599 \t loss: 0.1070 \t train acc: 0.7812\n",
      "epoch: 2 \t batch_idx : 1699 \t loss: 0.0901 \t train acc: 0.9062\n",
      "epoch: 2 \t batch_idx : 1799 \t loss: 0.1204 \t train acc: 0.8438\n",
      "epoch: 2 \t batch_idx : 1899 \t loss: 0.1546 \t train acc: 0.6875\n",
      "epoch: 2 \t batch_idx : 1999 \t loss: 0.1101 \t train acc: 0.8438\n",
      "epoch: 2 \t batch_idx : 2099 \t loss: 0.1411 \t train acc: 0.7500\n",
      "epoch: 2 \t batch_idx : 2199 \t loss: 0.1683 \t train acc: 0.6875\n",
      "epoch: 2 \t batch_idx : 2299 \t loss: 0.0824 \t train acc: 0.8438\n",
      "epoch: 2 \t batch_idx : 2399 \t loss: 0.1301 \t train acc: 0.7500\n",
      "epoch: 2 \t batch_idx : 2499 \t loss: 0.0955 \t train acc: 0.9062\n",
      "epoch: 2 \t batch_idx : 2599 \t loss: 0.1002 \t train acc: 0.8438\n",
      "epoch: 2 \t batch_idx : 2699 \t loss: 0.1415 \t train acc: 0.7500\n",
      "epoch: 2 \t batch_idx : 2799 \t loss: 0.1514 \t train acc: 0.7812\n",
      "epoch: 2 \t batch_idx : 2899 \t loss: 0.1108 \t train acc: 0.8438\n",
      "epoch: 2 \t batch_idx : 2999 \t loss: 0.0476 \t train acc: 0.9375\n",
      "epoch: 2 \t batch_idx : 3099 \t loss: 0.0860 \t train acc: 0.8438\n",
      "epoch: 2 \t batch_idx : 3199 \t loss: 0.1907 \t train acc: 0.7188\n",
      "epoch: 2 \t batch_idx : 3299 \t loss: 0.1057 \t train acc: 0.8438\n",
      "epoch: 2 \t batch_idx : 3399 \t loss: 0.1721 \t train acc: 0.7188\n",
      "epoch: 2 \t batch_idx : 3499 \t loss: 0.1313 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 3599 \t loss: 0.1179 \t train acc: 0.8438\n",
      "epoch: 2 \t batch_idx : 3699 \t loss: 0.1030 \t train acc: 0.8750\n",
      "epoch: 2 \t batch_idx : 3799 \t loss: 0.0859 \t train acc: 0.8438\n",
      "epoch: 2 \t batch_idx : 3899 \t loss: 0.1187 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 3999 \t loss: 0.0897 \t train acc: 0.8750\n",
      "epoch: 2 \t batch_idx : 4099 \t loss: 0.1135 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 4199 \t loss: 0.1251 \t train acc: 0.7812\n",
      "epoch: 2 \t batch_idx : 4299 \t loss: 0.1424 \t train acc: 0.7188\n",
      "epoch: 2 \t batch_idx : 4399 \t loss: 0.0639 \t train acc: 0.8750\n",
      "epoch: 2 \t batch_idx : 4499 \t loss: 0.1028 \t train acc: 0.8750\n",
      "epoch: 2 \t batch_idx : 4599 \t loss: 0.1037 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 4699 \t loss: 0.1231 \t train acc: 0.7812\n",
      "epoch: 2 \t batch_idx : 4799 \t loss: 0.1678 \t train acc: 0.7188\n",
      "epoch: 2 \t batch_idx : 4899 \t loss: 0.1102 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 4999 \t loss: 0.0816 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 5099 \t loss: 0.1350 \t train acc: 0.7812\n",
      "epoch: 2 \t batch_idx : 5199 \t loss: 0.0980 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 5299 \t loss: 0.0810 \t train acc: 0.8750\n",
      "epoch: 2 \t batch_idx : 5399 \t loss: 0.1427 \t train acc: 0.7812\n",
      "epoch: 2 \t batch_idx : 5499 \t loss: 0.0917 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 5599 \t loss: 0.0884 \t train acc: 0.8750\n",
      "epoch: 2 \t batch_idx : 5699 \t loss: 0.0975 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 5799 \t loss: 0.1097 \t train acc: 0.8750\n",
      "epoch: 2 \t batch_idx : 5899 \t loss: 0.1629 \t train acc: 0.7500\n",
      "epoch: 2 \t batch_idx : 5999 \t loss: 0.1315 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 6099 \t loss: 0.0659 \t train acc: 0.9062\n",
      "epoch: 2 \t batch_idx : 6199 \t loss: 0.1374 \t train acc: 0.7812\n",
      "epoch: 2 \t batch_idx : 6299 \t loss: 0.1025 \t train acc: 0.7812\n",
      "epoch: 2 \t batch_idx : 6399 \t loss: 0.1241 \t train acc: 0.7500\n",
      "epoch: 2 \t batch_idx : 6499 \t loss: 0.1558 \t train acc: 0.7500\n",
      "epoch: 2 \t batch_idx : 6599 \t loss: 0.0957 \t train acc: 0.7812\n",
      "epoch: 2 \t batch_idx : 6699 \t loss: 0.0804 \t train acc: 0.9062\n",
      "epoch: 2 \t batch_idx : 6799 \t loss: 0.0909 \t train acc: 0.8750\n",
      "epoch: 2 \t batch_idx : 6899 \t loss: 0.0947 \t train acc: 0.8750\n",
      "epoch: 2 \t batch_idx : 6999 \t loss: 0.1400 \t train acc: 0.7812\n",
      "epoch: 2 \t batch_idx : 7099 \t loss: 0.1615 \t train acc: 0.6875\n",
      "epoch: 2 \t batch_idx : 7199 \t loss: 0.1149 \t train acc: 0.9062\n",
      "epoch: 2 \t batch_idx : 7299 \t loss: 0.0976 \t train acc: 0.8750\n",
      "epoch: 2 \t batch_idx : 7399 \t loss: 0.1264 \t train acc: 0.7500\n",
      "epoch: 2 \t batch_idx : 7499 \t loss: 0.0796 \t train acc: 0.9062\n",
      "epoch: 2 \t batch_idx : 7599 \t loss: 0.1075 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 7699 \t loss: 0.0998 \t train acc: 0.8438\n",
      "epoch: 2 \t batch_idx : 7799 \t loss: 0.0945 \t train acc: 0.8750\n",
      "epoch: 2 \t batch_idx : 7899 \t loss: 0.0899 \t train acc: 0.8750\n",
      "epoch: 2 \t batch_idx : 7999 \t loss: 0.1425 \t train acc: 0.7500\n",
      "epoch: 2 \t batch_idx : 8099 \t loss: 0.1288 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 8199 \t loss: 0.0718 \t train acc: 0.9062\n",
      "epoch: 2 \t batch_idx : 8299 \t loss: 0.1439 \t train acc: 0.7812\n",
      "epoch: 2 \t batch_idx : 8399 \t loss: 0.0775 \t train acc: 0.8750\n",
      "epoch: 2 \t batch_idx : 8499 \t loss: 0.0628 \t train acc: 0.9375\n",
      "epoch: 2 \t batch_idx : 8599 \t loss: 0.1072 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 8699 \t loss: 0.0826 \t train acc: 0.9375\n",
      "epoch: 2 \t batch_idx : 8799 \t loss: 0.1072 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 8899 \t loss: 0.0845 \t train acc: 0.8750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 \t batch_idx : 8999 \t loss: 0.1171 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 9099 \t loss: 0.1081 \t train acc: 0.8438\n",
      "epoch: 2 \t batch_idx : 9199 \t loss: 0.0872 \t train acc: 0.8750\n",
      "epoch: 2 \t batch_idx : 9299 \t loss: 0.1438 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 9399 \t loss: 0.1718 \t train acc: 0.7188\n",
      "epoch: 2 \t batch_idx : 9499 \t loss: 0.1470 \t train acc: 0.7500\n",
      "epoch: 2 \t batch_idx : 9599 \t loss: 0.0638 \t train acc: 0.9375\n",
      "epoch: 2 \t batch_idx : 9699 \t loss: 0.1440 \t train acc: 0.7500\n",
      "epoch: 2 \t batch_idx : 9799 \t loss: 0.1190 \t train acc: 0.8438\n",
      "epoch: 2 \t batch_idx : 9899 \t loss: 0.1473 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 9999 \t loss: 0.1228 \t train acc: 0.7812\n",
      "epoch: 2 \t batch_idx : 10099 \t loss: 0.1449 \t train acc: 0.7812\n",
      "epoch: 2 \t batch_idx : 10199 \t loss: 0.0864 \t train acc: 0.8438\n",
      "epoch: 2 \t batch_idx : 10299 \t loss: 0.0546 \t train acc: 0.9375\n",
      "epoch: 2 \t batch_idx : 10399 \t loss: 0.0743 \t train acc: 0.9062\n",
      "epoch: 2 \t batch_idx : 10499 \t loss: 0.1503 \t train acc: 0.7188\n",
      "epoch: 2 \t batch_idx : 10599 \t loss: 0.0978 \t train acc: 0.9375\n",
      "epoch: 2 \t batch_idx : 10699 \t loss: 0.1172 \t train acc: 0.7812\n",
      "epoch: 2 \t batch_idx : 10799 \t loss: 0.0646 \t train acc: 0.9062\n",
      "epoch: 2 \t batch_idx : 10899 \t loss: 0.1256 \t train acc: 0.7812\n",
      "epoch: 2 \t batch_idx : 10999 \t loss: 0.1337 \t train acc: 0.7812\n",
      "epoch: 2 \t batch_idx : 11099 \t loss: 0.0779 \t train acc: 0.8750\n",
      "epoch: 2 \t batch_idx : 11199 \t loss: 0.0816 \t train acc: 0.8750\n",
      "epoch: 2 \t batch_idx : 11299 \t loss: 0.1230 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 11399 \t loss: 0.1159 \t train acc: 0.8438\n",
      "epoch: 2 \t batch_idx : 11499 \t loss: 0.1782 \t train acc: 0.7188\n",
      "epoch: 2 \t batch_idx : 11599 \t loss: 0.2113 \t train acc: 0.6562\n",
      "epoch: 2 \t batch_idx : 11699 \t loss: 0.0842 \t train acc: 0.8438\n",
      "epoch: 2 \t batch_idx : 11799 \t loss: 0.0773 \t train acc: 0.9062\n",
      "epoch: 2 \t batch_idx : 11899 \t loss: 0.1299 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 11999 \t loss: 0.1163 \t train acc: 0.8438\n",
      "epoch: 2 \t batch_idx : 12099 \t loss: 0.1418 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 12199 \t loss: 0.1409 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 12299 \t loss: 0.1060 \t train acc: 0.7812\n",
      "epoch: 2 \t batch_idx : 12399 \t loss: 0.0834 \t train acc: 0.8750\n",
      "epoch: 2 \t batch_idx : 12499 \t loss: 0.1543 \t train acc: 0.7188\n",
      "epoch: 2 \t batch_idx : 12599 \t loss: 0.0420 \t train acc: 1.0000\n",
      "epoch: 2 \t batch_idx : 12699 \t loss: 0.1238 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 12799 \t loss: 0.1043 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 12899 \t loss: 0.1085 \t train acc: 0.8750\n",
      "epoch: 2 \t batch_idx : 12999 \t loss: 0.1201 \t train acc: 0.7812\n",
      "epoch: 2 \t batch_idx : 13099 \t loss: 0.1746 \t train acc: 0.7188\n",
      "epoch: 2 \t batch_idx : 13199 \t loss: 0.1028 \t train acc: 0.8438\n",
      "epoch: 2 \t batch_idx : 13299 \t loss: 0.0664 \t train acc: 0.8750\n",
      "epoch: 2 \t batch_idx : 13399 \t loss: 0.1206 \t train acc: 0.7500\n",
      "epoch: 2 \t batch_idx : 13499 \t loss: 0.1456 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 13599 \t loss: 0.1006 \t train acc: 0.8438\n",
      "epoch: 2 \t batch_idx : 13699 \t loss: 0.1225 \t train acc: 0.8438\n",
      "epoch: 2 \t batch_idx : 13799 \t loss: 0.0825 \t train acc: 0.9062\n",
      "epoch: 2 \t batch_idx : 13899 \t loss: 0.0845 \t train acc: 0.8438\n",
      "epoch: 2 \t batch_idx : 13999 \t loss: 0.1225 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 14099 \t loss: 0.1132 \t train acc: 0.8438\n",
      "epoch: 2 \t batch_idx : 14199 \t loss: 0.0691 \t train acc: 0.8438\n",
      "epoch: 2 \t batch_idx : 14299 \t loss: 0.0957 \t train acc: 0.8750\n",
      "epoch: 2 \t batch_idx : 14399 \t loss: 0.1098 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 14499 \t loss: 0.0865 \t train acc: 0.9062\n",
      "epoch: 2 \t batch_idx : 14599 \t loss: 0.0969 \t train acc: 0.8438\n",
      "epoch: 2 \t batch_idx : 14699 \t loss: 0.1078 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 14799 \t loss: 0.1125 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 14899 \t loss: 0.1502 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 14999 \t loss: 0.1027 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 15099 \t loss: 0.0930 \t train acc: 0.7812\n",
      "epoch: 2 \t batch_idx : 15199 \t loss: 0.1297 \t train acc: 0.7812\n",
      "epoch: 2 \t batch_idx : 15299 \t loss: 0.0803 \t train acc: 0.8750\n",
      "epoch: 2 \t batch_idx : 15399 \t loss: 0.1296 \t train acc: 0.7500\n",
      "epoch: 2 \t batch_idx : 15499 \t loss: 0.0985 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 15599 \t loss: 0.0942 \t train acc: 0.8438\n",
      "epoch: 2 \t batch_idx : 15699 \t loss: 0.1431 \t train acc: 0.7812\n",
      "epoch: 2 \t batch_idx : 15799 \t loss: 0.0734 \t train acc: 0.8750\n",
      "epoch: 2 \t batch_idx : 15899 \t loss: 0.1653 \t train acc: 0.7500\n",
      "epoch: 2 \t batch_idx : 15999 \t loss: 0.0864 \t train acc: 0.8750\n",
      "epoch: 2 \t batch_idx : 16099 \t loss: 0.1061 \t train acc: 0.8750\n",
      "epoch: 2 \t batch_idx : 16199 \t loss: 0.0558 \t train acc: 0.9375\n",
      "epoch: 2 \t batch_idx : 16299 \t loss: 0.0879 \t train acc: 0.9062\n",
      "epoch: 2 \t batch_idx : 16399 \t loss: 0.0723 \t train acc: 0.8438\n",
      "epoch: 2 \t batch_idx : 16499 \t loss: 0.1511 \t train acc: 0.7812\n",
      "epoch: 2 \t batch_idx : 16599 \t loss: 0.1660 \t train acc: 0.6875\n",
      "epoch: 2 \t batch_idx : 16699 \t loss: 0.0686 \t train acc: 0.9062\n",
      "epoch: 2 \t batch_idx : 16799 \t loss: 0.0989 \t train acc: 0.8125\n",
      "epoch: 2 \t batch_idx : 16899 \t loss: 0.0831 \t train acc: 0.9062\n",
      "val acc : 0.8200 > 0.8171 saving model\n",
      "test acc: 0.8200\n",
      "epoch: 3 \t batch_idx : 99 \t loss: 0.1250 \t train acc: 0.7812\n",
      "epoch: 3 \t batch_idx : 199 \t loss: 0.1349 \t train acc: 0.7500\n",
      "epoch: 3 \t batch_idx : 299 \t loss: 0.0649 \t train acc: 0.9375\n",
      "epoch: 3 \t batch_idx : 399 \t loss: 0.0828 \t train acc: 0.9375\n",
      "epoch: 3 \t batch_idx : 499 \t loss: 0.1951 \t train acc: 0.6875\n",
      "epoch: 3 \t batch_idx : 599 \t loss: 0.1598 \t train acc: 0.6250\n",
      "epoch: 3 \t batch_idx : 699 \t loss: 0.1087 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 799 \t loss: 0.0901 \t train acc: 0.9062\n",
      "epoch: 3 \t batch_idx : 899 \t loss: 0.1392 \t train acc: 0.7188\n",
      "epoch: 3 \t batch_idx : 999 \t loss: 0.1320 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 1099 \t loss: 0.0913 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 1199 \t loss: 0.1196 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 1299 \t loss: 0.0733 \t train acc: 0.9375\n",
      "epoch: 3 \t batch_idx : 1399 \t loss: 0.0840 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 1499 \t loss: 0.1531 \t train acc: 0.7188\n",
      "epoch: 3 \t batch_idx : 1599 \t loss: 0.1051 \t train acc: 0.8750\n",
      "epoch: 3 \t batch_idx : 1699 \t loss: 0.0654 \t train acc: 0.8750\n",
      "epoch: 3 \t batch_idx : 1799 \t loss: 0.1084 \t train acc: 0.8750\n",
      "epoch: 3 \t batch_idx : 1899 \t loss: 0.1234 \t train acc: 0.7500\n",
      "epoch: 3 \t batch_idx : 1999 \t loss: 0.1132 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 2099 \t loss: 0.1039 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 2199 \t loss: 0.0782 \t train acc: 0.9062\n",
      "epoch: 3 \t batch_idx : 2299 \t loss: 0.0855 \t train acc: 0.9062\n",
      "epoch: 3 \t batch_idx : 2399 \t loss: 0.0721 \t train acc: 0.8750\n",
      "epoch: 3 \t batch_idx : 2499 \t loss: 0.0638 \t train acc: 0.9375\n",
      "epoch: 3 \t batch_idx : 2599 \t loss: 0.1247 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 2699 \t loss: 0.1032 \t train acc: 0.7812\n",
      "epoch: 3 \t batch_idx : 2799 \t loss: 0.0827 \t train acc: 0.8750\n",
      "epoch: 3 \t batch_idx : 2899 \t loss: 0.1210 \t train acc: 0.8750\n",
      "epoch: 3 \t batch_idx : 2999 \t loss: 0.0434 \t train acc: 0.9375\n",
      "epoch: 3 \t batch_idx : 3099 \t loss: 0.1012 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 3199 \t loss: 0.0894 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 3299 \t loss: 0.0871 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 3399 \t loss: 0.1247 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 3499 \t loss: 0.1138 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 3599 \t loss: 0.1081 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 3699 \t loss: 0.0479 \t train acc: 0.9375\n",
      "epoch: 3 \t batch_idx : 3799 \t loss: 0.0839 \t train acc: 0.8750\n",
      "epoch: 3 \t batch_idx : 3899 \t loss: 0.1687 \t train acc: 0.7500\n",
      "epoch: 3 \t batch_idx : 3999 \t loss: 0.1119 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 4099 \t loss: 0.1360 \t train acc: 0.7188\n",
      "epoch: 3 \t batch_idx : 4199 \t loss: 0.0975 \t train acc: 0.9062\n",
      "epoch: 3 \t batch_idx : 4299 \t loss: 0.1246 \t train acc: 0.7812\n",
      "epoch: 3 \t batch_idx : 4399 \t loss: 0.0749 \t train acc: 0.8750\n",
      "epoch: 3 \t batch_idx : 4499 \t loss: 0.1089 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 4599 \t loss: 0.0589 \t train acc: 0.9375\n",
      "epoch: 3 \t batch_idx : 4699 \t loss: 0.1228 \t train acc: 0.7500\n",
      "epoch: 3 \t batch_idx : 4799 \t loss: 0.1147 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 4899 \t loss: 0.1741 \t train acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 \t batch_idx : 4999 \t loss: 0.0869 \t train acc: 0.9062\n",
      "epoch: 3 \t batch_idx : 5099 \t loss: 0.1015 \t train acc: 0.7812\n",
      "epoch: 3 \t batch_idx : 5199 \t loss: 0.1103 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 5299 \t loss: 0.0636 \t train acc: 0.9062\n",
      "epoch: 3 \t batch_idx : 5399 \t loss: 0.1499 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 5499 \t loss: 0.0855 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 5599 \t loss: 0.1192 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 5699 \t loss: 0.1166 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 5799 \t loss: 0.0809 \t train acc: 0.9062\n",
      "epoch: 3 \t batch_idx : 5899 \t loss: 0.0763 \t train acc: 0.9062\n",
      "epoch: 3 \t batch_idx : 5999 \t loss: 0.0600 \t train acc: 0.9375\n",
      "epoch: 3 \t batch_idx : 6099 \t loss: 0.1308 \t train acc: 0.7812\n",
      "epoch: 3 \t batch_idx : 6199 \t loss: 0.1155 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 6299 \t loss: 0.1118 \t train acc: 0.7812\n",
      "epoch: 3 \t batch_idx : 6399 \t loss: 0.0760 \t train acc: 0.9062\n",
      "epoch: 3 \t batch_idx : 6499 \t loss: 0.0869 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 6599 \t loss: 0.0670 \t train acc: 0.9375\n",
      "epoch: 3 \t batch_idx : 6699 \t loss: 0.0910 \t train acc: 0.8750\n",
      "epoch: 3 \t batch_idx : 6799 \t loss: 0.0988 \t train acc: 0.9062\n",
      "epoch: 3 \t batch_idx : 6899 \t loss: 0.0874 \t train acc: 0.9062\n",
      "epoch: 3 \t batch_idx : 6999 \t loss: 0.1066 \t train acc: 0.8750\n",
      "epoch: 3 \t batch_idx : 7099 \t loss: 0.0942 \t train acc: 0.8750\n",
      "epoch: 3 \t batch_idx : 7199 \t loss: 0.1029 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 7299 \t loss: 0.1058 \t train acc: 0.9062\n",
      "epoch: 3 \t batch_idx : 7399 \t loss: 0.0813 \t train acc: 0.8750\n",
      "epoch: 3 \t batch_idx : 7499 \t loss: 0.1557 \t train acc: 0.7812\n",
      "epoch: 3 \t batch_idx : 7599 \t loss: 0.0959 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 7699 \t loss: 0.1639 \t train acc: 0.7500\n",
      "epoch: 3 \t batch_idx : 7799 \t loss: 0.1346 \t train acc: 0.7812\n",
      "epoch: 3 \t batch_idx : 7899 \t loss: 0.0865 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 7999 \t loss: 0.1493 \t train acc: 0.7188\n",
      "epoch: 3 \t batch_idx : 8099 \t loss: 0.1096 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 8199 \t loss: 0.1106 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 8299 \t loss: 0.0935 \t train acc: 0.8750\n",
      "epoch: 3 \t batch_idx : 8399 \t loss: 0.0363 \t train acc: 1.0000\n",
      "epoch: 3 \t batch_idx : 8499 \t loss: 0.1337 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 8599 \t loss: 0.1053 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 8699 \t loss: 0.0971 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 8799 \t loss: 0.0678 \t train acc: 0.9062\n",
      "epoch: 3 \t batch_idx : 8899 \t loss: 0.0692 \t train acc: 0.9062\n",
      "epoch: 3 \t batch_idx : 8999 \t loss: 0.0945 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 9099 \t loss: 0.1301 \t train acc: 0.7812\n",
      "epoch: 3 \t batch_idx : 9199 \t loss: 0.1739 \t train acc: 0.7188\n",
      "epoch: 3 \t batch_idx : 9299 \t loss: 0.0845 \t train acc: 0.9375\n",
      "epoch: 3 \t batch_idx : 9399 \t loss: 0.1048 \t train acc: 0.7812\n",
      "epoch: 3 \t batch_idx : 9499 \t loss: 0.1075 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 9599 \t loss: 0.1410 \t train acc: 0.7812\n",
      "epoch: 3 \t batch_idx : 9699 \t loss: 0.0653 \t train acc: 0.8750\n",
      "epoch: 3 \t batch_idx : 9799 \t loss: 0.0773 \t train acc: 0.8750\n",
      "epoch: 3 \t batch_idx : 9899 \t loss: 0.1352 \t train acc: 0.8750\n",
      "epoch: 3 \t batch_idx : 9999 \t loss: 0.1006 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 10099 \t loss: 0.1174 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 10199 \t loss: 0.0938 \t train acc: 0.9062\n",
      "epoch: 3 \t batch_idx : 10299 \t loss: 0.0917 \t train acc: 0.9062\n",
      "epoch: 3 \t batch_idx : 10399 \t loss: 0.1469 \t train acc: 0.7188\n",
      "epoch: 3 \t batch_idx : 10499 \t loss: 0.1006 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 10599 \t loss: 0.1044 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 10699 \t loss: 0.1225 \t train acc: 0.7500\n",
      "epoch: 3 \t batch_idx : 10799 \t loss: 0.1036 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 10899 \t loss: 0.1189 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 10999 \t loss: 0.1116 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 11099 \t loss: 0.0831 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 11199 \t loss: 0.1317 \t train acc: 0.6875\n",
      "epoch: 3 \t batch_idx : 11299 \t loss: 0.1940 \t train acc: 0.6875\n",
      "epoch: 3 \t batch_idx : 11399 \t loss: 0.0585 \t train acc: 0.9375\n",
      "epoch: 3 \t batch_idx : 11499 \t loss: 0.1373 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 11599 \t loss: 0.1153 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 11699 \t loss: 0.0953 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 11799 \t loss: 0.1460 \t train acc: 0.7188\n",
      "epoch: 3 \t batch_idx : 11899 \t loss: 0.1387 \t train acc: 0.7812\n",
      "epoch: 3 \t batch_idx : 11999 \t loss: 0.1044 \t train acc: 0.8750\n",
      "epoch: 3 \t batch_idx : 12099 \t loss: 0.0929 \t train acc: 0.8750\n",
      "epoch: 3 \t batch_idx : 12199 \t loss: 0.1471 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 12299 \t loss: 0.1626 \t train acc: 0.7188\n",
      "epoch: 3 \t batch_idx : 12399 \t loss: 0.0774 \t train acc: 0.8750\n",
      "epoch: 3 \t batch_idx : 12499 \t loss: 0.1310 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 12599 \t loss: 0.1068 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 12699 \t loss: 0.1159 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 12799 \t loss: 0.0705 \t train acc: 0.9062\n",
      "epoch: 3 \t batch_idx : 12899 \t loss: 0.1155 \t train acc: 0.7812\n",
      "epoch: 3 \t batch_idx : 12999 \t loss: 0.1143 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 13099 \t loss: 0.1265 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 13199 \t loss: 0.0913 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 13299 \t loss: 0.0742 \t train acc: 0.9062\n",
      "epoch: 3 \t batch_idx : 13399 \t loss: 0.1451 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 13499 \t loss: 0.1026 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 13599 \t loss: 0.1113 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 13699 \t loss: 0.0841 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 13799 \t loss: 0.1040 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 13899 \t loss: 0.1382 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 13999 \t loss: 0.1301 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 14099 \t loss: 0.1306 \t train acc: 0.7188\n",
      "epoch: 3 \t batch_idx : 14199 \t loss: 0.0860 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 14299 \t loss: 0.1110 \t train acc: 0.7812\n",
      "epoch: 3 \t batch_idx : 14399 \t loss: 0.1016 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 14499 \t loss: 0.1101 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 14599 \t loss: 0.0786 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 14699 \t loss: 0.1813 \t train acc: 0.6875\n",
      "epoch: 3 \t batch_idx : 14799 \t loss: 0.1284 \t train acc: 0.7812\n",
      "epoch: 3 \t batch_idx : 14899 \t loss: 0.0632 \t train acc: 0.8750\n",
      "epoch: 3 \t batch_idx : 14999 \t loss: 0.0887 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 15099 \t loss: 0.1124 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 15199 \t loss: 0.0993 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 15299 \t loss: 0.1127 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 15399 \t loss: 0.1013 \t train acc: 0.8750\n",
      "epoch: 3 \t batch_idx : 15499 \t loss: 0.0841 \t train acc: 0.9062\n",
      "epoch: 3 \t batch_idx : 15599 \t loss: 0.1038 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 15699 \t loss: 0.0725 \t train acc: 0.9062\n",
      "epoch: 3 \t batch_idx : 15799 \t loss: 0.1082 \t train acc: 0.8125\n",
      "epoch: 3 \t batch_idx : 15899 \t loss: 0.0725 \t train acc: 0.9062\n",
      "epoch: 3 \t batch_idx : 15999 \t loss: 0.1710 \t train acc: 0.6562\n",
      "epoch: 3 \t batch_idx : 16099 \t loss: 0.0729 \t train acc: 0.8750\n",
      "epoch: 3 \t batch_idx : 16199 \t loss: 0.1213 \t train acc: 0.7188\n",
      "epoch: 3 \t batch_idx : 16299 \t loss: 0.0549 \t train acc: 1.0000\n",
      "epoch: 3 \t batch_idx : 16399 \t loss: 0.0738 \t train acc: 0.8750\n",
      "epoch: 3 \t batch_idx : 16499 \t loss: 0.1050 \t train acc: 0.8750\n",
      "epoch: 3 \t batch_idx : 16599 \t loss: 0.0960 \t train acc: 0.8438\n",
      "epoch: 3 \t batch_idx : 16699 \t loss: 0.0921 \t train acc: 0.9062\n",
      "epoch: 3 \t batch_idx : 16799 \t loss: 0.1374 \t train acc: 0.7500\n",
      "epoch: 3 \t batch_idx : 16899 \t loss: 0.0957 \t train acc: 0.9062\n",
      "test acc: 0.8198\n",
      "epoch: 4 \t batch_idx : 99 \t loss: 0.0407 \t train acc: 0.9688\n",
      "epoch: 4 \t batch_idx : 199 \t loss: 0.0735 \t train acc: 0.9375\n",
      "epoch: 4 \t batch_idx : 299 \t loss: 0.0680 \t train acc: 0.9375\n",
      "epoch: 4 \t batch_idx : 399 \t loss: 0.0925 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 499 \t loss: 0.0790 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 599 \t loss: 0.0720 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 699 \t loss: 0.0806 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 799 \t loss: 0.0733 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 899 \t loss: 0.0686 \t train acc: 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 \t batch_idx : 999 \t loss: 0.1304 \t train acc: 0.7812\n",
      "epoch: 4 \t batch_idx : 1099 \t loss: 0.0459 \t train acc: 0.9375\n",
      "epoch: 4 \t batch_idx : 1199 \t loss: 0.0780 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 1299 \t loss: 0.0845 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 1399 \t loss: 0.0992 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 1499 \t loss: 0.0888 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 1599 \t loss: 0.1028 \t train acc: 0.8125\n",
      "epoch: 4 \t batch_idx : 1699 \t loss: 0.1988 \t train acc: 0.7188\n",
      "epoch: 4 \t batch_idx : 1799 \t loss: 0.0756 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 1899 \t loss: 0.0771 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 1999 \t loss: 0.1266 \t train acc: 0.8125\n",
      "epoch: 4 \t batch_idx : 2099 \t loss: 0.0663 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 2199 \t loss: 0.0955 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 2299 \t loss: 0.0693 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 2399 \t loss: 0.0905 \t train acc: 0.8125\n",
      "epoch: 4 \t batch_idx : 2499 \t loss: 0.1001 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 2599 \t loss: 0.0563 \t train acc: 0.9375\n",
      "epoch: 4 \t batch_idx : 2699 \t loss: 0.0580 \t train acc: 0.9375\n",
      "epoch: 4 \t batch_idx : 2799 \t loss: 0.1088 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 2899 \t loss: 0.0874 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 2999 \t loss: 0.0494 \t train acc: 0.9375\n",
      "epoch: 4 \t batch_idx : 3099 \t loss: 0.0514 \t train acc: 0.9375\n",
      "epoch: 4 \t batch_idx : 3199 \t loss: 0.0479 \t train acc: 0.9688\n",
      "epoch: 4 \t batch_idx : 3299 \t loss: 0.0771 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 3399 \t loss: 0.0594 \t train acc: 0.9375\n",
      "epoch: 4 \t batch_idx : 3499 \t loss: 0.0813 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 3599 \t loss: 0.1100 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 3699 \t loss: 0.1041 \t train acc: 0.8125\n",
      "epoch: 4 \t batch_idx : 3799 \t loss: 0.1027 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 3899 \t loss: 0.0314 \t train acc: 0.9688\n",
      "epoch: 4 \t batch_idx : 3999 \t loss: 0.0876 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 4099 \t loss: 0.1232 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 4199 \t loss: 0.0768 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 4299 \t loss: 0.0801 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 4399 \t loss: 0.0647 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 4499 \t loss: 0.0837 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 4599 \t loss: 0.0834 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 4699 \t loss: 0.1271 \t train acc: 0.7500\n",
      "epoch: 4 \t batch_idx : 4799 \t loss: 0.0866 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 4899 \t loss: 0.1472 \t train acc: 0.7500\n",
      "epoch: 4 \t batch_idx : 4999 \t loss: 0.0505 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 5099 \t loss: 0.1054 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 5199 \t loss: 0.0851 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 5299 \t loss: 0.0732 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 5399 \t loss: 0.1144 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 5499 \t loss: 0.1044 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 5599 \t loss: 0.0759 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 5699 \t loss: 0.1474 \t train acc: 0.7500\n",
      "epoch: 4 \t batch_idx : 5799 \t loss: 0.0713 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 5899 \t loss: 0.0676 \t train acc: 0.9375\n",
      "epoch: 4 \t batch_idx : 5999 \t loss: 0.1002 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 6099 \t loss: 0.0978 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 6199 \t loss: 0.0489 \t train acc: 0.9688\n",
      "epoch: 4 \t batch_idx : 6299 \t loss: 0.0879 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 6399 \t loss: 0.1093 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 6499 \t loss: 0.0702 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 6599 \t loss: 0.0597 \t train acc: 0.9375\n",
      "epoch: 4 \t batch_idx : 6699 \t loss: 0.0942 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 6799 \t loss: 0.0868 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 6899 \t loss: 0.0987 \t train acc: 0.7812\n",
      "epoch: 4 \t batch_idx : 6999 \t loss: 0.0612 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 7099 \t loss: 0.0803 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 7199 \t loss: 0.0858 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 7299 \t loss: 0.0940 \t train acc: 0.8125\n",
      "epoch: 4 \t batch_idx : 7399 \t loss: 0.0799 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 7499 \t loss: 0.1292 \t train acc: 0.8125\n",
      "epoch: 4 \t batch_idx : 7599 \t loss: 0.0652 \t train acc: 0.9375\n",
      "epoch: 4 \t batch_idx : 7699 \t loss: 0.0557 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 7799 \t loss: 0.1466 \t train acc: 0.7812\n",
      "epoch: 4 \t batch_idx : 7899 \t loss: 0.0672 \t train acc: 0.9375\n",
      "epoch: 4 \t batch_idx : 7999 \t loss: 0.1011 \t train acc: 0.7812\n",
      "epoch: 4 \t batch_idx : 8099 \t loss: 0.1665 \t train acc: 0.7812\n",
      "epoch: 4 \t batch_idx : 8199 \t loss: 0.0571 \t train acc: 0.9375\n",
      "epoch: 4 \t batch_idx : 8299 \t loss: 0.0672 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 8399 \t loss: 0.1000 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 8499 \t loss: 0.0645 \t train acc: 0.9375\n",
      "epoch: 4 \t batch_idx : 8599 \t loss: 0.1012 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 8699 \t loss: 0.1144 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 8799 \t loss: 0.1037 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 8899 \t loss: 0.0907 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 8999 \t loss: 0.1335 \t train acc: 0.7812\n",
      "epoch: 4 \t batch_idx : 9099 \t loss: 0.0557 \t train acc: 0.9375\n",
      "epoch: 4 \t batch_idx : 9199 \t loss: 0.1228 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 9299 \t loss: 0.1003 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 9399 \t loss: 0.0808 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 9499 \t loss: 0.0652 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 9599 \t loss: 0.0641 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 9699 \t loss: 0.0747 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 9799 \t loss: 0.0940 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 9899 \t loss: 0.0858 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 9999 \t loss: 0.0630 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 10099 \t loss: 0.0644 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 10199 \t loss: 0.0474 \t train acc: 0.9688\n",
      "epoch: 4 \t batch_idx : 10299 \t loss: 0.0580 \t train acc: 0.9375\n",
      "epoch: 4 \t batch_idx : 10399 \t loss: 0.0922 \t train acc: 0.8125\n",
      "epoch: 4 \t batch_idx : 10499 \t loss: 0.0697 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 10599 \t loss: 0.0459 \t train acc: 0.9688\n",
      "epoch: 4 \t batch_idx : 10699 \t loss: 0.0930 \t train acc: 0.8125\n",
      "epoch: 4 \t batch_idx : 10799 \t loss: 0.1019 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 10899 \t loss: 0.1036 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 10999 \t loss: 0.1139 \t train acc: 0.8125\n",
      "epoch: 4 \t batch_idx : 11099 \t loss: 0.0941 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 11199 \t loss: 0.0959 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 11299 \t loss: 0.0898 \t train acc: 0.8125\n",
      "epoch: 4 \t batch_idx : 11399 \t loss: 0.1312 \t train acc: 0.8125\n",
      "epoch: 4 \t batch_idx : 11499 \t loss: 0.1010 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 11599 \t loss: 0.1315 \t train acc: 0.8125\n",
      "epoch: 4 \t batch_idx : 11699 \t loss: 0.0864 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 11799 \t loss: 0.1617 \t train acc: 0.7500\n",
      "epoch: 4 \t batch_idx : 11899 \t loss: 0.1004 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 11999 \t loss: 0.0580 \t train acc: 0.9688\n",
      "epoch: 4 \t batch_idx : 12099 \t loss: 0.1133 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 12199 \t loss: 0.1122 \t train acc: 0.8125\n",
      "epoch: 4 \t batch_idx : 12299 \t loss: 0.1343 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 12399 \t loss: 0.0979 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 12499 \t loss: 0.0714 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 12599 \t loss: 0.1442 \t train acc: 0.7500\n",
      "epoch: 4 \t batch_idx : 12699 \t loss: 0.1251 \t train acc: 0.8125\n",
      "epoch: 4 \t batch_idx : 12799 \t loss: 0.1128 \t train acc: 0.7812\n",
      "epoch: 4 \t batch_idx : 12899 \t loss: 0.0706 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 12999 \t loss: 0.0974 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 13099 \t loss: 0.1062 \t train acc: 0.8125\n",
      "epoch: 4 \t batch_idx : 13199 \t loss: 0.1060 \t train acc: 0.7812\n",
      "epoch: 4 \t batch_idx : 13299 \t loss: 0.0600 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 13399 \t loss: 0.0844 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 13499 \t loss: 0.1817 \t train acc: 0.6875\n",
      "epoch: 4 \t batch_idx : 13599 \t loss: 0.0882 \t train acc: 0.8125\n",
      "epoch: 4 \t batch_idx : 13699 \t loss: 0.0730 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 13799 \t loss: 0.1299 \t train acc: 0.7812\n",
      "epoch: 4 \t batch_idx : 13899 \t loss: 0.1362 \t train acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 \t batch_idx : 13999 \t loss: 0.0891 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 14099 \t loss: 0.1423 \t train acc: 0.7812\n",
      "epoch: 4 \t batch_idx : 14199 \t loss: 0.1100 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 14299 \t loss: 0.0501 \t train acc: 0.9375\n",
      "epoch: 4 \t batch_idx : 14399 \t loss: 0.0412 \t train acc: 0.9688\n",
      "epoch: 4 \t batch_idx : 14499 \t loss: 0.0788 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 14599 \t loss: 0.1044 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 14699 \t loss: 0.1408 \t train acc: 0.7812\n",
      "epoch: 4 \t batch_idx : 14799 \t loss: 0.0835 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 14899 \t loss: 0.0890 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 14999 \t loss: 0.0749 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 15099 \t loss: 0.0788 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 15199 \t loss: 0.0655 \t train acc: 0.9375\n",
      "epoch: 4 \t batch_idx : 15299 \t loss: 0.0683 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 15399 \t loss: 0.1201 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 15499 \t loss: 0.1094 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 15599 \t loss: 0.0835 \t train acc: 0.8750\n",
      "epoch: 4 \t batch_idx : 15699 \t loss: 0.1107 \t train acc: 0.8125\n",
      "epoch: 4 \t batch_idx : 15799 \t loss: 0.1000 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 15899 \t loss: 0.0842 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 15999 \t loss: 0.0443 \t train acc: 0.9375\n",
      "epoch: 4 \t batch_idx : 16099 \t loss: 0.0677 \t train acc: 0.9375\n",
      "epoch: 4 \t batch_idx : 16199 \t loss: 0.0324 \t train acc: 1.0000\n",
      "epoch: 4 \t batch_idx : 16299 \t loss: 0.1573 \t train acc: 0.6562\n",
      "epoch: 4 \t batch_idx : 16399 \t loss: 0.0428 \t train acc: 0.9688\n",
      "epoch: 4 \t batch_idx : 16499 \t loss: 0.0693 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 16599 \t loss: 0.0764 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 16699 \t loss: 0.0621 \t train acc: 0.9062\n",
      "epoch: 4 \t batch_idx : 16799 \t loss: 0.0931 \t train acc: 0.8438\n",
      "epoch: 4 \t batch_idx : 16899 \t loss: 0.0932 \t train acc: 0.8125\n",
      "val acc : 0.8203 > 0.8200 saving model\n",
      "test acc: 0.8203\n",
      "epoch: 5 \t batch_idx : 99 \t loss: 0.0719 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 199 \t loss: 0.0555 \t train acc: 0.9375\n",
      "epoch: 5 \t batch_idx : 299 \t loss: 0.1211 \t train acc: 0.8125\n",
      "epoch: 5 \t batch_idx : 399 \t loss: 0.0767 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 499 \t loss: 0.0507 \t train acc: 0.9375\n",
      "epoch: 5 \t batch_idx : 599 \t loss: 0.0586 \t train acc: 0.9375\n",
      "epoch: 5 \t batch_idx : 699 \t loss: 0.0745 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 799 \t loss: 0.0627 \t train acc: 0.9375\n",
      "epoch: 5 \t batch_idx : 899 \t loss: 0.0735 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 999 \t loss: 0.0525 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 1099 \t loss: 0.0887 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 1199 \t loss: 0.0516 \t train acc: 0.9375\n",
      "epoch: 5 \t batch_idx : 1299 \t loss: 0.0766 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 1399 \t loss: 0.1089 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 1499 \t loss: 0.1023 \t train acc: 0.8438\n",
      "epoch: 5 \t batch_idx : 1599 \t loss: 0.0579 \t train acc: 0.9688\n",
      "epoch: 5 \t batch_idx : 1699 \t loss: 0.0854 \t train acc: 0.8125\n",
      "epoch: 5 \t batch_idx : 1799 \t loss: 0.1149 \t train acc: 0.8438\n",
      "epoch: 5 \t batch_idx : 1899 \t loss: 0.0321 \t train acc: 0.9375\n",
      "epoch: 5 \t batch_idx : 1999 \t loss: 0.0759 \t train acc: 0.8438\n",
      "epoch: 5 \t batch_idx : 2099 \t loss: 0.0959 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 2199 \t loss: 0.0941 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 2299 \t loss: 0.0639 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 2399 \t loss: 0.0711 \t train acc: 0.9375\n",
      "epoch: 5 \t batch_idx : 2499 \t loss: 0.0607 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 2599 \t loss: 0.0318 \t train acc: 0.9688\n",
      "epoch: 5 \t batch_idx : 2699 \t loss: 0.0767 \t train acc: 0.9375\n",
      "epoch: 5 \t batch_idx : 2799 \t loss: 0.0330 \t train acc: 0.9688\n",
      "epoch: 5 \t batch_idx : 2899 \t loss: 0.1039 \t train acc: 0.8438\n",
      "epoch: 5 \t batch_idx : 2999 \t loss: 0.1365 \t train acc: 0.8125\n",
      "epoch: 5 \t batch_idx : 3099 \t loss: 0.0580 \t train acc: 0.9375\n",
      "epoch: 5 \t batch_idx : 3199 \t loss: 0.0858 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 3299 \t loss: 0.0448 \t train acc: 0.9375\n",
      "epoch: 5 \t batch_idx : 3399 \t loss: 0.0883 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 3499 \t loss: 0.0319 \t train acc: 0.9688\n",
      "epoch: 5 \t batch_idx : 3599 \t loss: 0.0735 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 3699 \t loss: 0.0584 \t train acc: 0.9375\n",
      "epoch: 5 \t batch_idx : 3799 \t loss: 0.0459 \t train acc: 0.9375\n",
      "epoch: 5 \t batch_idx : 3899 \t loss: 0.0747 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 3999 \t loss: 0.0504 \t train acc: 0.9688\n",
      "epoch: 5 \t batch_idx : 4099 \t loss: 0.0730 \t train acc: 0.8438\n",
      "epoch: 5 \t batch_idx : 4199 \t loss: 0.0705 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 4299 \t loss: 0.0673 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 4399 \t loss: 0.0617 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 4499 \t loss: 0.0962 \t train acc: 0.8125\n",
      "epoch: 5 \t batch_idx : 4599 \t loss: 0.1104 \t train acc: 0.8438\n",
      "epoch: 5 \t batch_idx : 4699 \t loss: 0.0975 \t train acc: 0.7812\n",
      "epoch: 5 \t batch_idx : 4799 \t loss: 0.0727 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 4899 \t loss: 0.0779 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 4999 \t loss: 0.0809 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 5099 \t loss: 0.0492 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 5199 \t loss: 0.0565 \t train acc: 0.9375\n",
      "epoch: 5 \t batch_idx : 5299 \t loss: 0.0568 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 5399 \t loss: 0.0565 \t train acc: 0.9375\n",
      "epoch: 5 \t batch_idx : 5499 \t loss: 0.0669 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 5599 \t loss: 0.0767 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 5699 \t loss: 0.0955 \t train acc: 0.8438\n",
      "epoch: 5 \t batch_idx : 5799 \t loss: 0.0680 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 5899 \t loss: 0.0563 \t train acc: 0.9375\n",
      "epoch: 5 \t batch_idx : 5999 \t loss: 0.1470 \t train acc: 0.8125\n",
      "epoch: 5 \t batch_idx : 6099 \t loss: 0.0679 \t train acc: 0.9375\n",
      "epoch: 5 \t batch_idx : 6199 \t loss: 0.1391 \t train acc: 0.8125\n",
      "epoch: 5 \t batch_idx : 6299 \t loss: 0.0729 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 6399 \t loss: 0.1138 \t train acc: 0.8125\n",
      "epoch: 5 \t batch_idx : 6499 \t loss: 0.1178 \t train acc: 0.7500\n",
      "epoch: 5 \t batch_idx : 6599 \t loss: 0.1693 \t train acc: 0.7188\n",
      "epoch: 5 \t batch_idx : 6699 \t loss: 0.0368 \t train acc: 0.9688\n",
      "epoch: 5 \t batch_idx : 6799 \t loss: 0.0200 \t train acc: 1.0000\n",
      "epoch: 5 \t batch_idx : 6899 \t loss: 0.1522 \t train acc: 0.7812\n",
      "epoch: 5 \t batch_idx : 6999 \t loss: 0.1123 \t train acc: 0.8438\n",
      "epoch: 5 \t batch_idx : 7099 \t loss: 0.1262 \t train acc: 0.7812\n",
      "epoch: 5 \t batch_idx : 7199 \t loss: 0.1060 \t train acc: 0.8125\n",
      "epoch: 5 \t batch_idx : 7299 \t loss: 0.0679 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 7399 \t loss: 0.0645 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 7499 \t loss: 0.1415 \t train acc: 0.7500\n",
      "epoch: 5 \t batch_idx : 7599 \t loss: 0.1197 \t train acc: 0.8438\n",
      "epoch: 5 \t batch_idx : 7699 \t loss: 0.1129 \t train acc: 0.8438\n",
      "epoch: 5 \t batch_idx : 7799 \t loss: 0.0941 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 7899 \t loss: 0.0677 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 7999 \t loss: 0.1310 \t train acc: 0.8125\n",
      "epoch: 5 \t batch_idx : 8099 \t loss: 0.0951 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 8199 \t loss: 0.1062 \t train acc: 0.8125\n",
      "epoch: 5 \t batch_idx : 8299 \t loss: 0.0893 \t train acc: 0.8438\n",
      "epoch: 5 \t batch_idx : 8399 \t loss: 0.0955 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 8499 \t loss: 0.0799 \t train acc: 0.8438\n",
      "epoch: 5 \t batch_idx : 8599 \t loss: 0.0407 \t train acc: 1.0000\n",
      "epoch: 5 \t batch_idx : 8699 \t loss: 0.1628 \t train acc: 0.7500\n",
      "epoch: 5 \t batch_idx : 8799 \t loss: 0.0346 \t train acc: 0.9688\n",
      "epoch: 5 \t batch_idx : 8899 \t loss: 0.1059 \t train acc: 0.8125\n",
      "epoch: 5 \t batch_idx : 8999 \t loss: 0.1160 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 9099 \t loss: 0.1452 \t train acc: 0.7812\n",
      "epoch: 5 \t batch_idx : 9199 \t loss: 0.0310 \t train acc: 0.9688\n",
      "epoch: 5 \t batch_idx : 9299 \t loss: 0.0852 \t train acc: 0.8438\n",
      "epoch: 5 \t batch_idx : 9399 \t loss: 0.0559 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 9499 \t loss: 0.0622 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 9599 \t loss: 0.1230 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 9699 \t loss: 0.0321 \t train acc: 0.9688\n",
      "epoch: 5 \t batch_idx : 9799 \t loss: 0.1106 \t train acc: 0.7500\n",
      "epoch: 5 \t batch_idx : 9899 \t loss: 0.0385 \t train acc: 0.9688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 \t batch_idx : 9999 \t loss: 0.1015 \t train acc: 0.8438\n",
      "epoch: 5 \t batch_idx : 10099 \t loss: 0.1016 \t train acc: 0.8125\n",
      "epoch: 5 \t batch_idx : 10199 \t loss: 0.0822 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 10299 \t loss: 0.0555 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 10399 \t loss: 0.0521 \t train acc: 0.9375\n",
      "epoch: 5 \t batch_idx : 10499 \t loss: 0.0934 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 10599 \t loss: 0.1072 \t train acc: 0.8125\n",
      "epoch: 5 \t batch_idx : 10699 \t loss: 0.1171 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 10799 \t loss: 0.1938 \t train acc: 0.7500\n",
      "epoch: 5 \t batch_idx : 10899 \t loss: 0.0830 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 10999 \t loss: 0.1140 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 11099 \t loss: 0.1017 \t train acc: 0.8438\n",
      "epoch: 5 \t batch_idx : 11199 \t loss: 0.0774 \t train acc: 0.8438\n",
      "epoch: 5 \t batch_idx : 11299 \t loss: 0.0847 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 11399 \t loss: 0.0503 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 11499 \t loss: 0.1741 \t train acc: 0.7812\n",
      "epoch: 5 \t batch_idx : 11599 \t loss: 0.0942 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 11699 \t loss: 0.1302 \t train acc: 0.8125\n",
      "epoch: 5 \t batch_idx : 11799 \t loss: 0.1484 \t train acc: 0.7500\n",
      "epoch: 5 \t batch_idx : 11899 \t loss: 0.1094 \t train acc: 0.8438\n",
      "epoch: 5 \t batch_idx : 11999 \t loss: 0.0237 \t train acc: 0.9688\n",
      "epoch: 5 \t batch_idx : 12099 \t loss: 0.0568 \t train acc: 0.9375\n",
      "epoch: 5 \t batch_idx : 12199 \t loss: 0.0973 \t train acc: 0.8438\n",
      "epoch: 5 \t batch_idx : 12299 \t loss: 0.0621 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 12399 \t loss: 0.0903 \t train acc: 0.8438\n",
      "epoch: 5 \t batch_idx : 12499 \t loss: 0.0869 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 12599 \t loss: 0.0930 \t train acc: 0.8438\n",
      "epoch: 5 \t batch_idx : 12699 \t loss: 0.0912 \t train acc: 0.8438\n",
      "epoch: 5 \t batch_idx : 12799 \t loss: 0.1481 \t train acc: 0.8438\n",
      "epoch: 5 \t batch_idx : 12899 \t loss: 0.0821 \t train acc: 0.8438\n",
      "epoch: 5 \t batch_idx : 12999 \t loss: 0.0727 \t train acc: 0.9375\n",
      "epoch: 5 \t batch_idx : 13099 \t loss: 0.0849 \t train acc: 0.8438\n",
      "epoch: 5 \t batch_idx : 13199 \t loss: 0.0834 \t train acc: 0.9375\n",
      "epoch: 5 \t batch_idx : 13299 \t loss: 0.0800 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 13399 \t loss: 0.0747 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 13499 \t loss: 0.0390 \t train acc: 0.9688\n",
      "epoch: 5 \t batch_idx : 13599 \t loss: 0.0740 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 13699 \t loss: 0.0488 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 13799 \t loss: 0.0967 \t train acc: 0.8438\n",
      "epoch: 5 \t batch_idx : 13899 \t loss: 0.0341 \t train acc: 0.9688\n",
      "epoch: 5 \t batch_idx : 13999 \t loss: 0.0860 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 14099 \t loss: 0.1388 \t train acc: 0.7500\n",
      "epoch: 5 \t batch_idx : 14199 \t loss: 0.1045 \t train acc: 0.8125\n",
      "epoch: 5 \t batch_idx : 14299 \t loss: 0.0921 \t train acc: 0.8438\n",
      "epoch: 5 \t batch_idx : 14399 \t loss: 0.0957 \t train acc: 0.8438\n",
      "epoch: 5 \t batch_idx : 14499 \t loss: 0.1015 \t train acc: 0.8438\n",
      "epoch: 5 \t batch_idx : 14599 \t loss: 0.0691 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 14699 \t loss: 0.0331 \t train acc: 0.9688\n",
      "epoch: 5 \t batch_idx : 14799 \t loss: 0.0859 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 14899 \t loss: 0.1034 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 14999 \t loss: 0.0930 \t train acc: 0.8438\n",
      "epoch: 5 \t batch_idx : 15099 \t loss: 0.0608 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 15199 \t loss: 0.1168 \t train acc: 0.8438\n",
      "epoch: 5 \t batch_idx : 15299 \t loss: 0.0454 \t train acc: 0.9375\n",
      "epoch: 5 \t batch_idx : 15399 \t loss: 0.0769 \t train acc: 0.9375\n",
      "epoch: 5 \t batch_idx : 15499 \t loss: 0.1754 \t train acc: 0.7188\n",
      "epoch: 5 \t batch_idx : 15599 \t loss: 0.0684 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 15699 \t loss: 0.0616 \t train acc: 0.9375\n",
      "epoch: 5 \t batch_idx : 15799 \t loss: 0.0829 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 15899 \t loss: 0.0390 \t train acc: 0.9688\n",
      "epoch: 5 \t batch_idx : 15999 \t loss: 0.1039 \t train acc: 0.8438\n",
      "epoch: 5 \t batch_idx : 16099 \t loss: 0.0652 \t train acc: 0.9375\n",
      "epoch: 5 \t batch_idx : 16199 \t loss: 0.0959 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 16299 \t loss: 0.0798 \t train acc: 0.8125\n",
      "epoch: 5 \t batch_idx : 16399 \t loss: 0.0685 \t train acc: 0.8438\n",
      "epoch: 5 \t batch_idx : 16499 \t loss: 0.1176 \t train acc: 0.8750\n",
      "epoch: 5 \t batch_idx : 16599 \t loss: 0.0298 \t train acc: 0.9688\n",
      "epoch: 5 \t batch_idx : 16699 \t loss: 0.0377 \t train acc: 0.9375\n",
      "epoch: 5 \t batch_idx : 16799 \t loss: 0.0765 \t train acc: 0.9062\n",
      "epoch: 5 \t batch_idx : 16899 \t loss: 0.0550 \t train acc: 0.9375\n",
      "test acc: 0.8150\n",
      "epoch: 6 \t batch_idx : 99 \t loss: 0.1053 \t train acc: 0.8438\n",
      "epoch: 6 \t batch_idx : 199 \t loss: 0.0522 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 299 \t loss: 0.0796 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 399 \t loss: 0.0821 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 499 \t loss: 0.0475 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 599 \t loss: 0.0182 \t train acc: 1.0000\n",
      "epoch: 6 \t batch_idx : 699 \t loss: 0.0596 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 799 \t loss: 0.0585 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 899 \t loss: 0.0807 \t train acc: 0.7812\n",
      "epoch: 6 \t batch_idx : 999 \t loss: 0.0654 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 1099 \t loss: 0.0679 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 1199 \t loss: 0.0467 \t train acc: 0.9688\n",
      "epoch: 6 \t batch_idx : 1299 \t loss: 0.1342 \t train acc: 0.8125\n",
      "epoch: 6 \t batch_idx : 1399 \t loss: 0.0416 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 1499 \t loss: 0.0535 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 1599 \t loss: 0.0602 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 1699 \t loss: 0.0969 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 1799 \t loss: 0.0480 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 1899 \t loss: 0.0779 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 1999 \t loss: 0.1149 \t train acc: 0.8438\n",
      "epoch: 6 \t batch_idx : 2099 \t loss: 0.0521 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 2199 \t loss: 0.1431 \t train acc: 0.7812\n",
      "epoch: 6 \t batch_idx : 2299 \t loss: 0.0496 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 2399 \t loss: 0.0692 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 2499 \t loss: 0.0535 \t train acc: 0.9688\n",
      "epoch: 6 \t batch_idx : 2599 \t loss: 0.1175 \t train acc: 0.8438\n",
      "epoch: 6 \t batch_idx : 2699 \t loss: 0.0804 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 2799 \t loss: 0.0393 \t train acc: 0.9688\n",
      "epoch: 6 \t batch_idx : 2899 \t loss: 0.1031 \t train acc: 0.7812\n",
      "epoch: 6 \t batch_idx : 2999 \t loss: 0.0914 \t train acc: 0.8438\n",
      "epoch: 6 \t batch_idx : 3099 \t loss: 0.0490 \t train acc: 0.9688\n",
      "epoch: 6 \t batch_idx : 3199 \t loss: 0.0549 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 3299 \t loss: 0.0618 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 3399 \t loss: 0.0810 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 3499 \t loss: 0.0610 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 3599 \t loss: 0.0803 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 3699 \t loss: 0.0727 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 3799 \t loss: 0.0619 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 3899 \t loss: 0.0647 \t train acc: 0.8438\n",
      "epoch: 6 \t batch_idx : 3999 \t loss: 0.0762 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 4099 \t loss: 0.0576 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 4199 \t loss: 0.0780 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 4299 \t loss: 0.0580 \t train acc: 0.9688\n",
      "epoch: 6 \t batch_idx : 4399 \t loss: 0.0463 \t train acc: 0.9688\n",
      "epoch: 6 \t batch_idx : 4499 \t loss: 0.0288 \t train acc: 0.9688\n",
      "epoch: 6 \t batch_idx : 4599 \t loss: 0.0900 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 4699 \t loss: 0.0259 \t train acc: 1.0000\n",
      "epoch: 6 \t batch_idx : 4799 \t loss: 0.0139 \t train acc: 1.0000\n",
      "epoch: 6 \t batch_idx : 4899 \t loss: 0.0571 \t train acc: 0.9688\n",
      "epoch: 6 \t batch_idx : 4999 \t loss: 0.0510 \t train acc: 0.9688\n",
      "epoch: 6 \t batch_idx : 5099 \t loss: 0.0517 \t train acc: 0.9688\n",
      "epoch: 6 \t batch_idx : 5199 \t loss: 0.1013 \t train acc: 0.8438\n",
      "epoch: 6 \t batch_idx : 5299 \t loss: 0.0741 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 5399 \t loss: 0.0562 \t train acc: 0.9688\n",
      "epoch: 6 \t batch_idx : 5499 \t loss: 0.0611 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 5599 \t loss: 0.0130 \t train acc: 1.0000\n",
      "epoch: 6 \t batch_idx : 5699 \t loss: 0.0842 \t train acc: 0.8438\n",
      "epoch: 6 \t batch_idx : 5799 \t loss: 0.0972 \t train acc: 0.8438\n",
      "epoch: 6 \t batch_idx : 5899 \t loss: 0.0413 \t train acc: 0.9688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 \t batch_idx : 5999 \t loss: 0.0406 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 6099 \t loss: 0.0602 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 6199 \t loss: 0.1276 \t train acc: 0.8125\n",
      "epoch: 6 \t batch_idx : 6299 \t loss: 0.1191 \t train acc: 0.8125\n",
      "epoch: 6 \t batch_idx : 6399 \t loss: 0.0955 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 6499 \t loss: 0.0488 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 6599 \t loss: 0.1280 \t train acc: 0.7500\n",
      "epoch: 6 \t batch_idx : 6699 \t loss: 0.0333 \t train acc: 0.9688\n",
      "epoch: 6 \t batch_idx : 6799 \t loss: 0.0798 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 6899 \t loss: 0.0549 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 6999 \t loss: 0.0533 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 7099 \t loss: 0.0772 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 7199 \t loss: 0.0277 \t train acc: 1.0000\n",
      "epoch: 6 \t batch_idx : 7299 \t loss: 0.0843 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 7399 \t loss: 0.0375 \t train acc: 0.9688\n",
      "epoch: 6 \t batch_idx : 7499 \t loss: 0.0591 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 7599 \t loss: 0.0175 \t train acc: 0.9688\n",
      "epoch: 6 \t batch_idx : 7699 \t loss: 0.0868 \t train acc: 0.8438\n",
      "epoch: 6 \t batch_idx : 7799 \t loss: 0.0419 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 7899 \t loss: 0.0359 \t train acc: 0.9688\n",
      "epoch: 6 \t batch_idx : 7999 \t loss: 0.0629 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 8099 \t loss: 0.0763 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 8199 \t loss: 0.0511 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 8299 \t loss: 0.0303 \t train acc: 1.0000\n",
      "epoch: 6 \t batch_idx : 8399 \t loss: 0.0530 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 8499 \t loss: 0.0178 \t train acc: 1.0000\n",
      "epoch: 6 \t batch_idx : 8599 \t loss: 0.0557 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 8699 \t loss: 0.0612 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 8799 \t loss: 0.0403 \t train acc: 0.9688\n",
      "epoch: 6 \t batch_idx : 8899 \t loss: 0.0649 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 8999 \t loss: 0.1044 \t train acc: 0.8125\n",
      "epoch: 6 \t batch_idx : 9099 \t loss: 0.0579 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 9199 \t loss: 0.1402 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 9299 \t loss: 0.0584 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 9399 \t loss: 0.0727 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 9499 \t loss: 0.1099 \t train acc: 0.8125\n",
      "epoch: 6 \t batch_idx : 9599 \t loss: 0.0379 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 9699 \t loss: 0.0376 \t train acc: 0.9688\n",
      "epoch: 6 \t batch_idx : 9799 \t loss: 0.0716 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 9899 \t loss: 0.0639 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 9999 \t loss: 0.1455 \t train acc: 0.8125\n",
      "epoch: 6 \t batch_idx : 10099 \t loss: 0.0321 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 10199 \t loss: 0.0472 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 10299 \t loss: 0.0939 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 10399 \t loss: 0.1177 \t train acc: 0.8438\n",
      "epoch: 6 \t batch_idx : 10499 \t loss: 0.0724 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 10599 \t loss: 0.0877 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 10699 \t loss: 0.0439 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 10799 \t loss: 0.0363 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 10899 \t loss: 0.0807 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 10999 \t loss: 0.0467 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 11099 \t loss: 0.0609 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 11199 \t loss: 0.0545 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 11299 \t loss: 0.1037 \t train acc: 0.8125\n",
      "epoch: 6 \t batch_idx : 11399 \t loss: 0.1039 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 11499 \t loss: 0.0370 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 11599 \t loss: 0.0825 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 11699 \t loss: 0.0749 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 11799 \t loss: 0.0445 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 11899 \t loss: 0.0640 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 11999 \t loss: 0.0773 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 12099 \t loss: 0.0201 \t train acc: 1.0000\n",
      "epoch: 6 \t batch_idx : 12199 \t loss: 0.0655 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 12299 \t loss: 0.0741 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 12399 \t loss: 0.1255 \t train acc: 0.8125\n",
      "epoch: 6 \t batch_idx : 12499 \t loss: 0.0277 \t train acc: 1.0000\n",
      "epoch: 6 \t batch_idx : 12599 \t loss: 0.0992 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 12699 \t loss: 0.1290 \t train acc: 0.8438\n",
      "epoch: 6 \t batch_idx : 12799 \t loss: 0.0953 \t train acc: 0.8125\n",
      "epoch: 6 \t batch_idx : 12899 \t loss: 0.0195 \t train acc: 1.0000\n",
      "epoch: 6 \t batch_idx : 12999 \t loss: 0.0721 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 13099 \t loss: 0.0236 \t train acc: 0.9688\n",
      "epoch: 6 \t batch_idx : 13199 \t loss: 0.0814 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 13299 \t loss: 0.1397 \t train acc: 0.7812\n",
      "epoch: 6 \t batch_idx : 13399 \t loss: 0.0682 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 13499 \t loss: 0.0993 \t train acc: 0.8125\n",
      "epoch: 6 \t batch_idx : 13599 \t loss: 0.0469 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 13699 \t loss: 0.0624 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 13799 \t loss: 0.0444 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 13899 \t loss: 0.0938 \t train acc: 0.8438\n",
      "epoch: 6 \t batch_idx : 13999 \t loss: 0.1477 \t train acc: 0.7812\n",
      "epoch: 6 \t batch_idx : 14099 \t loss: 0.0444 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 14199 \t loss: 0.0471 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 14299 \t loss: 0.0762 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 14399 \t loss: 0.0883 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 14499 \t loss: 0.0272 \t train acc: 1.0000\n",
      "epoch: 6 \t batch_idx : 14599 \t loss: 0.0758 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 14699 \t loss: 0.0594 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 14799 \t loss: 0.1016 \t train acc: 0.8125\n",
      "epoch: 6 \t batch_idx : 14899 \t loss: 0.1026 \t train acc: 0.8438\n",
      "epoch: 6 \t batch_idx : 14999 \t loss: 0.0879 \t train acc: 0.8438\n",
      "epoch: 6 \t batch_idx : 15099 \t loss: 0.0490 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 15199 \t loss: 0.0664 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 15299 \t loss: 0.0693 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 15399 \t loss: 0.0665 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 15499 \t loss: 0.0515 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 15599 \t loss: 0.1349 \t train acc: 0.7812\n",
      "epoch: 6 \t batch_idx : 15699 \t loss: 0.0484 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 15799 \t loss: 0.0887 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 15899 \t loss: 0.0397 \t train acc: 0.9688\n",
      "epoch: 6 \t batch_idx : 15999 \t loss: 0.1219 \t train acc: 0.7812\n",
      "epoch: 6 \t batch_idx : 16099 \t loss: 0.0985 \t train acc: 0.8438\n",
      "epoch: 6 \t batch_idx : 16199 \t loss: 0.0526 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 16299 \t loss: 0.0610 \t train acc: 0.8750\n",
      "epoch: 6 \t batch_idx : 16399 \t loss: 0.0777 \t train acc: 0.8438\n",
      "epoch: 6 \t batch_idx : 16499 \t loss: 0.0506 \t train acc: 0.9375\n",
      "epoch: 6 \t batch_idx : 16599 \t loss: 0.0396 \t train acc: 0.9688\n",
      "epoch: 6 \t batch_idx : 16699 \t loss: 0.0636 \t train acc: 0.9062\n",
      "epoch: 6 \t batch_idx : 16799 \t loss: 0.0182 \t train acc: 1.0000\n",
      "epoch: 6 \t batch_idx : 16899 \t loss: 0.0974 \t train acc: 0.8750\n",
      "test acc: 0.8124\n",
      "epoch: 7 \t batch_idx : 99 \t loss: 0.0409 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 199 \t loss: 0.0690 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 299 \t loss: 0.0948 \t train acc: 0.8438\n",
      "epoch: 7 \t batch_idx : 399 \t loss: 0.0272 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 499 \t loss: 0.0225 \t train acc: 1.0000\n",
      "epoch: 7 \t batch_idx : 599 \t loss: 0.0242 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 699 \t loss: 0.0152 \t train acc: 1.0000\n",
      "epoch: 7 \t batch_idx : 799 \t loss: 0.0179 \t train acc: 1.0000\n",
      "epoch: 7 \t batch_idx : 899 \t loss: 0.0167 \t train acc: 1.0000\n",
      "epoch: 7 \t batch_idx : 999 \t loss: 0.0398 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 1099 \t loss: 0.0344 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 1199 \t loss: 0.0929 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 1299 \t loss: 0.0643 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 1399 \t loss: 0.0941 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 1499 \t loss: 0.0484 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 1599 \t loss: 0.0443 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 1699 \t loss: 0.0717 \t train acc: 0.8750\n",
      "epoch: 7 \t batch_idx : 1799 \t loss: 0.0383 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 1899 \t loss: 0.0308 \t train acc: 0.9688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 \t batch_idx : 1999 \t loss: 0.0991 \t train acc: 0.8438\n",
      "epoch: 7 \t batch_idx : 2099 \t loss: 0.0611 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 2199 \t loss: 0.0161 \t train acc: 1.0000\n",
      "epoch: 7 \t batch_idx : 2299 \t loss: 0.0224 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 2399 \t loss: 0.0635 \t train acc: 0.8438\n",
      "epoch: 7 \t batch_idx : 2499 \t loss: 0.0136 \t train acc: 1.0000\n",
      "epoch: 7 \t batch_idx : 2599 \t loss: 0.0834 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 2699 \t loss: 0.0499 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 2799 \t loss: 0.0456 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 2899 \t loss: 0.0186 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 2999 \t loss: 0.0265 \t train acc: 1.0000\n",
      "epoch: 7 \t batch_idx : 3099 \t loss: 0.0440 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 3199 \t loss: 0.0513 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 3299 \t loss: 0.0608 \t train acc: 0.8438\n",
      "epoch: 7 \t batch_idx : 3399 \t loss: 0.0681 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 3499 \t loss: 0.0897 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 3599 \t loss: 0.0358 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 3699 \t loss: 0.0870 \t train acc: 0.8438\n",
      "epoch: 7 \t batch_idx : 3799 \t loss: 0.0508 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 3899 \t loss: 0.1201 \t train acc: 0.8438\n",
      "epoch: 7 \t batch_idx : 3999 \t loss: 0.1408 \t train acc: 0.7812\n",
      "epoch: 7 \t batch_idx : 4099 \t loss: 0.1234 \t train acc: 0.7812\n",
      "epoch: 7 \t batch_idx : 4199 \t loss: 0.0198 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 4299 \t loss: 0.0789 \t train acc: 0.8750\n",
      "epoch: 7 \t batch_idx : 4399 \t loss: 0.0266 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 4499 \t loss: 0.0176 \t train acc: 1.0000\n",
      "epoch: 7 \t batch_idx : 4599 \t loss: 0.0918 \t train acc: 0.7812\n",
      "epoch: 7 \t batch_idx : 4699 \t loss: 0.0622 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 4799 \t loss: 0.0199 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 4899 \t loss: 0.0781 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 4999 \t loss: 0.0287 \t train acc: 1.0000\n",
      "epoch: 7 \t batch_idx : 5099 \t loss: 0.0584 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 5199 \t loss: 0.0715 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 5299 \t loss: 0.0450 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 5399 \t loss: 0.0659 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 5499 \t loss: 0.0466 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 5599 \t loss: 0.0659 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 5699 \t loss: 0.0307 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 5799 \t loss: 0.0753 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 5899 \t loss: 0.0509 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 5999 \t loss: 0.0574 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 6099 \t loss: 0.0421 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 6199 \t loss: 0.0747 \t train acc: 0.8750\n",
      "epoch: 7 \t batch_idx : 6299 \t loss: 0.1311 \t train acc: 0.8438\n",
      "epoch: 7 \t batch_idx : 6399 \t loss: 0.0308 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 6499 \t loss: 0.0580 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 6599 \t loss: 0.0267 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 6699 \t loss: 0.0726 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 6799 \t loss: 0.0244 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 6899 \t loss: 0.0662 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 6999 \t loss: 0.0573 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 7099 \t loss: 0.0814 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 7199 \t loss: 0.0666 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 7299 \t loss: 0.0482 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 7399 \t loss: 0.0945 \t train acc: 0.8125\n",
      "epoch: 7 \t batch_idx : 7499 \t loss: 0.0185 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 7599 \t loss: 0.0566 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 7699 \t loss: 0.0157 \t train acc: 1.0000\n",
      "epoch: 7 \t batch_idx : 7799 \t loss: 0.0265 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 7899 \t loss: 0.0423 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 7999 \t loss: 0.0865 \t train acc: 0.8750\n",
      "epoch: 7 \t batch_idx : 8099 \t loss: 0.0698 \t train acc: 0.8438\n",
      "epoch: 7 \t batch_idx : 8199 \t loss: 0.0954 \t train acc: 0.8125\n",
      "epoch: 7 \t batch_idx : 8299 \t loss: 0.0456 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 8399 \t loss: 0.0809 \t train acc: 0.8438\n",
      "epoch: 7 \t batch_idx : 8499 \t loss: 0.0861 \t train acc: 0.8750\n",
      "epoch: 7 \t batch_idx : 8599 \t loss: 0.0447 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 8699 \t loss: 0.0766 \t train acc: 0.8750\n",
      "epoch: 7 \t batch_idx : 8799 \t loss: 0.0240 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 8899 \t loss: 0.0726 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 8999 \t loss: 0.0553 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 9099 \t loss: 0.0536 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 9199 \t loss: 0.0613 \t train acc: 0.8438\n",
      "epoch: 7 \t batch_idx : 9299 \t loss: 0.0462 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 9399 \t loss: 0.0592 \t train acc: 0.8750\n",
      "epoch: 7 \t batch_idx : 9499 \t loss: 0.0561 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 9599 \t loss: 0.0381 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 9699 \t loss: 0.0841 \t train acc: 0.8750\n",
      "epoch: 7 \t batch_idx : 9799 \t loss: 0.0418 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 9899 \t loss: 0.0690 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 9999 \t loss: 0.0514 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 10099 \t loss: 0.0433 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 10199 \t loss: 0.0912 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 10299 \t loss: 0.0486 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 10399 \t loss: 0.0553 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 10499 \t loss: 0.0653 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 10599 \t loss: 0.0558 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 10699 \t loss: 0.0208 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 10799 \t loss: 0.0947 \t train acc: 0.8750\n",
      "epoch: 7 \t batch_idx : 10899 \t loss: 0.0688 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 10999 \t loss: 0.0337 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 11099 \t loss: 0.0277 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 11199 \t loss: 0.0453 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 11299 \t loss: 0.0436 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 11399 \t loss: 0.0367 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 11499 \t loss: 0.0248 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 11599 \t loss: 0.0080 \t train acc: 1.0000\n",
      "epoch: 7 \t batch_idx : 11699 \t loss: 0.0900 \t train acc: 0.8125\n",
      "epoch: 7 \t batch_idx : 11799 \t loss: 0.0711 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 11899 \t loss: 0.0352 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 11999 \t loss: 0.1300 \t train acc: 0.8438\n",
      "epoch: 7 \t batch_idx : 12099 \t loss: 0.0810 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 12199 \t loss: 0.0628 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 12299 \t loss: 0.0689 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 12399 \t loss: 0.1054 \t train acc: 0.8750\n",
      "epoch: 7 \t batch_idx : 12499 \t loss: 0.0827 \t train acc: 0.8750\n",
      "epoch: 7 \t batch_idx : 12599 \t loss: 0.0437 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 12699 \t loss: 0.0228 \t train acc: 1.0000\n",
      "epoch: 7 \t batch_idx : 12799 \t loss: 0.0708 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 12899 \t loss: 0.0497 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 12999 \t loss: 0.0458 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 13099 \t loss: 0.0931 \t train acc: 0.8438\n",
      "epoch: 7 \t batch_idx : 13199 \t loss: 0.0745 \t train acc: 0.8750\n",
      "epoch: 7 \t batch_idx : 13299 \t loss: 0.0519 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 13399 \t loss: 0.0311 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 13499 \t loss: 0.0588 \t train acc: 0.8750\n",
      "epoch: 7 \t batch_idx : 13599 \t loss: 0.0822 \t train acc: 0.8750\n",
      "epoch: 7 \t batch_idx : 13699 \t loss: 0.0397 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 13799 \t loss: 0.0568 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 13899 \t loss: 0.0300 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 13999 \t loss: 0.0238 \t train acc: 1.0000\n",
      "epoch: 7 \t batch_idx : 14099 \t loss: 0.0943 \t train acc: 0.8750\n",
      "epoch: 7 \t batch_idx : 14199 \t loss: 0.0551 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 14299 \t loss: 0.0610 \t train acc: 0.8750\n",
      "epoch: 7 \t batch_idx : 14399 \t loss: 0.0378 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 14499 \t loss: 0.0676 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 14599 \t loss: 0.0637 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 14699 \t loss: 0.0836 \t train acc: 0.8750\n",
      "epoch: 7 \t batch_idx : 14799 \t loss: 0.0417 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 14899 \t loss: 0.0754 \t train acc: 0.9062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 \t batch_idx : 14999 \t loss: 0.0497 \t train acc: 0.9688\n",
      "epoch: 7 \t batch_idx : 15099 \t loss: 0.0524 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 15199 \t loss: 0.0209 \t train acc: 1.0000\n",
      "epoch: 7 \t batch_idx : 15299 \t loss: 0.0811 \t train acc: 0.8750\n",
      "epoch: 7 \t batch_idx : 15399 \t loss: 0.0369 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 15499 \t loss: 0.0365 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 15599 \t loss: 0.1094 \t train acc: 0.8750\n",
      "epoch: 7 \t batch_idx : 15699 \t loss: 0.0689 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 15799 \t loss: 0.0409 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 15899 \t loss: 0.0089 \t train acc: 1.0000\n",
      "epoch: 7 \t batch_idx : 15999 \t loss: 0.0455 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 16099 \t loss: 0.0690 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 16199 \t loss: 0.0443 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 16299 \t loss: 0.0469 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 16399 \t loss: 0.0449 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 16499 \t loss: 0.0437 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 16599 \t loss: 0.0877 \t train acc: 0.8750\n",
      "epoch: 7 \t batch_idx : 16699 \t loss: 0.0568 \t train acc: 0.9375\n",
      "epoch: 7 \t batch_idx : 16799 \t loss: 0.0911 \t train acc: 0.9062\n",
      "epoch: 7 \t batch_idx : 16899 \t loss: 0.1516 \t train acc: 0.8125\n",
      "test acc: 0.8054\n",
      "epoch: 8 \t batch_idx : 99 \t loss: 0.0448 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 199 \t loss: 0.0360 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 299 \t loss: 0.0628 \t train acc: 0.8750\n",
      "epoch: 8 \t batch_idx : 399 \t loss: 0.0696 \t train acc: 0.9062\n",
      "epoch: 8 \t batch_idx : 499 \t loss: 0.0636 \t train acc: 0.9062\n",
      "epoch: 8 \t batch_idx : 599 \t loss: 0.0325 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 699 \t loss: 0.0599 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 799 \t loss: 0.0183 \t train acc: 1.0000\n",
      "epoch: 8 \t batch_idx : 899 \t loss: 0.0376 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 999 \t loss: 0.0438 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 1099 \t loss: 0.0319 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 1199 \t loss: 0.0508 \t train acc: 0.9062\n",
      "epoch: 8 \t batch_idx : 1299 \t loss: 0.0648 \t train acc: 0.8750\n",
      "epoch: 8 \t batch_idx : 1399 \t loss: 0.0565 \t train acc: 0.9062\n",
      "epoch: 8 \t batch_idx : 1499 \t loss: 0.0216 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 1599 \t loss: 0.0625 \t train acc: 0.9062\n",
      "epoch: 8 \t batch_idx : 1699 \t loss: 0.0437 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 1799 \t loss: 0.0210 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 1899 \t loss: 0.0171 \t train acc: 1.0000\n",
      "epoch: 8 \t batch_idx : 1999 \t loss: 0.0457 \t train acc: 0.9062\n",
      "epoch: 8 \t batch_idx : 2099 \t loss: 0.0230 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 2199 \t loss: 0.0694 \t train acc: 0.8750\n",
      "epoch: 8 \t batch_idx : 2299 \t loss: 0.0656 \t train acc: 0.8750\n",
      "epoch: 8 \t batch_idx : 2399 \t loss: 0.0234 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 2499 \t loss: 0.0352 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 2599 \t loss: 0.0809 \t train acc: 0.8750\n",
      "epoch: 8 \t batch_idx : 2699 \t loss: 0.0571 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 2799 \t loss: 0.0500 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 2899 \t loss: 0.0516 \t train acc: 0.8750\n",
      "epoch: 8 \t batch_idx : 2999 \t loss: 0.0601 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 3099 \t loss: 0.0519 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 3199 \t loss: 0.0422 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 3299 \t loss: 0.0652 \t train acc: 0.9062\n",
      "epoch: 8 \t batch_idx : 3399 \t loss: 0.0666 \t train acc: 0.8750\n",
      "epoch: 8 \t batch_idx : 3499 \t loss: 0.0833 \t train acc: 0.9062\n",
      "epoch: 8 \t batch_idx : 3599 \t loss: 0.0381 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 3699 \t loss: 0.0284 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 3799 \t loss: 0.0140 \t train acc: 1.0000\n",
      "epoch: 8 \t batch_idx : 3899 \t loss: 0.0672 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 3999 \t loss: 0.0483 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 4099 \t loss: 0.0774 \t train acc: 0.8750\n",
      "epoch: 8 \t batch_idx : 4199 \t loss: 0.0146 \t train acc: 1.0000\n",
      "epoch: 8 \t batch_idx : 4299 \t loss: 0.1236 \t train acc: 0.8750\n",
      "epoch: 8 \t batch_idx : 4399 \t loss: 0.0412 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 4499 \t loss: 0.0256 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 4599 \t loss: 0.0808 \t train acc: 0.8438\n",
      "epoch: 8 \t batch_idx : 4699 \t loss: 0.0714 \t train acc: 0.9062\n",
      "epoch: 8 \t batch_idx : 4799 \t loss: 0.0208 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 4899 \t loss: 0.0529 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 4999 \t loss: 0.0190 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 5099 \t loss: 0.0657 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 5199 \t loss: 0.0210 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 5299 \t loss: 0.0627 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 5399 \t loss: 0.0577 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 5499 \t loss: 0.0641 \t train acc: 0.8750\n",
      "epoch: 8 \t batch_idx : 5599 \t loss: 0.0502 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 5699 \t loss: 0.0438 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 5799 \t loss: 0.0492 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 5899 \t loss: 0.0200 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 5999 \t loss: 0.0538 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 6099 \t loss: 0.0096 \t train acc: 1.0000\n",
      "epoch: 8 \t batch_idx : 6199 \t loss: 0.0405 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 6299 \t loss: 0.0267 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 6399 \t loss: 0.0532 \t train acc: 0.9062\n",
      "epoch: 8 \t batch_idx : 6499 \t loss: 0.0448 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 6599 \t loss: 0.0585 \t train acc: 0.9062\n",
      "epoch: 8 \t batch_idx : 6699 \t loss: 0.0249 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 6799 \t loss: 0.0482 \t train acc: 0.9062\n",
      "epoch: 8 \t batch_idx : 6899 \t loss: 0.0377 \t train acc: 0.9062\n",
      "epoch: 8 \t batch_idx : 6999 \t loss: 0.0324 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 7099 \t loss: 0.0142 \t train acc: 1.0000\n",
      "epoch: 8 \t batch_idx : 7199 \t loss: 0.0229 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 7299 \t loss: 0.0377 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 7399 \t loss: 0.0824 \t train acc: 0.8750\n",
      "epoch: 8 \t batch_idx : 7499 \t loss: 0.0509 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 7599 \t loss: 0.0365 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 7699 \t loss: 0.0495 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 7799 \t loss: 0.0697 \t train acc: 0.8750\n",
      "epoch: 8 \t batch_idx : 7899 \t loss: 0.0837 \t train acc: 0.8750\n",
      "epoch: 8 \t batch_idx : 7999 \t loss: 0.0221 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 8099 \t loss: 0.0861 \t train acc: 0.9062\n",
      "epoch: 8 \t batch_idx : 8199 \t loss: 0.0126 \t train acc: 1.0000\n",
      "epoch: 8 \t batch_idx : 8299 \t loss: 0.0568 \t train acc: 0.9062\n",
      "epoch: 8 \t batch_idx : 8399 \t loss: 0.0301 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 8499 \t loss: 0.0493 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 8599 \t loss: 0.0527 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 8699 \t loss: 0.1149 \t train acc: 0.8750\n",
      "epoch: 8 \t batch_idx : 8799 \t loss: 0.1037 \t train acc: 0.8125\n",
      "epoch: 8 \t batch_idx : 8899 \t loss: 0.0559 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 8999 \t loss: 0.0349 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 9099 \t loss: 0.0796 \t train acc: 0.8750\n",
      "epoch: 8 \t batch_idx : 9199 \t loss: 0.0143 \t train acc: 1.0000\n",
      "epoch: 8 \t batch_idx : 9299 \t loss: 0.0627 \t train acc: 0.9062\n",
      "epoch: 8 \t batch_idx : 9399 \t loss: 0.0748 \t train acc: 0.9062\n",
      "epoch: 8 \t batch_idx : 9499 \t loss: 0.0445 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 9599 \t loss: 0.0145 \t train acc: 1.0000\n",
      "epoch: 8 \t batch_idx : 9699 \t loss: 0.0560 \t train acc: 0.9062\n",
      "epoch: 8 \t batch_idx : 9799 \t loss: 0.0857 \t train acc: 0.9062\n",
      "epoch: 8 \t batch_idx : 9899 \t loss: 0.0659 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 9999 \t loss: 0.0183 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 10099 \t loss: 0.0329 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 10199 \t loss: 0.0422 \t train acc: 0.9062\n",
      "epoch: 8 \t batch_idx : 10299 \t loss: 0.0618 \t train acc: 0.9062\n",
      "epoch: 8 \t batch_idx : 10399 \t loss: 0.0627 \t train acc: 0.9062\n",
      "epoch: 8 \t batch_idx : 10499 \t loss: 0.0417 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 10599 \t loss: 0.0421 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 10699 \t loss: 0.0339 \t train acc: 1.0000\n",
      "epoch: 8 \t batch_idx : 10799 \t loss: 0.0616 \t train acc: 0.8750\n",
      "epoch: 8 \t batch_idx : 10899 \t loss: 0.0616 \t train acc: 0.9062\n",
      "epoch: 8 \t batch_idx : 10999 \t loss: 0.0558 \t train acc: 0.8750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 \t batch_idx : 11099 \t loss: 0.0281 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 11199 \t loss: 0.0298 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 11299 \t loss: 0.0591 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 11399 \t loss: 0.0316 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 11499 \t loss: 0.0488 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 11599 \t loss: 0.0511 \t train acc: 0.9062\n",
      "epoch: 8 \t batch_idx : 11699 \t loss: 0.0561 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 11799 \t loss: 0.0427 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 11899 \t loss: 0.0668 \t train acc: 0.8438\n",
      "epoch: 8 \t batch_idx : 11999 \t loss: 0.0602 \t train acc: 0.9062\n",
      "epoch: 8 \t batch_idx : 12099 \t loss: 0.0622 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 12199 \t loss: 0.0461 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 12299 \t loss: 0.0372 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 12399 \t loss: 0.0219 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 12499 \t loss: 0.0158 \t train acc: 1.0000\n",
      "epoch: 8 \t batch_idx : 12599 \t loss: 0.0409 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 12699 \t loss: 0.0607 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 12799 \t loss: 0.0158 \t train acc: 1.0000\n",
      "epoch: 8 \t batch_idx : 12899 \t loss: 0.0461 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 12999 \t loss: 0.0368 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 13099 \t loss: 0.0730 \t train acc: 0.9062\n",
      "epoch: 8 \t batch_idx : 13199 \t loss: 0.0386 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 13299 \t loss: 0.0097 \t train acc: 1.0000\n",
      "epoch: 8 \t batch_idx : 13399 \t loss: 0.0252 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 13499 \t loss: 0.0595 \t train acc: 0.9062\n",
      "epoch: 8 \t batch_idx : 13599 \t loss: 0.0520 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 13699 \t loss: 0.0224 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 13799 \t loss: 0.0204 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 13899 \t loss: 0.0160 \t train acc: 1.0000\n",
      "epoch: 8 \t batch_idx : 13999 \t loss: 0.0637 \t train acc: 0.9062\n",
      "epoch: 8 \t batch_idx : 14099 \t loss: 0.0443 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 14199 \t loss: 0.0499 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 14299 \t loss: 0.0064 \t train acc: 1.0000\n",
      "epoch: 8 \t batch_idx : 14399 \t loss: 0.0085 \t train acc: 1.0000\n",
      "epoch: 8 \t batch_idx : 14499 \t loss: 0.0293 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 14599 \t loss: 0.1039 \t train acc: 0.7812\n",
      "epoch: 8 \t batch_idx : 14699 \t loss: 0.0733 \t train acc: 0.9062\n",
      "epoch: 8 \t batch_idx : 14799 \t loss: 0.0293 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 14899 \t loss: 0.0661 \t train acc: 0.8438\n",
      "epoch: 8 \t batch_idx : 14999 \t loss: 0.0212 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 15099 \t loss: 0.0385 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 15199 \t loss: 0.0621 \t train acc: 0.8750\n",
      "epoch: 8 \t batch_idx : 15299 \t loss: 0.0645 \t train acc: 0.8750\n",
      "epoch: 8 \t batch_idx : 15399 \t loss: 0.1061 \t train acc: 0.8750\n",
      "epoch: 8 \t batch_idx : 15499 \t loss: 0.1732 \t train acc: 0.7812\n",
      "epoch: 8 \t batch_idx : 15599 \t loss: 0.0832 \t train acc: 0.8750\n",
      "epoch: 8 \t batch_idx : 15699 \t loss: 0.0676 \t train acc: 0.8438\n",
      "epoch: 8 \t batch_idx : 15799 \t loss: 0.0543 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 15899 \t loss: 0.0579 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 15999 \t loss: 0.0635 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 16099 \t loss: 0.0298 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 16199 \t loss: 0.0435 \t train acc: 0.9375\n",
      "epoch: 8 \t batch_idx : 16299 \t loss: 0.0380 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 16399 \t loss: 0.0740 \t train acc: 0.9062\n",
      "epoch: 8 \t batch_idx : 16499 \t loss: 0.1085 \t train acc: 0.8125\n",
      "epoch: 8 \t batch_idx : 16599 \t loss: 0.0214 \t train acc: 0.9688\n",
      "epoch: 8 \t batch_idx : 16699 \t loss: 0.1367 \t train acc: 0.7812\n",
      "epoch: 8 \t batch_idx : 16799 \t loss: 0.0864 \t train acc: 0.8750\n",
      "epoch: 8 \t batch_idx : 16899 \t loss: 0.0445 \t train acc: 0.8750\n",
      "test acc: 0.8065\n",
      "epoch: 9 \t batch_idx : 99 \t loss: 0.0199 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 199 \t loss: 0.0226 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 299 \t loss: 0.0170 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 399 \t loss: 0.0295 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 499 \t loss: 0.0194 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 599 \t loss: 0.0154 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 699 \t loss: 0.0576 \t train acc: 0.9062\n",
      "epoch: 9 \t batch_idx : 799 \t loss: 0.0455 \t train acc: 0.9062\n",
      "epoch: 9 \t batch_idx : 899 \t loss: 0.0570 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 999 \t loss: 0.0208 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 1099 \t loss: 0.0074 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 1199 \t loss: 0.0224 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 1299 \t loss: 0.0057 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 1399 \t loss: 0.0193 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 1499 \t loss: 0.0120 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 1599 \t loss: 0.0221 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 1699 \t loss: 0.0602 \t train acc: 0.8750\n",
      "epoch: 9 \t batch_idx : 1799 \t loss: 0.0149 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 1899 \t loss: 0.0472 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 1999 \t loss: 0.0076 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 2099 \t loss: 0.0537 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 2199 \t loss: 0.0238 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 2299 \t loss: 0.0632 \t train acc: 0.8438\n",
      "epoch: 9 \t batch_idx : 2399 \t loss: 0.0336 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 2499 \t loss: 0.0527 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 2599 \t loss: 0.0499 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 2699 \t loss: 0.0820 \t train acc: 0.9062\n",
      "epoch: 9 \t batch_idx : 2799 \t loss: 0.0064 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 2899 \t loss: 0.0363 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 2999 \t loss: 0.0395 \t train acc: 0.9062\n",
      "epoch: 9 \t batch_idx : 3099 \t loss: 0.0316 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 3199 \t loss: 0.0424 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 3299 \t loss: 0.0239 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 3399 \t loss: 0.0496 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 3499 \t loss: 0.0073 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 3599 \t loss: 0.0595 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 3699 \t loss: 0.0456 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 3799 \t loss: 0.0497 \t train acc: 0.9062\n",
      "epoch: 9 \t batch_idx : 3899 \t loss: 0.0045 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 3999 \t loss: 0.0220 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 4099 \t loss: 0.0735 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 4199 \t loss: 0.0690 \t train acc: 0.9062\n",
      "epoch: 9 \t batch_idx : 4299 \t loss: 0.0540 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 4399 \t loss: 0.0346 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 4499 \t loss: 0.0386 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 4599 \t loss: 0.0431 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 4699 \t loss: 0.0465 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 4799 \t loss: 0.0423 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 4899 \t loss: 0.0481 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 4999 \t loss: 0.0434 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 5099 \t loss: 0.0544 \t train acc: 0.9062\n",
      "epoch: 9 \t batch_idx : 5199 \t loss: 0.0551 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 5299 \t loss: 0.0776 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 5399 \t loss: 0.0169 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 5499 \t loss: 0.0658 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 5599 \t loss: 0.0189 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 5699 \t loss: 0.0179 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 5799 \t loss: 0.0461 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 5899 \t loss: 0.0138 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 5999 \t loss: 0.0301 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 6099 \t loss: 0.0236 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 6199 \t loss: 0.0703 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 6299 \t loss: 0.0585 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 6399 \t loss: 0.0197 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 6499 \t loss: 0.0401 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 6599 \t loss: 0.0174 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 6699 \t loss: 0.0315 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 6799 \t loss: 0.0189 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 6899 \t loss: 0.0754 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 6999 \t loss: 0.0120 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 7099 \t loss: 0.0301 \t train acc: 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 \t batch_idx : 7199 \t loss: 0.0827 \t train acc: 0.8438\n",
      "epoch: 9 \t batch_idx : 7299 \t loss: 0.0173 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 7399 \t loss: 0.0285 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 7499 \t loss: 0.0578 \t train acc: 0.9062\n",
      "epoch: 9 \t batch_idx : 7599 \t loss: 0.0366 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 7699 \t loss: 0.0599 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 7799 \t loss: 0.0062 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 7899 \t loss: 0.0080 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 7999 \t loss: 0.0325 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 8099 \t loss: 0.0967 \t train acc: 0.9062\n",
      "epoch: 9 \t batch_idx : 8199 \t loss: 0.0586 \t train acc: 0.9062\n",
      "epoch: 9 \t batch_idx : 8299 \t loss: 0.0374 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 8399 \t loss: 0.0538 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 8499 \t loss: 0.0760 \t train acc: 0.9062\n",
      "epoch: 9 \t batch_idx : 8599 \t loss: 0.0076 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 8699 \t loss: 0.0419 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 8799 \t loss: 0.0408 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 8899 \t loss: 0.0386 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 8999 \t loss: 0.0457 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 9099 \t loss: 0.0450 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 9199 \t loss: 0.0323 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 9299 \t loss: 0.0703 \t train acc: 0.9062\n",
      "epoch: 9 \t batch_idx : 9399 \t loss: 0.0160 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 9499 \t loss: 0.0295 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 9599 \t loss: 0.0165 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 9699 \t loss: 0.0759 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 9799 \t loss: 0.0480 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 9899 \t loss: 0.0342 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 9999 \t loss: 0.0444 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 10099 \t loss: 0.0375 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 10199 \t loss: 0.0056 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 10299 \t loss: 0.0133 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 10399 \t loss: 0.0099 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 10499 \t loss: 0.0243 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 10599 \t loss: 0.0680 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 10699 \t loss: 0.0839 \t train acc: 0.9062\n",
      "epoch: 9 \t batch_idx : 10799 \t loss: 0.0222 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 10899 \t loss: 0.0185 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 10999 \t loss: 0.0167 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 11099 \t loss: 0.0901 \t train acc: 0.8750\n",
      "epoch: 9 \t batch_idx : 11199 \t loss: 0.0814 \t train acc: 0.8750\n",
      "epoch: 9 \t batch_idx : 11299 \t loss: 0.0450 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 11399 \t loss: 0.0590 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 11499 \t loss: 0.0981 \t train acc: 0.8750\n",
      "epoch: 9 \t batch_idx : 11599 \t loss: 0.0282 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 11699 \t loss: 0.0321 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 11799 \t loss: 0.1006 \t train acc: 0.8438\n",
      "epoch: 9 \t batch_idx : 11899 \t loss: 0.0227 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 11999 \t loss: 0.0127 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 12099 \t loss: 0.0404 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 12199 \t loss: 0.0094 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 12299 \t loss: 0.0367 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 12399 \t loss: 0.0388 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 12499 \t loss: 0.0398 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 12599 \t loss: 0.0456 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 12699 \t loss: 0.0190 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 12799 \t loss: 0.1072 \t train acc: 0.8750\n",
      "epoch: 9 \t batch_idx : 12899 \t loss: 0.0379 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 12999 \t loss: 0.0756 \t train acc: 0.9062\n",
      "epoch: 9 \t batch_idx : 13099 \t loss: 0.0588 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 13199 \t loss: 0.0533 \t train acc: 0.9062\n",
      "epoch: 9 \t batch_idx : 13299 \t loss: 0.0108 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 13399 \t loss: 0.0083 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 13499 \t loss: 0.0312 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 13599 \t loss: 0.0226 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 13699 \t loss: 0.0506 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 13799 \t loss: 0.0522 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 13899 \t loss: 0.0464 \t train acc: 0.9062\n",
      "epoch: 9 \t batch_idx : 13999 \t loss: 0.0270 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 14099 \t loss: 0.0665 \t train acc: 0.9062\n",
      "epoch: 9 \t batch_idx : 14199 \t loss: 0.0267 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 14299 \t loss: 0.0633 \t train acc: 0.9062\n",
      "epoch: 9 \t batch_idx : 14399 \t loss: 0.0476 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 14499 \t loss: 0.0266 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 14599 \t loss: 0.0139 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 14699 \t loss: 0.0330 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 14799 \t loss: 0.0571 \t train acc: 0.9062\n",
      "epoch: 9 \t batch_idx : 14899 \t loss: 0.0227 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 14999 \t loss: 0.0814 \t train acc: 0.8750\n",
      "epoch: 9 \t batch_idx : 15099 \t loss: 0.0286 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 15199 \t loss: 0.0218 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 15299 \t loss: 0.0360 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 15399 \t loss: 0.0552 \t train acc: 0.9062\n",
      "epoch: 9 \t batch_idx : 15499 \t loss: 0.0081 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 15599 \t loss: 0.0563 \t train acc: 0.9062\n",
      "epoch: 9 \t batch_idx : 15699 \t loss: 0.0497 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 15799 \t loss: 0.0537 \t train acc: 0.8750\n",
      "epoch: 9 \t batch_idx : 15899 \t loss: 0.0506 \t train acc: 0.9062\n",
      "epoch: 9 \t batch_idx : 15999 \t loss: 0.1022 \t train acc: 0.8438\n",
      "epoch: 9 \t batch_idx : 16099 \t loss: 0.0497 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 16199 \t loss: 0.0493 \t train acc: 0.9062\n",
      "epoch: 9 \t batch_idx : 16299 \t loss: 0.0309 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 16399 \t loss: 0.0128 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 16499 \t loss: 0.0471 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 16599 \t loss: 0.0445 \t train acc: 0.9375\n",
      "epoch: 9 \t batch_idx : 16699 \t loss: 0.0034 \t train acc: 1.0000\n",
      "epoch: 9 \t batch_idx : 16799 \t loss: 0.0356 \t train acc: 0.9688\n",
      "epoch: 9 \t batch_idx : 16899 \t loss: 0.0650 \t train acc: 0.9375\n",
      "test acc: 0.7995\n",
      "epoch: 10 \t batch_idx : 99 \t loss: 0.0742 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 199 \t loss: 0.0907 \t train acc: 0.9062\n",
      "epoch: 10 \t batch_idx : 299 \t loss: 0.0546 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 399 \t loss: 0.0188 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 499 \t loss: 0.0160 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 599 \t loss: 0.0104 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 699 \t loss: 0.0088 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 799 \t loss: 0.0187 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 899 \t loss: 0.0208 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 999 \t loss: 0.0192 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 1099 \t loss: 0.0517 \t train acc: 0.9062\n",
      "epoch: 10 \t batch_idx : 1199 \t loss: 0.0284 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 1299 \t loss: 0.0122 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 1399 \t loss: 0.0361 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 1499 \t loss: 0.0098 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 1599 \t loss: 0.0596 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 1699 \t loss: 0.0153 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 1799 \t loss: 0.0439 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 1899 \t loss: 0.0345 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 1999 \t loss: 0.0517 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 2099 \t loss: 0.0436 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 2199 \t loss: 0.0023 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 2299 \t loss: 0.0202 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 2399 \t loss: 0.0551 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 2499 \t loss: 0.0102 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 2599 \t loss: 0.0402 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 2699 \t loss: 0.0236 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 2799 \t loss: 0.0466 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 2899 \t loss: 0.0160 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 2999 \t loss: 0.0504 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 3099 \t loss: 0.0179 \t train acc: 0.9688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 \t batch_idx : 3199 \t loss: 0.0491 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 3299 \t loss: 0.0206 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 3399 \t loss: 0.0958 \t train acc: 0.8750\n",
      "epoch: 10 \t batch_idx : 3499 \t loss: 0.0265 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 3599 \t loss: 0.0035 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 3699 \t loss: 0.0550 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 3799 \t loss: 0.0155 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 3899 \t loss: 0.0392 \t train acc: 0.9062\n",
      "epoch: 10 \t batch_idx : 3999 \t loss: 0.0329 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 4099 \t loss: 0.0192 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 4199 \t loss: 0.0381 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 4299 \t loss: 0.0177 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 4399 \t loss: 0.0206 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 4499 \t loss: 0.0856 \t train acc: 0.8438\n",
      "epoch: 10 \t batch_idx : 4599 \t loss: 0.0406 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 4699 \t loss: 0.0204 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 4799 \t loss: 0.0057 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 4899 \t loss: 0.0048 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 4999 \t loss: 0.0079 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 5099 \t loss: 0.0322 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 5199 \t loss: 0.0267 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 5299 \t loss: 0.0105 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 5399 \t loss: 0.0379 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 5499 \t loss: 0.0206 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 5599 \t loss: 0.0346 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 5699 \t loss: 0.0279 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 5799 \t loss: 0.0066 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 5899 \t loss: 0.0205 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 5999 \t loss: 0.0482 \t train acc: 0.9062\n",
      "epoch: 10 \t batch_idx : 6099 \t loss: 0.0109 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 6199 \t loss: 0.0590 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 6299 \t loss: 0.0186 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 6399 \t loss: 0.0383 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 6499 \t loss: 0.0481 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 6599 \t loss: 0.0397 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 6699 \t loss: 0.0202 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 6799 \t loss: 0.0493 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 6899 \t loss: 0.0347 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 6999 \t loss: 0.0069 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 7099 \t loss: 0.0898 \t train acc: 0.9062\n",
      "epoch: 10 \t batch_idx : 7199 \t loss: 0.0325 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 7299 \t loss: 0.0013 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 7399 \t loss: 0.0409 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 7499 \t loss: 0.0185 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 7599 \t loss: 0.0213 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 7699 \t loss: 0.0345 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 7799 \t loss: 0.0283 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 7899 \t loss: 0.0655 \t train acc: 0.9062\n",
      "epoch: 10 \t batch_idx : 7999 \t loss: 0.0496 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 8099 \t loss: 0.0523 \t train acc: 0.9062\n",
      "epoch: 10 \t batch_idx : 8199 \t loss: 0.0560 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 8299 \t loss: 0.0344 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 8399 \t loss: 0.0309 \t train acc: 0.9062\n",
      "epoch: 10 \t batch_idx : 8499 \t loss: 0.0310 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 8599 \t loss: 0.0040 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 8699 \t loss: 0.0058 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 8799 \t loss: 0.0677 \t train acc: 0.9062\n",
      "epoch: 10 \t batch_idx : 8899 \t loss: 0.0418 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 8999 \t loss: 0.0265 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 9099 \t loss: 0.0892 \t train acc: 0.8438\n",
      "epoch: 10 \t batch_idx : 9199 \t loss: 0.0159 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 9299 \t loss: 0.0071 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 9399 \t loss: 0.0302 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 9499 \t loss: 0.0424 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 9599 \t loss: 0.0023 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 9699 \t loss: 0.0226 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 9799 \t loss: 0.0300 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 9899 \t loss: 0.0118 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 9999 \t loss: 0.0457 \t train acc: 0.9062\n",
      "epoch: 10 \t batch_idx : 10099 \t loss: 0.0488 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 10199 \t loss: 0.0753 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 10299 \t loss: 0.0270 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 10399 \t loss: 0.0104 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 10499 \t loss: 0.0108 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 10599 \t loss: 0.0631 \t train acc: 0.9062\n",
      "epoch: 10 \t batch_idx : 10699 \t loss: 0.0502 \t train acc: 0.9062\n",
      "epoch: 10 \t batch_idx : 10799 \t loss: 0.0753 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 10899 \t loss: 0.0387 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 10999 \t loss: 0.0153 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 11099 \t loss: 0.0324 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 11199 \t loss: 0.0379 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 11299 \t loss: 0.0405 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 11399 \t loss: 0.0756 \t train acc: 0.9062\n",
      "epoch: 10 \t batch_idx : 11499 \t loss: 0.0588 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 11599 \t loss: 0.0687 \t train acc: 0.9062\n",
      "epoch: 10 \t batch_idx : 11699 \t loss: 0.0542 \t train acc: 0.9062\n",
      "epoch: 10 \t batch_idx : 11799 \t loss: 0.0546 \t train acc: 0.9062\n",
      "epoch: 10 \t batch_idx : 11899 \t loss: 0.0999 \t train acc: 0.8125\n",
      "epoch: 10 \t batch_idx : 11999 \t loss: 0.0263 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 12099 \t loss: 0.0878 \t train acc: 0.9062\n",
      "epoch: 10 \t batch_idx : 12199 \t loss: 0.0489 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 12299 \t loss: 0.0860 \t train acc: 0.8750\n",
      "epoch: 10 \t batch_idx : 12399 \t loss: 0.0089 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 12499 \t loss: 0.0130 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 12599 \t loss: 0.0555 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 12699 \t loss: 0.0845 \t train acc: 0.8750\n",
      "epoch: 10 \t batch_idx : 12799 \t loss: 0.0234 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 12899 \t loss: 0.0589 \t train acc: 0.9062\n",
      "epoch: 10 \t batch_idx : 12999 \t loss: 0.0230 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 13099 \t loss: 0.0020 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 13199 \t loss: 0.0085 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 13299 \t loss: 0.0500 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 13399 \t loss: 0.0044 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 13499 \t loss: 0.0336 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 13599 \t loss: 0.0131 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 13699 \t loss: 0.0293 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 13799 \t loss: 0.0152 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 13899 \t loss: 0.0534 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 13999 \t loss: 0.0411 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 14099 \t loss: 0.0953 \t train acc: 0.9062\n",
      "epoch: 10 \t batch_idx : 14199 \t loss: 0.0143 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 14299 \t loss: 0.0572 \t train acc: 0.9062\n",
      "epoch: 10 \t batch_idx : 14399 \t loss: 0.0822 \t train acc: 0.8750\n",
      "epoch: 10 \t batch_idx : 14499 \t loss: 0.0187 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 14599 \t loss: 0.0342 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 14699 \t loss: 0.0393 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 14799 \t loss: 0.0128 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 14899 \t loss: 0.0648 \t train acc: 0.9062\n",
      "epoch: 10 \t batch_idx : 14999 \t loss: 0.0302 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 15099 \t loss: 0.0505 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 15199 \t loss: 0.0686 \t train acc: 0.8750\n",
      "epoch: 10 \t batch_idx : 15299 \t loss: 0.0262 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 15399 \t loss: 0.0240 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 15499 \t loss: 0.0060 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 15599 \t loss: 0.0344 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 15699 \t loss: 0.0248 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 15799 \t loss: 0.0035 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 15899 \t loss: 0.0453 \t train acc: 0.9062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 \t batch_idx : 15999 \t loss: 0.0106 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 16099 \t loss: 0.0278 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 16199 \t loss: 0.0299 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 16299 \t loss: 0.0458 \t train acc: 0.9375\n",
      "epoch: 10 \t batch_idx : 16399 \t loss: 0.0320 \t train acc: 0.9688\n",
      "epoch: 10 \t batch_idx : 16499 \t loss: 0.0113 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 16599 \t loss: 0.0168 \t train acc: 1.0000\n",
      "epoch: 10 \t batch_idx : 16699 \t loss: 0.1287 \t train acc: 0.8438\n",
      "epoch: 10 \t batch_idx : 16799 \t loss: 0.0827 \t train acc: 0.9062\n",
      "epoch: 10 \t batch_idx : 16899 \t loss: 0.1014 \t train acc: 0.8750\n",
      "test acc: 0.7922\n",
      "epoch: 11 \t batch_idx : 99 \t loss: 0.0336 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 199 \t loss: 0.0278 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 299 \t loss: 0.0198 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 399 \t loss: 0.0130 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 499 \t loss: 0.0272 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 599 \t loss: 0.0313 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 699 \t loss: 0.0767 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 799 \t loss: 0.0212 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 899 \t loss: 0.0416 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 999 \t loss: 0.0079 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 1099 \t loss: 0.0038 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 1199 \t loss: 0.0246 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 1299 \t loss: 0.0113 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 1399 \t loss: 0.0761 \t train acc: 0.8750\n",
      "epoch: 11 \t batch_idx : 1499 \t loss: 0.0571 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 1599 \t loss: 0.0051 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 1699 \t loss: 0.0063 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 1799 \t loss: 0.0231 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 1899 \t loss: 0.0324 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 1999 \t loss: 0.0114 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 2099 \t loss: 0.0031 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 2199 \t loss: 0.0156 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 2299 \t loss: 0.0266 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 2399 \t loss: 0.0107 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 2499 \t loss: 0.0024 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 2599 \t loss: 0.0387 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 2699 \t loss: 0.0056 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 2799 \t loss: 0.0367 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 2899 \t loss: 0.0646 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 2999 \t loss: 0.0250 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 3099 \t loss: 0.0111 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 3199 \t loss: 0.0314 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 3299 \t loss: 0.0202 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 3399 \t loss: 0.0263 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 3499 \t loss: 0.0200 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 3599 \t loss: 0.0264 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 3699 \t loss: 0.0628 \t train acc: 0.9062\n",
      "epoch: 11 \t batch_idx : 3799 \t loss: 0.0216 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 3899 \t loss: 0.0441 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 3999 \t loss: 0.0179 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 4099 \t loss: 0.0675 \t train acc: 0.9062\n",
      "epoch: 11 \t batch_idx : 4199 \t loss: 0.0087 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 4299 \t loss: 0.0106 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 4399 \t loss: 0.0323 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 4499 \t loss: 0.0551 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 4599 \t loss: 0.0143 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 4699 \t loss: 0.0489 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 4799 \t loss: 0.0162 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 4899 \t loss: 0.0389 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 4999 \t loss: 0.0417 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 5099 \t loss: 0.0021 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 5199 \t loss: 0.0373 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 5299 \t loss: 0.0049 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 5399 \t loss: 0.0336 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 5499 \t loss: 0.0424 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 5599 \t loss: 0.0112 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 5699 \t loss: 0.0132 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 5799 \t loss: 0.0082 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 5899 \t loss: 0.0569 \t train acc: 0.9062\n",
      "epoch: 11 \t batch_idx : 5999 \t loss: 0.0214 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 6099 \t loss: 0.1250 \t train acc: 0.8438\n",
      "epoch: 11 \t batch_idx : 6199 \t loss: 0.0200 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 6299 \t loss: 0.1040 \t train acc: 0.8750\n",
      "epoch: 11 \t batch_idx : 6399 \t loss: 0.0341 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 6499 \t loss: 0.0321 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 6599 \t loss: 0.0229 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 6699 \t loss: 0.0295 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 6799 \t loss: 0.0715 \t train acc: 0.9062\n",
      "epoch: 11 \t batch_idx : 6899 \t loss: 0.0495 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 6999 \t loss: 0.0713 \t train acc: 0.8750\n",
      "epoch: 11 \t batch_idx : 7099 \t loss: 0.0544 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 7199 \t loss: 0.0256 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 7299 \t loss: 0.0225 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 7399 \t loss: 0.0259 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 7499 \t loss: 0.0161 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 7599 \t loss: 0.0137 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 7699 \t loss: 0.0501 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 7799 \t loss: 0.0184 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 7899 \t loss: 0.0562 \t train acc: 0.9062\n",
      "epoch: 11 \t batch_idx : 7999 \t loss: 0.0530 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 8099 \t loss: 0.0304 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 8199 \t loss: 0.0365 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 8299 \t loss: 0.0481 \t train acc: 0.9062\n",
      "epoch: 11 \t batch_idx : 8399 \t loss: 0.0437 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 8499 \t loss: 0.0343 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 8599 \t loss: 0.0462 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 8699 \t loss: 0.0149 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 8799 \t loss: 0.0353 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 8899 \t loss: 0.0373 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 8999 \t loss: 0.0129 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 9099 \t loss: 0.0389 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 9199 \t loss: 0.0427 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 9299 \t loss: 0.0231 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 9399 \t loss: 0.0167 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 9499 \t loss: 0.0266 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 9599 \t loss: 0.0221 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 9699 \t loss: 0.0061 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 9799 \t loss: 0.0131 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 9899 \t loss: 0.0331 \t train acc: 0.9062\n",
      "epoch: 11 \t batch_idx : 9999 \t loss: 0.0080 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 10099 \t loss: 0.0140 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 10199 \t loss: 0.0099 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 10299 \t loss: 0.0338 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 10399 \t loss: 0.0106 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 10499 \t loss: 0.0302 \t train acc: 0.9062\n",
      "epoch: 11 \t batch_idx : 10599 \t loss: 0.0090 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 10699 \t loss: 0.0015 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 10799 \t loss: 0.0373 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 10899 \t loss: 0.0362 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 10999 \t loss: 0.0245 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 11099 \t loss: 0.0293 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 11199 \t loss: 0.0347 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 11299 \t loss: 0.1020 \t train acc: 0.8750\n",
      "epoch: 11 \t batch_idx : 11399 \t loss: 0.0680 \t train acc: 0.9062\n",
      "epoch: 11 \t batch_idx : 11499 \t loss: 0.0511 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 11599 \t loss: 0.0345 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 11699 \t loss: 0.0091 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 11799 \t loss: 0.0153 \t train acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11 \t batch_idx : 11899 \t loss: 0.0394 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 11999 \t loss: 0.1364 \t train acc: 0.8438\n",
      "epoch: 11 \t batch_idx : 12099 \t loss: 0.0202 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 12199 \t loss: 0.0244 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 12299 \t loss: 0.0506 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 12399 \t loss: 0.0109 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 12499 \t loss: 0.0908 \t train acc: 0.9062\n",
      "epoch: 11 \t batch_idx : 12599 \t loss: 0.0029 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 12699 \t loss: 0.0481 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 12799 \t loss: 0.0186 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 12899 \t loss: 0.0102 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 12999 \t loss: 0.0394 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 13099 \t loss: 0.0358 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 13199 \t loss: 0.0186 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 13299 \t loss: 0.0143 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 13399 \t loss: 0.0261 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 13499 \t loss: 0.0123 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 13599 \t loss: 0.0190 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 13699 \t loss: 0.0470 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 13799 \t loss: 0.0022 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 13899 \t loss: 0.0173 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 13999 \t loss: 0.0174 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 14099 \t loss: 0.0099 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 14199 \t loss: 0.0760 \t train acc: 0.8750\n",
      "epoch: 11 \t batch_idx : 14299 \t loss: 0.0097 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 14399 \t loss: 0.0181 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 14499 \t loss: 0.0129 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 14599 \t loss: 0.0626 \t train acc: 0.9062\n",
      "epoch: 11 \t batch_idx : 14699 \t loss: 0.0116 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 14799 \t loss: 0.1079 \t train acc: 0.8750\n",
      "epoch: 11 \t batch_idx : 14899 \t loss: 0.0201 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 14999 \t loss: 0.0149 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 15099 \t loss: 0.0317 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 15199 \t loss: 0.0098 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 15299 \t loss: 0.0652 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 15399 \t loss: 0.0108 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 15499 \t loss: 0.0655 \t train acc: 0.9062\n",
      "epoch: 11 \t batch_idx : 15599 \t loss: 0.0214 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 15699 \t loss: 0.0347 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 15799 \t loss: 0.0365 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 15899 \t loss: 0.0188 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 15999 \t loss: 0.0402 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 16099 \t loss: 0.0465 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 16199 \t loss: 0.0190 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 16299 \t loss: 0.0425 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 16399 \t loss: 0.0256 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 16499 \t loss: 0.0445 \t train acc: 0.9688\n",
      "epoch: 11 \t batch_idx : 16599 \t loss: 0.0286 \t train acc: 0.9375\n",
      "epoch: 11 \t batch_idx : 16699 \t loss: 0.1104 \t train acc: 0.9062\n",
      "epoch: 11 \t batch_idx : 16799 \t loss: 0.0135 \t train acc: 1.0000\n",
      "epoch: 11 \t batch_idx : 16899 \t loss: 0.0755 \t train acc: 0.9062\n",
      "test acc: 0.7952\n"
     ]
    }
   ],
   "source": [
    "#train_iter\n",
    "for epoch in range(n_epoch):\n",
    "    for batch_idx, batch in enumerate(train_iter):\n",
    "        data = batch.Seqs\n",
    "        target = batch.Label\n",
    "        target = torch.sparse.torch.eye(n_class).index_select(dim=0, index=target.cpu().data)\n",
    "        target = target.to(DEVICE)\n",
    "        data = data.permute(1,0)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        # loss = -target*torch.log(out)-(1-target)*torch.log(1-out)\n",
    "        # loss = loss.sum(-1).mean()\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "        output = torch.sqrt(torch.sum(output*output, 2))\n",
    "        loss1 = target*F.relu(0.9-output)**2 + 0.5*(1-target)*F.relu(output-0.1)**2\n",
    "        loss1 = loss1.sum(dim=1).mean()\n",
    "        #loss2 = ((data-pred_img)**2).mean()\n",
    "        loss = loss1#+0.0005*loss2\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if (batch_idx+1) %100 == 0:\n",
    "            _,y_pre = torch.max(output,-1)\n",
    "            acc = torch.mean((torch.tensor(y_pre == batch.Label,dtype=torch.float)))\n",
    "            print('epoch: %d \\t batch_idx : %d \\t loss: %.4f \\t train acc: %.4f'\n",
    "                  %(epoch,batch_idx,loss,acc))\n",
    "    \n",
    "    val_accs = []\n",
    "    #val_iter\n",
    "    for batch_idx, batch in enumerate(val_iter):\n",
    "        data = batch.Seqs\n",
    "        target = batch.Label\n",
    "        target = torch.sparse.torch.eye(n_class).index_select(dim=0, index=target.cpu().data)\n",
    "        target = target.to(DEVICE)\n",
    "        data = data.permute(1,0)\n",
    "        out = model(data)\n",
    "        out = torch.sqrt(torch.sum(out*out, 2))\n",
    "        _,y_pre = torch.max(out,-1)\n",
    "        acc = torch.mean((torch.tensor(y_pre == batch.Label,dtype=torch.float)))\n",
    "        val_accs.append(acc)\n",
    "    acc =torch.mean(torch.stack(val_accs))\n",
    "    #acc = np.array(val_accs).mean()\n",
    "    if acc > best_val_acc:\n",
    "        print('val acc : %.4f > %.4f saving model'%(acc,best_val_acc))\n",
    "        torch.save(model.state_dict(), 'compareModel/2021MDPI_CapsNet/best_params.pkl')\n",
    "        best_val_acc = acc\n",
    "    print('test acc: %.4f'%(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-16T17:30:55.332328Z",
     "start_time": "2022-01-16T17:25:02.702501Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "model.load_state_dict(torch.load('compareModel/2021MDPI_CapsNet/best_params.pkl'))#, map_location='cpu'))\n",
    "model.eval()\n",
    "\n",
    "all_pred1 = []\n",
    "all_true1 = []\n",
    "all_p1 = []\n",
    "\n",
    "for batch_idx, batch in enumerate(test_iter):\n",
    "    data = batch.Seqs\n",
    "    data = data.permute(1, 0)\n",
    "    \n",
    "    out = model(data)\n",
    "    out = torch.sqrt(torch.sum(out * out, 2))\n",
    "    out1 = out[:, 1]\n",
    "    _, y_pre = torch.max(out, -1)\n",
    "    all_p1.extend(list(out1.cpu().detach().numpy()))\n",
    "    all_pred1.extend(list(y_pre.cpu().detach().numpy()))\n",
    "    all_true1.extend(list(batch.Label.cpu().detach().numpy()))\n",
    "    \n",
    "# file = open(args.result,'w');\n",
    "# file.write(str(all_pred1));\n",
    "# file.close();\n",
    "# print(\"The results have been saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-16T17:30:55.418828Z",
     "start_time": "2022-01-16T17:30:55.338897Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8203267589049161\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(all_true1,all_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T16:49:48.851274Z",
     "start_time": "2022-01-17T16:49:48.574510Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.81      0.82     67942\n",
      "           1       0.82      0.83      0.82     67938\n",
      "\n",
      "    accuracy                           0.82    135880\n",
      "   macro avg       0.82      0.82      0.82    135880\n",
      "weighted avg       0.82      0.82      0.82    135880\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(all_true1, all_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T16:50:19.365611Z",
     "start_time": "2022-01-17T16:50:19.013473Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf auc : 0.8969610392752138\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARIAAAEWCAYAAACqphg1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAhS0lEQVR4nO3dfZgV5X3/8fdHHgIqAgKp8hQQSRTkwYBSbE1BoyJqIg0KmDZpai41FatJ06LGWJrHprE1sRoNiZaYn0CjqDGWqFFUkqhRVETEaEAgLIEIRBFR5On7+2Nml8OyD7OcnbPn7H5e17XXnjnnPjPfs3o+3HPPzD2KCMzMinFQSxdgZpXPQWJmRXOQmFnRHCRmVjQHiZkVzUFiZkVzkJhZ0RwkbZCk1ZLelfS2pA2SZks6tFabkyQtlLRV0hZJP5M0pFabwyR9R9Lv03WtSJd7lvYTWUtzkLRd50TEocBI4HjgquoXJI0FHgJ+CvQGBgIvAL+WdFTapiPwCDAUmAAcBpwEbAZOzKtoSe3zWrcdOAdJGxcRG4AHSQKl2n8At0fEdyNia0T8KSKuAZ4CZqZtPgX0ByZFxPKI2BMRr0fEVyNiQV3bkjRU0i8k/UnSHyVdnT4/W9LXCtqNk1RVsLxa0gxJS4Ftkq6RdFetdX9X0g3p466SbpW0XtI6SV+T1K64v5Q1xEHSxknqC5wJrEiXDybpWdxZR/OfAKeljz8KPBARb2fcThfgYeABkl7O0SQ9mqymAWcB3YAfAxMlHZauux1wPjAnbfsjYFe6jeOB04HPNmFb1kQOkrbrXklbgbXA68C/ps8fTvL/xfo63rMeqB7/6FFPm/qcDWyIiP+MiO1pT+c3TXj/DRGxNiLejYg1wHPAuelrpwDvRMRTkv6MJBiviIhtEfE6cD0wtQnbsiZykLRd50ZEF2AccAx7A+INYA9wZB3vORLYlD7eXE+b+vQDVh5QpYm1tZbnkPRSAC5gb2/kA0AHYL2kNyW9CXwfeH8R27ZGOEjauIh4HJgNXJcubwOeBM6ro/n57N0deRg4Q9IhGTe1FhhUz2vbgIMLlo+oq9Ray3cC49Jds0nsDZK1wHtAz4jolv4cFhFDM9ZpB8BBYgDfAU6TNDJdvhL4tKR/lNRFUvd0MHQs8G9pmx+TfGnnSzpG0kGSeki6WtLEOrZxP3CEpCskvS9d75j0tSUkYx6HSzoCuKKxgiNiI/AY8D/Aqoh4OX1+PckRp/9MD08fJGmQpL9q4t/EmsBBYtVfytuBL6fLvwLOAP6aZBxkDcmg5V9GxO/SNu+RDLj+FvgF8BbwNMku0n5jHxGxlWSg9hxgA/A7YHz68o9JDi+vJgmB/81Y+py0hjm1nv8U0BFYTrKrdhdN2w2zJpInNjKzYrlHYmZFc5CYWdEcJGZWNAeJmRWt4i6A6tmzZwwYMKClyzBrc5599tlNEdGrrtcqLkgGDBjA4sWLW7oMszZH0pr6XvOujZkVzUFiZkVzkJhZ0RwkZlY0B4mZFS23IJF0m6TXJS2r53VJuiGdMHippA/nVYuZ5SvPw7+zgRtJriqty5nA4PRnDHBz+tvM5h8J723IfzvtDoYjToch/wK9xh7wanILkohYJGlAA00+TjLBcABPSeom6ch0Pgmz8jRHLV1B89r9Dqy7F/7wf/DRxw84TFryhLQ+7Dt9XlX63H5BIuki4CKA/v37l6Q4a4WenwEv/0dLV1GeYie8/lhFBkld0V7n5CgRMQuYBTB69GhPoGJ7tbYeQktRB3j/uAN+e0sGSRXJhMDV+gJ/aKFarFw5KPJV7mMkGdwHTJc0j2SQdYvHR9ooh0X92nWBKW+1dBWNyi1IJM0ludVBz/Suaf9KcpsAIuIWYAEwkeTGTO8An8mrFisDrS0suhwL5yxv6SrKRp5HbaY18noAl+a1fWtBlRAaF3iorTlV3DQCVobKJTgcDi3GQWJN11LB4aAoWw4Sy6YU4fG+I+ATHm+vRA4Sq1uep2i7Z9HqOEhsrzx6HQ6NNsFB0tY1d3g4ONokB0lb1Jzh4eAwHCRtx5yDqOdSpiYSXLCnGdZjrYmDpLVrjt6Hex3WCAdJa1VsgDg8rAkcJK1NMQHi8LAD5CBpLQ40QBwe1gwcJJXuQALE4WHNzEFSqRwgVkYcJJXGAWJlyEFSSZoaIg4QKxEHSSVwgFiZc5CUMweIVQgHSblqSog4QKyFOUjKzZz2wO5sbR0gViYcJOUkcy/EF85ZeXGQlAPvxliFc5C0tKwh4gCxMnZQSxfQpjlErJVwj6SlZAkRB4hVCPdIWoJDxFoZ90hKrbEQcYBYBXKPpJQcItZKOUhKxSFirZiDpBQcItbKOUjy5hCxNiDXIJE0QdIrklZIurKO17tK+pmkFyS9JOkzedZTcg4RayNyCxJJ7YCbgDOBIcA0SUNqNbsUWB4RI4BxwH9K6phXTSXlELE2JM8eyYnAioh4LSJ2APOAj9dqE0AXSQIOBf4E7MqxptJwiFgbk2eQ9AHWFixXpc8VuhE4FvgD8CJweUTsd1mrpIskLZa0eOPGjXnV2zwcItYG5RkkdX2jan+LzgCWAL2BkcCNkg7b700RsyJidESM7tWrV3PX2XwcItZG5RkkVUC/guW+JD2PQp8B7o7ECmAVcEyONeXHIWJtWJ5B8gwwWNLAdAB1KnBfrTa/B04FkPRnwIeA13KsqWU4RKyVy+1am4jYJWk68CDQDrgtIl6SdEn6+i3AV4HZkl4k2RWaERGb8qopNw31Rhwi1gbketFeRCwAFtR67paCx38ATs+zhtw5RMx8ZquZFc9BUgz3RswAB8mBc4iY1XCQNLcux7Z0BWYl5yA5EA31Rs5ZXro6zMqEg6Sp5h9Z/2vepbE2ykHSVO9tqPv59x1R2jrMyoiDpCka2qX5xPrS1WFWZhwkzcG7NNbGOUiyasr9ec3amMxBIumQPAupWO6NmDUeJJJOkrQceDldHiHpe7lXVk7cGzFrUJYeyfUkExBtBoiIF4CP5FlUxXBvxAzIuGsTEWtrPbU7h1rKk3sjZo3KMo3AWkknAZFOUPSPpLs5bZp7I2Y1svRILiG5bUQfkukTRwL/kGNN5aOhs1jNrEaWHsmHIuKThU9I+gvg1/mUVEbqO4vVvRGzfWTpkfx3xufMrI2qt0ciaSxwEtBL0hcKXjqMZA7W1q2+QVb3Rsz209CuTUeSu9+1B7oUPP8WMDnPosysstQbJBHxOPC4pNkRsaaENZlZhcky2PqOpG8DQ4FO1U9GxCm5VdXSvFtj1iRZBlvvAH4LDAT+DVhNcvMrMzMgW5D0iIhbgZ0R8XhE/D3w5znXZWYVJMuuzc7093pJZ5Hcv7dvfiW1MO/WmDVZliD5mqSuwD+RnD9yGHBFnkWZWWVpNEgi4v704RZgPNSc2dp2HPsvLV2BWVlr6IS0dsD5JNfYPBARyySdDVwNdAaOL02JJVTfbs3x3yptHWYVpqEeya1AP+Bp4AZJa4CxwJURcW8JajOzCtFQkIwGhkfEHkmdgE3A0RFRz5VsrZTvnGfWqIYO/+6IiD0AEbEdeLWpISJpgqRXJK2QdGU9bcZJWiLpJUmPN2X9zaq+3RrfOc+sUQ31SI6RtDR9LGBQuiwgImJ4QytOx1huAk4jmcfkGUn3RcTygjbdgO8BEyLi95Lef+AfxcxaSkNBUmyf/kRgRUS8BiBpHvBxoPCf+AuAuyPi9wAR8XqR2zSzFtDQRXvFXqjXByic67UKGFOrzQeBDpIeI7nC+LsRcXvtFUm6CLgIoH///kWWVQefhGZWlDxvkFXXt7P2N7M9MAo4i2Sm+i9L+uB+b4qYFRGjI2J0r169mr9SMytKljNbD1QVyeHjan1JTq+v3WZTRGwDtklaBIwAXs2xLjNrZpl6JJI6S/pQE9f9DDBY0sB09vmpwH212vwUOFlSe0kHk+z6lMcM9d6tMcssy532zgGWAA+kyyMl1Q6E/UTELmA68CBJOPwkIl6SdImkS9I2L6frXUpy4tsPI2LZAX6WA+P71pgVLcuuzUySIzCPAUTEEkkDsqw8IhYAC2o9d0ut5W8D386yPjMrT1l2bXZFxJbcKzGzipWlR7JM0gVAO0mDSe6090S+ZbUwj4+YNUmWHsllJPO1vgfMIZlO4Iocayodj4+YNYusd9r7EvClvIsxs8qUpUfyX5J+K+mrkobmXlFLa9el8TZmto9GgyQixgPjgI3ALEkvSrom78JazJS3WroCs4qT6YS0iNgQETcAl5CcU3JtnkWVhMdHzJpNlhPSjpU0U9Iy4EaSIzatdxZ5M2uyLIOt/wPMBU6PiNrXypiZZZpFvu3cDMvnj5gdkIZmkf9JRJwv6UX2vfw/0wxpZtZ2NNQjuTz9fXYpCikpD7SaNat6B1sjYn368B8iYk3hD/APpSnPzCpBlsO/p9Xx3JnNXYiZVa6Gxkg+R9LzOKpgNnlI5lb9dd6FlZwHWs0OWENjJHOAnwPfBArvSbM1Iv6Ua1VmVlEaCpKIiNWSLq39gqTDHSZmVq2xHsnZwLMkh38LD3UEcFSOdeXHR2zMml1D97U5O/09sHTlmFklynKtzV9IOiR9/DeS/ktSDnepMrNKleXw783AO5JGAP8CrAF+nGtVpeYjNmZFyTr5c5Dct/e7EfFdkkPAZmZAtqt/t0q6CvhbkptZtQM65FuWmVWSLD2SKSQTP/99RGwguTm470NjZjWyTLW4AbgD6CrpbGB7RNyee2V58KFfs1xkOWpzPsntNM8Dzgd+I2ly3oWZWeXIMkbyJeCEiHgdQFIv4GHgrjwLM7PKkWWM5KDqEEltzvi+yuBDv2ZFy9IjeUDSgyTztkIy+LqggfZm1sZkmbP1nyX9NfCXJNfbzIqIe3KvzMwqRkPzkQwGrgMGAS8CX4yIdaUqzMwqR0NjHbcB9wOfILkC+L+bunJJEyS9ImmFpCsbaHeCpN25Hg1aeEZuqzZr6xratekSET9IH78i6bmmrDg9A/Ymkqkaq4BnJN0XEcvraPct4MGmrL/JNjyU6+rN2rKGgqSTpOPZOw9J58LliGgsWE4EVkTEawCS5pFcr7O8VrvLgPnACU2s3czKRENBsh74r4LlDQXLAZzSyLr7AGsLlquAMYUNJPUBJqXrqjdIJF0EXATQv38zzmDgQ79mzaKhiY3GF7nuus5Hr/3N/Q4wIyJ2S/Wfvh4Rs4BZAKNHj/a336zMZDmP5EBVAf0KlvsCte8dPBqYl4ZIT2CipF0RcW+OdZlZM8szSJ4BBksaCKwDpgIXFDYonMZR0mzgfoeIWeXJLUgiYpek6SRHY9oBt0XES5IuSV+/Ja9tm1lpNRokSvY7PgkcFRFfSedrPSIinm7svRGxgFqn09cXIBHxd5kqNrOyk+Xiu+8BY4Fp6fJWkvNDKofnITHLVZZdmzER8WFJzwNExBuSOuZcl5lVkCw9kp3p2acBNfOR7Mm1KjOrKFmC5AbgHuD9kr4O/Ar4Rq5VlYJPRjNrNlmmEbhD0rPAqSQnmZ0bES/nXpmZVYwsR236A+8APyt8LiJ+n2dhZlY5sgy2/h97byLeCRgIvAIMzbEuM6sgWXZthhUuS/owcHFuFZlZxWnyJM7p9AG+5N/MamQZI/lCweJBwIeBjblVZGYVJ8sYSeENw3eRjJnMz6ecHPisVrPcNRgk6Yloh0bEP5eoHjOrQPWOkUhqHxG7SXZlWpcjTm/pCsxalYZ6JE+ThMgSSfcBdwLbql+MiLtzri0/p+Q7z7RZW5NljORwktt0nsLe80kCqNwgMbNm1VCQvD89YrOMvQFSzReqmFmNhoKkHXAo2SZxNrM2rMHbUUTEV0pWiZlVrIbObPUJGGaWSUNBcmrJqjCzilZvkETEn0pZSC58VqtZSTT5oj0zs9ocJGZWtLYXJJ6r1azZtb0gMbNm5yAxs6I5SMysaA4SMyuag8TMiuYgMbOi5RokkiZIekXSCklX1vH6JyUtTX+ekDQiz3rMLB+5BUk63+tNwJnAEGCapCG1mq0C/ioihgNfBWblVY+Z5SfPHsmJwIqIeC0idgDzgI8XNoiIJyLijXTxKaBvjvWYWU7yDJI+wNqC5ar0ufpcCPy8rhckXSRpsaTFGzdmvKWOL9gzK5k8gyTzzGqSxpMEyYy6Xo+IWRExOiJG9+rVqxlLNLPmkGXy5wNVBfQrWO4L/KF2I0nDgR8CZ0bE5hzrgdOeyHX1Zm1Vnj2SZ4DBkgZK6ghMBe4rbCCpP8ls9H8bEa/mWEui19jcN2HWFuXWI4mIXZKmAw+STCR9W0S8JOmS9PVbgGuBHsD3JAHsiojRedVkZvnIc9eGiFgALKj13C0Fjz8LfDbPGswsfz6z1cyK5iAxs6I5SMysaA4SMyuag8TMiuYgMbOiOUjMrGgOEjMrmoPEzIrWOoNk45MtXYFZm9I6g+QXJ7V0BWZtSusMEjMrqbYTJL7nr1lu2k6QmFluHCRmVjQHiZkVzUFiZkVzkJhZ0RwkZlY0B4mZFc1BYmZFy3UWeWt9du7cSVVVFdu3b2/pUiwnnTp1om/fvnTo0CHzexwk1iRVVVV06dKFAQMGkN6LyFqRiGDz5s1UVVUxcODAzO/zro01yfbt2+nRo4dDpJWSRI8ePZrc43SQWJM5RFq3A/nv6yAxs6I5SKzitGvXjpEjR3Lcccdxzjnn8OabbwKwevVqOnfuzMiRI2t+duzYUec6Lr/8cvr06cOePXtqnps5cybXXXfdPu0GDBjApk2bANiwYQNTp05l0KBBDBkyhIkTJ/Lqq68W9Vnee+89pkyZwtFHH82YMWNYvXp1ne3mzp3LsGHDGD58OBMmTKipac2aNZx66qkMHz6ccePGUVVVBcCjjz66z9+hU6dO3HvvvQCsWrWKMWPGMHjwYKZMmVLv36gpHCSWv41PwkvfbLaZ6zp37sySJUtYtmwZhx9+ODfddFPNa4MGDWLJkiU1Px07dtzv/Xv27OGee+6hX79+LFq0KNM2I4JJkyYxbtw4Vq5cyfLly/nGN77BH//4x6I+y6233kr37t1ZsWIFn//855kxY8Z+bXbt2sXll1/Oo48+ytKlSxk+fDg33ngjAF/84hf51Kc+xdKlS7n22mu56qqrABg/fnzN32DhwoUcfPDBnH766QDMmDGDz3/+8/zud7+je/fu3HrrrUV9BvBRGyvGs1fAG0sabrNzC7yxFNgDHATdh0OHrvW37z4SRn0ncwljx45l6dKlmdtD8q/1cccdx5QpU5g7dy7jxo3L9J4OHTpwySWX1Dw3cuTIJm23Lj/96U+ZOXMmAJMnT2b69OlExD7jFBFBRLBt2zZ69OjBW2+9xdFHHw3A8uXLuf7664EkPM4999z9tnHXXXdx5plncvDBBxMRLFy4kDlz5gDw6U9/mpkzZ/K5z32uqM/R+nokczwQWFZ2bCEJEZLfO7Y026p3797NI488wsc+9rGa51auXFnTnb/00kvrfN/cuXOZNm0akyZN4v7772fnzp2NbmvZsmWMGjUqU10nn3zyPrsV1T8PP/zwfm3XrVtHv379AGjfvj1du3Zl8+bN+7Tp0KEDN998M8OGDaN3794sX76cCy+8EIARI0Ywf/58AO655x62bt263/vnzZvHtGnTANi8eTPdunWjffukD9G3b1/WrVuX6XM1xD0SO3BZeg4bn4SFp8KeHXBQRzjpDug1tqjNvvvuu4wcOZLVq1czatQoTjvttJrXqndt6rNjxw4WLFjA9ddfT5cuXRgzZgwPPfQQZ511Vr1HK5p6FOOXv/xl5rYR+8/cV3t7O3fu5Oabb+b555/nqKOO4rLLLuOb3/wm11xzDddddx3Tp09n9uzZfOQjH6FPnz41IQGwfv16XnzxRc4444zM2zsQufZIJE2Q9IqkFZKurON1SbohfX2ppA/nWY+1gF5j4ZRHYPhXk99FhgjsHSNZs2YNO3bs2GeMpDEPPPAAW7ZsYdiwYQwYMIBf/epXzJ07F4AePXrwxhtv7NN+69atdOvWjaFDh/Lss89m2kZTeiR9+/Zl7dq1QDIWsmXLFg4//PB92lQH46BBg5DE+eefzxNPPAFA7969ufvuu3n++ef5+te/DkDXrnt3HX/yk58wadKkmrNUe/bsyZtvvsmuXbuA5ATD3r17Z/pcDare/2ruH6AdsBI4CugIvAAMqdVmIvBzQMCfA79pbL2jRo2KBt3B/j/WbJYvX97SJcQhhxxS8/i5556Lfv36xY4dO2LVqlUxdOjQBt87derUmDNnTs3y22+/Hb169Ypt27bFCy+8EMcdd1y89dZbERExf/78GD9+fERE7NmzJ0488cSYNWtWzXuffvrpeOyxx4r6LDfeeGNcfPHFERExd+7cOO+88/Zrs27dujjiiCPi9ddfj4iIa665Jr7whS9ERMTGjRtj9+7dERFx9dVXx5e//OV93jtmzJhYuHDhPs9Nnjw55s6dGxERF198cdx00037bbOu/87A4qjv+17fC8X+AGOBBwuWrwKuqtXm+8C0guVXgCMbWq+DpGWVW5BERJx99tlx++23Nxok27Zti+7du8eWLVv2eX7SpEkxb968iIi45ZZbYvjw4TFixIg47bTTYuXKlTXt1q1bF+edd14cddRRMWTIkJg4cWK8+uqrRX2Wd999NyZPnhyDBg2KE044YZ/tjRgxoubxzTffHMccc0wMGzYszj777Ni0aVNERNx5551x9NFHx+DBg+PCCy+M7du317xn1apV0bt375qgqbZy5co44YQTYtCgQTF58uR93lOtqUGiqGOfqTlImgxMiIjPpst/C4yJiOkFbe4H/j0ifpUuPwLMiIjFtdZ1EXARQP/+/UetWbOm/g3XNdjqGeSbzcsvv8yxxx7b0mVYzur67yzp2YgYXVf7PMdI6hrBqf2NztKGiJgVEaMjYnSvXr2apTgzaz55BkkV0K9guS/whwNo0zS1ex/ujZjlLs/Dv88AgyUNBNYBU4ELarW5D5guaR4wBtgSEeuL3rLDI1dR64Qpa10OZLgjtyCJiF2SpgMPkhzBuS0iXpJ0Sfr6LcACkiM3K4B3gM/kVY81j06dOrF582ZPJdBKRTofSadOnZr0vtwGW/MyevToWLx4ceMNLReeIa31q2+GtIYGW31mqzVJhw4dmjRzlrUNre9aGzMrOQeJmRXNQWJmRau4wVZJG4EGTm2t0RPYlHM5xXKNxSv3+qD8a8xa3wcios4zQisuSLKStLi+EeZy4RqLV+71QfnX2Bz1edfGzIrmIDGzorXmIJnV0gVk4BqLV+71QfnXWHR9rXaMxMxKpzX3SMysRBwkZla0ig+SSphgOkONn0xrWyrpCUkjyqm+gnYnSNqdzn5XUllqlDRO0hJJL0l6vJzqk9RV0s8kvZDWV9Ir3SXdJul1Scvqeb2470l9czBWwg85TTDdAjWeBHRPH59Zyhqz1FfQbiHJ1A+Ty/Bv2A1YDvRPl99fZvVdDXwrfdwL+BPQsYQ1fgT4MLCsnteL+p5Ueo/kRGBFRLwWETuAecDHa7X5OHB7JJ4Cukk6spxqjIgnIqL6PghPkcwUVzb1pS4D5gOvl7C2allqvAC4OyJ+DxARpawzS30BdFEyicuhJEGyq1QFRsSidJv1Kep7UulB0gdYW7BclT7X1DZ5aur2LyT5l6FUGq1PUh9gEnBLCesqlOVv+EGgu6THJD0r6VMlqy5bfTcCx5JMJfoicHlE7KF8FPU9qfT5SJptgukcZd6+pPEkQfKXuVZUa7N1PFe7vu+QzO6/u4VmRctSY3tgFHAq0Bl4UtJTEfFq3sWRrb4zgCXAKcAg4BeSfhkRb+VcW1ZFfU8qPUhaZoLppsm0fUnDgR8CZ0bE5tqv5yhLfaOBeWmI9AQmStoVEfeWpMLs/503RcQ2YJukRcAIoBRBkqW+z5DceiWAFZJWAccAT5egviyK+56UarAnpwGk9sBrwED2DnINrdXmLPYdRHq6DGvsTzJv7Unl+Des1X42pR9szfI3PBZ4JG17MLAMOK6M6rsZmJk+/jOSCdF7lvjvOID6B1uL+p5UdI8kKmCC6Yw1Xgv0AL6X/qu/K0p0tWjG+lpUlhoj4mVJDwBLgT3ADyOizkOdLVEf8FVgtqQXSb6sMyKiZFMLSJoLjAN6SqoC/hXoUFBfUd8TnyJvZkWr9KM2ZlYGHCRmVjQHiZkVzUFiZkVzkJhZ0RwkFSq9CndJwc+ABtq+3Qzbmy1pVbqt5ySNPYB1/FDSkPTx1bVee6LYGtP1VP9dlqVX23ZrpP1ISRObY9ttmQ//VihJb0fEoc3dtoF1zAbuj4i7JJ0OXBcRw4tYX9E1NbZeST8CXo2IrzfQ/u+A0RExvblraUvcI2klJB0q6ZG0t/CipP2u4JV0pKRFBf9in5w+f7qkJ9P33impsS/4IuDo9L1fSNe1TNIV6XOHSPq/dO6NZZKmpM8/Jmm0pH8HOqd13JG+9nb6+38LewhpT+gTktpJ+rakZ9L5Mi7O8Gd5kvTCM0knKpnr5fn094ckdQS+AkxJa5mS1n5bup3n6/o7Wh1KeYquf5r1dOfdJBeBLQHuITlN+7D0tZ4kZyhW9zjfTn//E/Cl9HE7oEvadhFwSPr8DODaOrY3m/TUeOA84DckF8m9CBxCcmn8S8DxwCeAHxS8t2v6+zGSf/1raipoU13jJOBH6eOOJFekdgYuAq5Jn38fsBgYWEedbxd8vjuBCenyYUD79PFHgfnp478Dbix4/zeAv0kfdyO5VueQlv7vXe4/FX2KfBv3bkSMrF6Q1AH4hqSPkJwi3ofkmo4NBe95BrgtbXtvRCyR9FfAEODX6en5HUn+Ja/LtyVdA2wkuUr5VOCeSC6UQ9LdwMnAA8B1kr5Fsjv0yyZ8rp8DN0h6HzABWBQR76a7U8O1d3a2rsBgYFWt93eWtITkupJngV8UtP+RpMEkV7V2qGf7pwMfk/TFdLkTybVQLzfhM7Q5DpLW45MkM2+NioidklaTfAlqRMSiNGjOAn4s6dvAG8AvImJahm38c0TcVb0g6aN1NYqIVyWNIrl245uSHoqIr2T5EBGxXdJjJJfdTwHmVm8OuCwiHmxkFe9GxEhJXYH7gUuBG0iudXk0IialA9OP1fN+AZ+IiFey1GsJj5G0Hl2B19MQGQ98oHYDSR9I2/wAuJVk6r2ngL+QVD3mcbCkD2bc5iLg3PQ9h5DslvxSUm/gnYj4f8B16XZq25n2jOoyj+SisZNJLoQj/f256vdI+mC6zTpFxBbgH4Evpu/pSnLFLSS7M9W2kuziVXsQuExp90zS8fVtw/ZykLQedwCjJS0m6Z38to4244Alkp4nGcf4bkRsJPlizZW0lCRYjsmywYh4jmTs5GmSMZMfRsTzwDDg6XQX40vA1+p4+yxgafVgay0Pkcwx+nAkUxdCMlfLcuA5JRMYf59GetRpLS8AU4H/IOkd/Zpk/KTao8CQ6sFWkp5Lh7S2ZemyNcKHf82saO6RmFnRHCRmVjQHiZkVzUFiZkVzkJhZ0RwkZlY0B4mZFe3/A7Ph3RUf7jUfAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# AUC\n",
    "\n",
    "rf_auc = roc_auc_score(all_true1, all_p1)\n",
    "print('rf auc : {}'.format(rf_auc))\n",
    "# plot the roc curve for the model_high\n",
    "rf_fpr, rf_tpr, _ = roc_curve(all_true1, all_p1)\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.plot(rf_fpr, rf_tpr, marker='.', label='RF AUC = {:.4f}'.format(rf_auc), color='orange')\n",
    "plt.title('ROC curve')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T16:50:33.737341Z",
     "start_time": "2022-01-17T16:50:33.326748Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cf_matrix = confusion_matrix(all_true1, all_pred1)\n",
    "plt.figure(figsize=(3, 3))\n",
    "sns.heatmap(cf_matrix, annot=True, fmt=',.0f')\n",
    "plt.show()\n",
    "plt.figure(figsize=(3, 3))\n",
    "sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, \n",
    "            fmt='.2%', cmap='Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-23T15:00:10.789435Z",
     "start_time": "2022-01-23T15:00:10.462394Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bis/miniconda3/envs/torch/lib/python3.6/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/home/bis/miniconda3/envs/torch/lib/python3.6/site-packages/torchtext/data/example.py:68: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
      "/home/bis/miniconda3/envs/torch/lib/python3.6/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
      "/home/bis/miniconda3/envs/torch/lib/python3.6/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from torchtext import data, datasets\n",
    "LABEL = data.Field(sequential=False, use_vocab=False)\n",
    "TEXT = data.Field(sequential=True, tokenize=tokenizer ,fix_length=45)\n",
    "pe1,dmp,mp = data.TabularDataset.splits(\n",
    "        path='compareModel/2021MDPI_CapsNet/', \n",
    "    train='pe1.csv',\n",
    "    validation='dmp.csv',\n",
    "    test='mp.csv',\n",
    "    format='csv',\n",
    "    skip_header=True,\n",
    "    fields=[('Seqs', TEXT), ('Label', LABEL)])\n",
    "TEXT.build_vocab(pe1,dmp,mp)\n",
    "pe1_iter = data.BucketIterator(pe1, batch_size=32, sort_key=lambda x: len(x.Seqs), \n",
    "                                 shuffle=True,device=DEVICE)\n",
    "dmp_iter = data.BucketIterator(dmp, batch_size=32, sort_key=lambda x: len(x.Seqs), \n",
    "                                 shuffle=True,device=DEVICE)\n",
    "mp_iter = data.BucketIterator(mp, batch_size=32, sort_key=lambda x: len(x.Seqs), \n",
    "                                 shuffle=True,device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-23T15:00:52.402577Z",
     "start_time": "2022-01-23T15:00:14.455270Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bis/miniconda3/envs/torch/lib/python3.6/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "all_pred_pe1 = []\n",
    "all_true_pe1 = []\n",
    "all_p_pe1 = []\n",
    "\n",
    "for batch_idx, batch in enumerate(pe1_iter):\n",
    "    data = batch.Seqs\n",
    "    data = data.permute(1, 0)\n",
    "    \n",
    "    out = model(data)\n",
    "    out = torch.sqrt(torch.sum(out * out, 2))\n",
    "    out1 = out[:, 1]\n",
    "    _, y_pre = torch.max(out, -1)\n",
    "    all_p_pe1.extend(list(out1.cpu().detach().numpy()))\n",
    "    all_pred_pe1.extend(list(y_pre.cpu().detach().numpy()))\n",
    "    all_true_pe1.extend(list(batch.Label.cpu().detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-23T15:01:31.194338Z",
     "start_time": "2022-01-23T15:00:52.404513Z"
    }
   },
   "outputs": [],
   "source": [
    "all_pred_dmp = []\n",
    "all_true_dmp = []\n",
    "all_p_dmp = []\n",
    "\n",
    "for batch_idx, batch in enumerate(dmp_iter):\n",
    "    data = batch.Seqs\n",
    "    data = data.permute(1, 0)\n",
    "    \n",
    "    out = model(data)\n",
    "    out = torch.sqrt(torch.sum(out * out, 2))\n",
    "    out1 = out[:, 1]\n",
    "    _, y_pre = torch.max(out, -1)\n",
    "    all_p_dmp.extend(list(out1.cpu().detach().numpy()))\n",
    "    all_pred_dmp.extend(list(y_pre.cpu().detach().numpy()))\n",
    "    all_true_dmp.extend(list(batch.Label.cpu().detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-23T15:02:06.371248Z",
     "start_time": "2022-01-23T15:01:31.196758Z"
    }
   },
   "outputs": [],
   "source": [
    "all_pred_mp = []\n",
    "all_true_mp = []\n",
    "all_p_mp = []\n",
    "\n",
    "for batch_idx, batch in enumerate(mp_iter):\n",
    "    data = batch.Seqs\n",
    "    data = data.permute(1, 0)\n",
    "    \n",
    "    out = model(data)\n",
    "    out = torch.sqrt(torch.sum(out * out, 2))\n",
    "    out1 = out[:, 1]\n",
    "    _, y_pre = torch.max(out, -1)\n",
    "    all_p_mp.extend(list(out1.cpu().detach().numpy()))\n",
    "    all_pred_mp.extend(list(y_pre.cpu().detach().numpy()))\n",
    "    all_true_mp.extend(list(batch.Label.cpu().detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T02:29:49.833485Z",
     "start_time": "2022-01-18T02:29:49.831186Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(classification_report(all_true_pe1, all_pred_pe1))\n",
    "\n",
    "# # AUC\n",
    "# rf_auc = roc_auc_score(all_true_pe1, all_p_pe1)\n",
    "# print('rf auc : {}'.format(rf_auc))\n",
    "# # plot the roc curve for the model_high\n",
    "# rf_fpr, rf_tpr, _ = roc_curve(all_true_pe1, all_p_pe1)\n",
    "# plt.figure(figsize=(4, 4))\n",
    "# plt.plot(rf_fpr, rf_tpr, marker='.', label='RF AUC = {:.4f}'.format(rf_auc), color='orange')\n",
    "# plt.title('ROC curve')\n",
    "# # axis labels\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# # show the legend\n",
    "# plt.legend()\n",
    "# # show the plot\n",
    "# plt.show()\n",
    "\n",
    "# cf_matrix = confusion_matrix(all_true_pe1, all_pred_pe1)\n",
    "# plt.figure(figsize=(3, 3))\n",
    "# sns.heatmap(cf_matrix, annot=True, fmt=',.0f')\n",
    "# plt.show()\n",
    "# plt.figure(figsize=(3, 3))\n",
    "# sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, \n",
    "#             fmt='.2%', cmap='Blues')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
