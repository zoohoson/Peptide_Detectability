{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T21:28:22.103622Z",
     "start_time": "2021-08-30T21:28:21.201330Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import time\n",
    "import pickle\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw code\n",
    "  - paper's code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T21:28:22.113120Z",
     "start_time": "2021-08-30T21:28:22.105563Z"
    }
   },
   "outputs": [],
   "source": [
    "def genData(file,max_len):\n",
    "    aa_dict={'A':1,'R':2,'N':3,'D':4,'C':5,'Q':6,'E':7,'G':8,'H':9,'I':10,\n",
    "             'L':11,'K':12,'M':13,'F':14,'P':15,'O':16,'S':17,'U':18,'T':19,\n",
    "             'W':20,'Y':21,'V':22,'X':23}\n",
    "    with open(file, 'r') as inf:\n",
    "        lines = inf.read().splitlines()\n",
    "        \n",
    "    long_pep_counter=0\n",
    "    pep_codes=[]\n",
    "    labels=[]\n",
    "    for pep in lines:\n",
    "        pep,label=pep.split(\",\")\n",
    "        labels.append(int(label))\n",
    "        if not len(pep) > max_len:\n",
    "            current_pep=[]\n",
    "            for aa in pep:\n",
    "                current_pep.append(aa_dict[aa])\n",
    "            pep_codes.append(torch.tensor(current_pep))\n",
    "        else:\n",
    "            long_pep_counter += 1\n",
    "    print(\"length > {}:\".format(max_len),long_pep_counter)\n",
    "    data = rnn_utils.pad_sequence(pep_codes, batch_first=True)  # padding\n",
    "    return data,torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T21:28:22.705925Z",
     "start_time": "2021-08-30T21:28:22.688398Z"
    }
   },
   "outputs": [],
   "source": [
    "class newModel(nn.Module):\n",
    "    def __init__(self, vocab_size=24):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = 25\n",
    "        self.batch_size = 256\n",
    "        self.emb_dim = 512\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, self.emb_dim, padding_idx=0)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=1)\n",
    "        \n",
    "        self.gru = nn.GRU(self.emb_dim, self.hidden_dim, num_layers=2, \n",
    "                               bidirectional=True, dropout=0.2)\n",
    "        \n",
    "        \n",
    "        self.block1=nn.Sequential(nn.Linear(4050,1024),\n",
    "                                            nn.BatchNorm1d(1024),\n",
    "                                            nn.LeakyReLU(),\n",
    "                                            nn.Linear(1024,256),\n",
    "                                 )\n",
    "\n",
    "        self.block2=nn.Sequential(\n",
    "                                               nn.BatchNorm1d(256),\n",
    "                                               nn.LeakyReLU(),\n",
    "                                               nn.Linear(256,128),\n",
    "                                               nn.BatchNorm1d(128),\n",
    "                                               nn.LeakyReLU(),\n",
    "                                               nn.Linear(128,64),\n",
    "                                               nn.BatchNorm1d(64),\n",
    "                                               nn.LeakyReLU(),\n",
    "                                               nn.Linear(64,2)\n",
    "                                            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x=self.embedding(x)\n",
    "        output=self.transformer_encoder(x).permute(1, 0, 2)\n",
    "        output,hn=self.gru(output)\n",
    "        output=output.permute(1,0,2)\n",
    "        hn=hn.permute(1,0,2)\n",
    "        output=output.reshape(output.shape[0],-1)\n",
    "        hn=hn.reshape(output.shape[0],-1)\n",
    "        output=torch.cat([output,hn],1)\n",
    "        return self.block1(output)\n",
    "\n",
    "    def trainModel(self, x):\n",
    "        with torch.no_grad():\n",
    "            output=self.forward(x)\n",
    "        return self.block2(output)\n",
    "\n",
    "\n",
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        # euclidean_distance: [128]\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +     # calmp夹断用法\n",
    "                                      (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))     \n",
    "        \n",
    "        return loss_contrastive\n",
    "    \n",
    "    \n",
    "def collate(batch):\n",
    "    seq1_ls=[]\n",
    "    seq2_ls=[]\n",
    "    label1_ls=[]\n",
    "    label2_ls=[]\n",
    "    label_ls=[]\n",
    "    batch_size=len(batch)\n",
    "    for i in range(int(batch_size/2)):\n",
    "        seq1,label1=batch[i][0],batch[i][1]\n",
    "        seq2,label2=batch[i+int(batch_size/2)][0],batch[i+int(batch_size/2)][1]\n",
    "        label1_ls.append(label1.unsqueeze(0))\n",
    "        label2_ls.append(label2.unsqueeze(0))\n",
    "        label=(label1^label2)\n",
    "        seq1_ls.append(seq1.unsqueeze(0))\n",
    "        seq2_ls.append(seq2.unsqueeze(0))\n",
    "        label_ls.append(label.unsqueeze(0))\n",
    "    seq1=torch.cat(seq1_ls).to(device)\n",
    "    seq2=torch.cat(seq2_ls).to(device)\n",
    "    label=torch.cat(label_ls).to(device)\n",
    "    label1=torch.cat(label1_ls).to(device)\n",
    "    label2=torch.cat(label2_ls).to(device)\n",
    "    return seq1,seq2,label,label1,label2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T21:28:43.270230Z",
     "start_time": "2021-08-30T21:28:41.280021Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length > 81: 0\n",
      "torch.Size([90000, 79]) torch.Size([90000])\n"
     ]
    }
   ],
   "source": [
    "data,label=genData(\"compareModel/2021ACS_PepFormer/dataset/Homo_sapiens.csv\",81)\n",
    "print(data.shape,label.shape)\n",
    "\n",
    "train_data,train_label=data[:70000],label[:70000]\n",
    "test_data,test_label=data[70000:],label[70000:]\n",
    "\n",
    "train_dataset = Data.TensorDataset(train_data, train_label)\n",
    "test_dataset = Data.TensorDataset(test_data, test_label)\n",
    "batch_size=256\n",
    "train_iter = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_iter = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_iter_cont = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \n",
    "                                                                  shuffle=True, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T21:28:48.296830Z",
     "start_time": "2021-08-30T21:28:44.688350Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newModel(\n",
      "  (embedding): Embedding(24, 512, padding_idx=0)\n",
      "  (encoder_layer): TransformerEncoderLayer(\n",
      "    (self_attn): MultiheadAttention(\n",
      "      (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
      "    )\n",
      "    (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout1): Dropout(p=0.1, inplace=False)\n",
      "    (dropout2): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (gru): GRU(512, 25, num_layers=2, dropout=0.2, bidirectional=True)\n",
      "  (block1): Sequential(\n",
      "    (0): Linear(in_features=4050, out_features=1024, bias=True)\n",
      "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01)\n",
      "    (3): Linear(in_features=1024, out_features=256, bias=True)\n",
      "  )\n",
      "  (block2): Sequential(\n",
      "    (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.01)\n",
      "    (5): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=64, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "total param cnt : 10,864,306\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\",0)\n",
    "model = newModel().to(device)\n",
    "\n",
    "print(model)\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print('total param cnt : {:,}'.format(params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T21:29:04.764149Z",
     "start_time": "2021-08-30T21:29:04.759884Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net):\n",
    "    acc_sum, n = 0.0, 0\n",
    "    for x, y in data_iter:\n",
    "        x,y=x.to(device),y.to(device)\n",
    "        outputs=net.trainModel(x)\n",
    "        acc_sum += (outputs.argmax(dim=1) == y).float().sum().item()\n",
    "        n += y.shape[0]\n",
    "    return acc_sum / n\n",
    "\n",
    "def to_log(log):\n",
    "    with open(\"compareModel/2021ACS_PepFormer/modelLog.log\",\"a+\") as f:\n",
    "        f.write(log+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T20:44:00.250478Z",
     "start_time": "2021-08-30T20:37:55.493756Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 181.73344, loss1: 2.59739, loss2_3: 179.13606\n",
      "\ttrain_acc: 0.5578, test_acc: \u001b[31m0.5561\u001b[0m, time: 36.31\n",
      "best_acc: 0.5561\n",
      "epoch: 2, loss: 175.95414, loss1: 1.02178, loss2_3: 174.93236\n",
      "\ttrain_acc: 0.5487, test_acc: \u001b[31m0.54865\u001b[0m, time: 36.38\n",
      "epoch: 3, loss: 174.22681, loss1: 1.02853, loss2_3: 173.19829\n",
      "\ttrain_acc: 0.5860, test_acc: \u001b[31m0.58595\u001b[0m, time: 36.33\n",
      "best_acc: 0.58595\n",
      "epoch: 4, loss: 162.11159, loss1: 1.02037, loss2_3: 161.09122\n",
      "\ttrain_acc: 0.6842, test_acc: \u001b[31m0.68805\u001b[0m, time: 36.34\n",
      "best_acc: 0.68805\n",
      "epoch: 5, loss: 138.78763, loss1: 0.98040, loss2_3: 137.80723\n",
      "\ttrain_acc: 0.7542, test_acc: \u001b[31m0.75475\u001b[0m, time: 36.39\n",
      "best_acc: 0.75475\n",
      "epoch: 1, loss: 181.11985, loss1: 2.38522, loss2_3: 178.73463\n",
      "\ttrain_acc: 0.5336, test_acc: \u001b[31m0.5331\u001b[0m, time: 36.31\n",
      "best_acc: 0.5331\n",
      "epoch: 2, loss: 176.11953, loss1: 1.02278, loss2_3: 175.09675\n",
      "\ttrain_acc: 0.5570, test_acc: \u001b[31m0.5606\u001b[0m, time: 36.32\n",
      "best_acc: 0.5606\n",
      "epoch: 3, loss: 159.19088, loss1: 1.01531, loss2_3: 158.17557\n",
      "\ttrain_acc: 0.7211, test_acc: \u001b[31m0.72255\u001b[0m, time: 36.32\n",
      "best_acc: 0.72255\n",
      "epoch: 4, loss: 138.65569, loss1: 0.98186, loss2_3: 137.67383\n",
      "\ttrain_acc: 0.7479, test_acc: \u001b[31m0.7491\u001b[0m, time: 36.35\n",
      "best_acc: 0.7491\n",
      "epoch: 5, loss: 133.79938, loss1: 0.95522, loss2_3: 132.84416\n",
      "\ttrain_acc: 0.7479, test_acc: \u001b[31m0.7467\u001b[0m, time: 36.36\n"
     ]
    }
   ],
   "source": [
    "for num_model in range(2):\n",
    "    net=newModel().to(device)\n",
    "    lr = 0.0001\n",
    "    \n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr,weight_decay=5e-4)\n",
    "    criterion = ContrastiveLoss()\n",
    "    criterion_model = nn.CrossEntropyLoss(reduction='sum')\n",
    "    \n",
    "    best_acc=0\n",
    "    EPOCH=5\n",
    "    for epoch in range(EPOCH):\n",
    "        loss_ls=[]\n",
    "        loss1_ls=[]\n",
    "        loss2_3_ls=[]\n",
    "        t0=time.time()\n",
    "        net.train()\n",
    "        for seq1,seq2,label,label1,label2 in train_iter_cont:\n",
    "                output1=net(seq1)\n",
    "                output2=net(seq2)\n",
    "                output3=net.trainModel(seq1)\n",
    "                output4=net.trainModel(seq2)\n",
    "                \n",
    "                loss1=criterion(output1, output2, label)\n",
    "                loss2=criterion_model(output3,label1)\n",
    "                loss3=criterion_model(output4,label2)\n",
    "                loss=loss1+loss2+loss3\n",
    "    #             print(loss)\n",
    "                optimizer.zero_grad() \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                loss_ls.append(loss.item())\n",
    "                loss1_ls.append(loss1.item())\n",
    "                loss2_3_ls.append((loss2+loss3).item())\n",
    "\n",
    "\n",
    "        net.eval() \n",
    "        with torch.no_grad(): \n",
    "            train_acc=evaluate_accuracy(train_iter,net)\n",
    "            test_acc=evaluate_accuracy(test_iter,net)\n",
    "            \n",
    "        results=f\"epoch: {epoch+1}, loss: {np.mean(loss_ls):.5f}, loss1: {np.mean(loss1_ls):.5f}, loss2_3: {np.mean(loss2_3_ls):.5f}\\n\"\n",
    "        results+=f'\\ttrain_acc: {train_acc:.4f}, test_acc: {colored(test_acc,\"red\")}, time: {time.time()-t0:.2f}'\n",
    "        print(results)\n",
    "        to_log(results)\n",
    "        if test_acc>best_acc:\n",
    "            best_acc=test_acc\n",
    "            torch.save({\"best_acc\":best_acc,\"model\":net.state_dict()},f'compareModel/2021ACS_PepFormer/Model/{num_model}.pl')\n",
    "            print(f\"best_acc: {best_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Make Data X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T21:29:27.310382Z",
     "start_time": "2021-08-30T21:29:26.383533Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T21:29:32.561604Z",
     "start_time": "2021-08-30T21:29:30.743803Z"
    }
   },
   "outputs": [],
   "source": [
    "df_detect_peptide_train = pd.read_csv('data/df_detect_peptide_train.csv')\n",
    "df_detect_peptide_test = pd.read_csv('data/df_detect_peptide_test.csv')\n",
    "\n",
    "tra, val = train_test_split(df_detect_peptide_train[['PEP', 'ID']], test_size=0.2, random_state=7)\n",
    "tra.to_csv('compareModel/2021ACS_PepFormer/detect_peptide_train.csv', header=False, index=False)\n",
    "val.to_csv('compareModel/2021ACS_PepFormer/detect_peptide_val.csv', header=False, index=False)\n",
    "df_detect_peptide_test[['PEP', 'ID']].to_csv('compareModel/2021ACS_PepFormer/detect_peptide_test.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T21:30:56.460546Z",
     "start_time": "2021-08-30T21:30:49.144762Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length > 30: 0\n",
      "torch.Size([215352, 30]) torch.Size([215352])\n",
      "length > 30: 0\n",
      "torch.Size([53838, 30]) torch.Size([53838])\n",
      "length > 30: 0\n",
      "torch.Size([67298, 30]) torch.Size([67298])\n"
     ]
    }
   ],
   "source": [
    "train_data,train_label=genData(\"compareModel/2021ACS_PepFormer/detect_peptide_train.csv\",30)\n",
    "print(train_data.shape, train_label.shape)\n",
    "train_dataset = Data.TensorDataset(train_data, train_label)\n",
    "\n",
    "val_data,val_label=genData(\"compareModel/2021ACS_PepFormer/detect_peptide_val.csv\",30)\n",
    "print(val_data.shape, val_label.shape)\n",
    "val_dataset = Data.TensorDataset(val_data, val_label)\n",
    "\n",
    "test_data,test_label=genData(\"compareModel/2021ACS_PepFormer/detect_peptide_test.csv\",30)\n",
    "print(test_data.shape, test_label.shape)\n",
    "test_dataset = Data.TensorDataset(test_data, test_label)\n",
    "\n",
    "batch_size=256\n",
    "train_iter = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "train_iter_cont = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \n",
    "                                                                  shuffle=True, collate_fn=collate)\n",
    "val_iter = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_iter = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T21:38:23.235474Z",
     "start_time": "2021-08-30T21:38:23.217948Z"
    }
   },
   "outputs": [],
   "source": [
    "class newModel(nn.Module):\n",
    "    def __init__(self, vocab_size=24):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = 25\n",
    "        self.batch_size = 256\n",
    "        self.emb_dim = 512\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, self.emb_dim, padding_idx=0)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=1)\n",
    "        \n",
    "        self.gru = nn.GRU(self.emb_dim, self.hidden_dim, num_layers=2, \n",
    "                               bidirectional=True, dropout=0.2)\n",
    "        \n",
    "        \n",
    "        self.block1=nn.Sequential(nn.Linear(1600,1024),\n",
    "                                            nn.BatchNorm1d(1024),\n",
    "                                            nn.LeakyReLU(),\n",
    "                                            nn.Linear(1024,256),\n",
    "                                 )\n",
    "\n",
    "        self.block2=nn.Sequential(\n",
    "                                               nn.BatchNorm1d(256),\n",
    "                                               nn.LeakyReLU(),\n",
    "                                               nn.Linear(256,128),\n",
    "                                               nn.BatchNorm1d(128),\n",
    "                                               nn.LeakyReLU(),\n",
    "                                               nn.Linear(128,64),\n",
    "                                               nn.BatchNorm1d(64),\n",
    "                                               nn.LeakyReLU(),\n",
    "                                               nn.Linear(64,2)\n",
    "                                            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x=self.embedding(x)\n",
    "        output=self.transformer_encoder(x).permute(1, 0, 2)\n",
    "        output,hn=self.gru(output)\n",
    "        output=output.permute(1,0,2)\n",
    "        hn=hn.permute(1,0,2)\n",
    "        output=output.reshape(output.shape[0],-1)\n",
    "        hn=hn.reshape(output.shape[0],-1)\n",
    "        output=torch.cat([output,hn],1)\n",
    "        return self.block1(output)\n",
    "\n",
    "    def trainModel(self, x):\n",
    "        with torch.no_grad():\n",
    "            output=self.forward(x)\n",
    "        return self.block2(output)\n",
    "\n",
    "\n",
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        # euclidean_distance: [128]\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +     # calmp夹断用法\n",
    "                                      (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))     \n",
    "        \n",
    "        return loss_contrastive\n",
    "    \n",
    "    \n",
    "def collate(batch):\n",
    "    seq1_ls=[]\n",
    "    seq2_ls=[]\n",
    "    label1_ls=[]\n",
    "    label2_ls=[]\n",
    "    label_ls=[]\n",
    "    batch_size=len(batch)\n",
    "    for i in range(int(batch_size/2)):\n",
    "        seq1,label1=batch[i][0],batch[i][1]\n",
    "        seq2,label2=batch[i+int(batch_size/2)][0],batch[i+int(batch_size/2)][1]\n",
    "        label1_ls.append(label1.unsqueeze(0))\n",
    "        label2_ls.append(label2.unsqueeze(0))\n",
    "        label=(label1^label2)\n",
    "        seq1_ls.append(seq1.unsqueeze(0))\n",
    "        seq2_ls.append(seq2.unsqueeze(0))\n",
    "        label_ls.append(label.unsqueeze(0))\n",
    "    seq1=torch.cat(seq1_ls).to(device)\n",
    "    seq2=torch.cat(seq2_ls).to(device)\n",
    "    label=torch.cat(label_ls).to(device)\n",
    "    label1=torch.cat(label1_ls).to(device)\n",
    "    label2=torch.cat(label2_ls).to(device)\n",
    "    return seq1,seq2,label,label1,label2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T01:27:34.173866Z",
     "start_time": "2021-08-30T21:38:23.696487Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 157.79544, loss1: 1.45590, loss2_3: 156.33954\n",
      "\ttrain_acc: 0.7599, test_acc: \u001b[31m0.7590549426055946\u001b[0m, time: 64.05\n",
      "best_acc: 0.7590549426055946\n",
      "epoch: 2, loss: 124.56889, loss1: 0.92612, loss2_3: 123.64277\n",
      "\ttrain_acc: 0.7897, test_acc: \u001b[31m0.7894795497603923\u001b[0m, time: 63.93\n",
      "best_acc: 0.7894795497603923\n",
      "epoch: 3, loss: 118.81992, loss1: 0.90422, loss2_3: 117.91570\n",
      "\ttrain_acc: 0.7891, test_acc: \u001b[31m0.7878450165310747\u001b[0m, time: 65.07\n",
      "epoch: 4, loss: 115.51865, loss1: 0.88433, loss2_3: 114.63432\n",
      "\ttrain_acc: 0.7891, test_acc: \u001b[31m0.7902225194100821\u001b[0m, time: 64.20\n",
      "best_acc: 0.7902225194100821\n",
      "epoch: 5, loss: 114.16350, loss1: 0.87263, loss2_3: 113.29087\n",
      "\ttrain_acc: 0.7854, test_acc: \u001b[31m0.7858018499944277\u001b[0m, time: 63.56\n",
      "epoch: 6, loss: 111.80736, loss1: 0.86087, loss2_3: 110.94649\n",
      "\ttrain_acc: 0.8099, test_acc: \u001b[31m0.8092611166833835\u001b[0m, time: 64.21\n",
      "best_acc: 0.8092611166833835\n",
      "epoch: 7, loss: 109.85247, loss1: 0.84179, loss2_3: 109.01068\n",
      "\ttrain_acc: 0.8106, test_acc: \u001b[31m0.8107099075002786\u001b[0m, time: 63.84\n",
      "best_acc: 0.8107099075002786\n",
      "epoch: 8, loss: 109.44311, loss1: 0.83591, loss2_3: 108.60720\n",
      "\ttrain_acc: 0.8121, test_acc: \u001b[31m0.8101155317805268\u001b[0m, time: 63.35\n",
      "epoch: 9, loss: 108.64083, loss1: 0.83127, loss2_3: 107.80956\n",
      "\ttrain_acc: 0.8157, test_acc: \u001b[31m0.8125859058657454\u001b[0m, time: 63.35\n",
      "best_acc: 0.8125859058657454\n",
      "epoch: 10, loss: 107.68807, loss1: 0.82630, loss2_3: 106.86178\n",
      "\ttrain_acc: 0.8129, test_acc: \u001b[31m0.8102269772279802\u001b[0m, time: 64.41\n",
      "epoch: 11, loss: 106.59012, loss1: 0.81937, loss2_3: 105.77074\n",
      "\ttrain_acc: 0.8102, test_acc: \u001b[31m0.8079423455551841\u001b[0m, time: 63.33\n",
      "epoch: 12, loss: 106.29528, loss1: 0.81450, loss2_3: 105.48077\n",
      "\ttrain_acc: 0.8202, test_acc: \u001b[31m0.8178795646197853\u001b[0m, time: 63.95\n",
      "best_acc: 0.8178795646197853\n",
      "epoch: 13, loss: 105.69258, loss1: 0.81075, loss2_3: 104.88183\n",
      "\ttrain_acc: 0.8216, test_acc: \u001b[31m0.8202199190163082\u001b[0m, time: 63.72\n",
      "best_acc: 0.8202199190163082\n",
      "epoch: 14, loss: 105.56074, loss1: 0.80635, loss2_3: 104.75439\n",
      "\ttrain_acc: 0.8208, test_acc: \u001b[31m0.8203313644637616\u001b[0m, time: 63.49\n",
      "best_acc: 0.8203313644637616\n",
      "epoch: 15, loss: 104.82173, loss1: 0.80000, loss2_3: 104.02173\n",
      "\ttrain_acc: 0.8188, test_acc: \u001b[31m0.816876555592704\u001b[0m, time: 63.69\n",
      "epoch: 16, loss: 104.74118, loss1: 0.79869, loss2_3: 103.94249\n",
      "\ttrain_acc: 0.8210, test_acc: \u001b[31m0.8181210297559345\u001b[0m, time: 64.41\n",
      "epoch: 17, loss: 104.40565, loss1: 0.79728, loss2_3: 103.60837\n",
      "\ttrain_acc: 0.8226, test_acc: \u001b[31m0.8205542553586685\u001b[0m, time: 64.14\n",
      "best_acc: 0.8205542553586685\n",
      "epoch: 18, loss: 104.03009, loss1: 0.79043, loss2_3: 103.23965\n",
      "\ttrain_acc: 0.8220, test_acc: \u001b[31m0.8184925145807794\u001b[0m, time: 65.44\n",
      "epoch: 19, loss: 103.98417, loss1: 0.79414, loss2_3: 103.19003\n",
      "\ttrain_acc: 0.8208, test_acc: \u001b[31m0.8181396039971767\u001b[0m, time: 64.01\n",
      "epoch: 20, loss: 103.31940, loss1: 0.78592, loss2_3: 102.53348\n",
      "\ttrain_acc: 0.8170, test_acc: \u001b[31m0.8147219436086036\u001b[0m, time: 63.62\n",
      "epoch: 21, loss: 103.35687, loss1: 0.78521, loss2_3: 102.57166\n",
      "\ttrain_acc: 0.8247, test_acc: \u001b[31m0.8223373825179241\u001b[0m, time: 64.06\n",
      "best_acc: 0.8223373825179241\n",
      "epoch: 22, loss: 102.95613, loss1: 0.78336, loss2_3: 102.17277\n",
      "\ttrain_acc: 0.8254, test_acc: \u001b[31m0.8229874809614027\u001b[0m, time: 64.33\n",
      "best_acc: 0.8229874809614027\n",
      "epoch: 23, loss: 102.85393, loss1: 0.78581, loss2_3: 102.06812\n",
      "\ttrain_acc: 0.8255, test_acc: \u001b[31m0.8230432036851295\u001b[0m, time: 63.83\n",
      "best_acc: 0.8230432036851295\n",
      "epoch: 24, loss: 102.48990, loss1: 0.78079, loss2_3: 101.70910\n",
      "\ttrain_acc: 0.8251, test_acc: \u001b[31m0.8230246294438872\u001b[0m, time: 63.94\n",
      "epoch: 25, loss: 102.27278, loss1: 0.77890, loss2_3: 101.49389\n",
      "\ttrain_acc: 0.8257, test_acc: \u001b[31m0.8235075597161856\u001b[0m, time: 63.53\n",
      "best_acc: 0.8235075597161856\n",
      "epoch: 26, loss: 102.25274, loss1: 0.77592, loss2_3: 101.47682\n",
      "\ttrain_acc: 0.8278, test_acc: \u001b[31m0.8255321520115904\u001b[0m, time: 64.22\n",
      "best_acc: 0.8255321520115904\n",
      "epoch: 27, loss: 101.82918, loss1: 0.77328, loss2_3: 101.05590\n",
      "\ttrain_acc: 0.8252, test_acc: \u001b[31m0.8220587688992904\u001b[0m, time: 64.68\n",
      "epoch: 28, loss: 101.76433, loss1: 0.77129, loss2_3: 100.99304\n",
      "\ttrain_acc: 0.8262, test_acc: \u001b[31m0.8243805490545711\u001b[0m, time: 63.84\n",
      "epoch: 29, loss: 101.33173, loss1: 0.76861, loss2_3: 100.56312\n",
      "\ttrain_acc: 0.8241, test_acc: \u001b[31m0.8215386901445076\u001b[0m, time: 63.98\n",
      "epoch: 30, loss: 101.14114, loss1: 0.77040, loss2_3: 100.37074\n",
      "\ttrain_acc: 0.8302, test_acc: \u001b[31m0.827482447342026\u001b[0m, time: 65.00\n",
      "best_acc: 0.827482447342026\n",
      "epoch: 31, loss: 100.89879, loss1: 0.76650, loss2_3: 100.13229\n",
      "\ttrain_acc: 0.8274, test_acc: \u001b[31m0.8243991232958133\u001b[0m, time: 63.89\n",
      "epoch: 32, loss: 100.83164, loss1: 0.76545, loss2_3: 100.06619\n",
      "\ttrain_acc: 0.8300, test_acc: \u001b[31m0.8276867639956907\u001b[0m, time: 63.37\n",
      "best_acc: 0.8276867639956907\n",
      "epoch: 33, loss: 100.61932, loss1: 0.76453, loss2_3: 99.85479\n",
      "\ttrain_acc: 0.8285, test_acc: \u001b[31m0.8270552397934544\u001b[0m, time: 63.67\n",
      "epoch: 34, loss: 100.45152, loss1: 0.76553, loss2_3: 99.68599\n",
      "\ttrain_acc: 0.8289, test_acc: \u001b[31m0.8270180913109699\u001b[0m, time: 64.22\n",
      "epoch: 35, loss: 100.33909, loss1: 0.76277, loss2_3: 99.57633\n",
      "\ttrain_acc: 0.8255, test_acc: \u001b[31m0.8233403915450054\u001b[0m, time: 63.94\n",
      "epoch: 36, loss: 100.12471, loss1: 0.75728, loss2_3: 99.36743\n",
      "\ttrain_acc: 0.8320, test_acc: \u001b[31m0.8287454957464988\u001b[0m, time: 63.68\n",
      "best_acc: 0.8287454957464988\n",
      "epoch: 37, loss: 99.89057, loss1: 0.75921, loss2_3: 99.13136\n",
      "\ttrain_acc: 0.8300, test_acc: \u001b[31m0.8284483078866228\u001b[0m, time: 63.25\n",
      "epoch: 38, loss: 99.92323, loss1: 0.76034, loss2_3: 99.16289\n",
      "\ttrain_acc: 0.8311, test_acc: \u001b[31m0.8262751216612801\u001b[0m, time: 63.34\n",
      "epoch: 39, loss: 99.73129, loss1: 0.75574, loss2_3: 98.97555\n",
      "\ttrain_acc: 0.8313, test_acc: \u001b[31m0.8297113562910955\u001b[0m, time: 64.21\n",
      "best_acc: 0.8297113562910955\n",
      "epoch: 40, loss: 99.57385, loss1: 0.75391, loss2_3: 98.81994\n",
      "\ttrain_acc: 0.8328, test_acc: \u001b[31m0.829674207808611\u001b[0m, time: 63.95\n",
      "epoch: 41, loss: 99.30772, loss1: 0.75407, loss2_3: 98.55365\n",
      "\ttrain_acc: 0.8313, test_acc: \u001b[31m0.82974850477358\u001b[0m, time: 63.83\n",
      "best_acc: 0.82974850477358\n",
      "epoch: 42, loss: 99.18516, loss1: 0.75338, loss2_3: 98.43178\n",
      "\ttrain_acc: 0.8320, test_acc: \u001b[31m0.8296184850848842\u001b[0m, time: 63.60\n",
      "epoch: 43, loss: 99.16088, loss1: 0.75075, loss2_3: 98.41012\n",
      "\ttrain_acc: 0.8321, test_acc: \u001b[31m0.8292841487425239\u001b[0m, time: 63.72\n",
      "epoch: 44, loss: 99.08377, loss1: 0.75126, loss2_3: 98.33251\n",
      "\ttrain_acc: 0.8323, test_acc: \u001b[31m0.8286526245402875\u001b[0m, time: 63.67\n",
      "epoch: 45, loss: 98.94970, loss1: 0.75116, loss2_3: 98.19854\n",
      "\ttrain_acc: 0.8316, test_acc: \u001b[31m0.8295256138786731\u001b[0m, time: 52.80\n",
      "epoch: 46, loss: 98.95309, loss1: 0.75186, loss2_3: 98.20124\n",
      "\ttrain_acc: 0.8335, test_acc: \u001b[31m0.8311229986255061\u001b[0m, time: 52.51\n",
      "best_acc: 0.8311229986255061\n",
      "epoch: 47, loss: 98.79001, loss1: 0.74996, loss2_3: 98.04005\n",
      "\ttrain_acc: 0.8325, test_acc: \u001b[31m0.8286154760578031\u001b[0m, time: 52.56\n",
      "epoch: 48, loss: 98.72106, loss1: 0.74574, loss2_3: 97.97532\n",
      "\ttrain_acc: 0.8344, test_acc: \u001b[31m0.8302685835283629\u001b[0m, time: 53.32\n",
      "epoch: 49, loss: 98.63181, loss1: 0.74941, loss2_3: 97.88241\n",
      "\ttrain_acc: 0.8344, test_acc: \u001b[31m0.8304357516995431\u001b[0m, time: 53.52\n",
      "epoch: 50, loss: 98.48596, loss1: 0.74820, loss2_3: 97.73777\n",
      "\ttrain_acc: 0.8349, test_acc: \u001b[31m0.8319959879638916\u001b[0m, time: 54.36\n",
      "best_acc: 0.8319959879638916\n",
      "epoch: 51, loss: 98.53916, loss1: 0.74496, loss2_3: 97.79420\n",
      "\ttrain_acc: 0.8336, test_acc: \u001b[31m0.8296370593261265\u001b[0m, time: 52.53\n",
      "epoch: 52, loss: 98.40668, loss1: 0.74694, loss2_3: 97.65974\n",
      "\ttrain_acc: 0.8346, test_acc: \u001b[31m0.8324046212712211\u001b[0m, time: 52.56\n",
      "best_acc: 0.8324046212712211\n",
      "epoch: 53, loss: 98.37763, loss1: 0.74560, loss2_3: 97.63203\n",
      "\ttrain_acc: 0.8339, test_acc: \u001b[31m0.8301014153571826\u001b[0m, time: 52.52\n",
      "epoch: 54, loss: 98.38744, loss1: 0.74528, loss2_3: 97.64216\n",
      "\ttrain_acc: 0.8351, test_acc: \u001b[31m0.8305657713882388\u001b[0m, time: 52.43\n",
      "epoch: 55, loss: 98.11198, loss1: 0.74422, loss2_3: 97.36776\n",
      "\ttrain_acc: 0.8353, test_acc: \u001b[31m0.8319774137226494\u001b[0m, time: 52.49\n",
      "epoch: 56, loss: 98.02468, loss1: 0.74504, loss2_3: 97.27963\n",
      "\ttrain_acc: 0.8351, test_acc: \u001b[31m0.8318845425164382\u001b[0m, time: 52.43\n",
      "epoch: 57, loss: 97.92789, loss1: 0.74289, loss2_3: 97.18501\n",
      "\ttrain_acc: 0.8322, test_acc: \u001b[31m0.8304543259407853\u001b[0m, time: 52.43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 58, loss: 97.93607, loss1: 0.74302, loss2_3: 97.19305\n",
      "\ttrain_acc: 0.8348, test_acc: \u001b[31m0.8316059288978045\u001b[0m, time: 52.46\n",
      "epoch: 59, loss: 97.90876, loss1: 0.74537, loss2_3: 97.16339\n",
      "\ttrain_acc: 0.8345, test_acc: \u001b[31m0.8321445818938297\u001b[0m, time: 54.71\n",
      "epoch: 60, loss: 97.88411, loss1: 0.74297, loss2_3: 97.14114\n",
      "\ttrain_acc: 0.8354, test_acc: \u001b[31m0.8314201864853821\u001b[0m, time: 57.76\n",
      "epoch: 61, loss: 97.77314, loss1: 0.74127, loss2_3: 97.03187\n",
      "\ttrain_acc: 0.8361, test_acc: \u001b[31m0.8315130576915933\u001b[0m, time: 52.58\n",
      "epoch: 62, loss: 97.64517, loss1: 0.74038, loss2_3: 96.90479\n",
      "\ttrain_acc: 0.8362, test_acc: \u001b[31m0.8320888591701029\u001b[0m, time: 52.42\n",
      "epoch: 63, loss: 97.55289, loss1: 0.74046, loss2_3: 96.81243\n",
      "\ttrain_acc: 0.8361, test_acc: \u001b[31m0.8321445818938297\u001b[0m, time: 52.51\n",
      "epoch: 64, loss: 97.61019, loss1: 0.73968, loss2_3: 96.87050\n",
      "\ttrain_acc: 0.8347, test_acc: \u001b[31m0.8298228017385489\u001b[0m, time: 52.42\n",
      "epoch: 65, loss: 97.58285, loss1: 0.73981, loss2_3: 96.84304\n",
      "\ttrain_acc: 0.8356, test_acc: \u001b[31m0.8322003046175563\u001b[0m, time: 52.46\n",
      "epoch: 66, loss: 97.39326, loss1: 0.74050, loss2_3: 96.65276\n",
      "\ttrain_acc: 0.8361, test_acc: \u001b[31m0.8316616516215313\u001b[0m, time: 52.40\n",
      "epoch: 67, loss: 97.35506, loss1: 0.74038, loss2_3: 96.61468\n",
      "\ttrain_acc: 0.8354, test_acc: \u001b[31m0.8307143653181768\u001b[0m, time: 52.46\n",
      "epoch: 68, loss: 97.32439, loss1: 0.73874, loss2_3: 96.58566\n",
      "\ttrain_acc: 0.8364, test_acc: \u001b[31m0.8325160667186745\u001b[0m, time: 52.40\n",
      "best_acc: 0.8325160667186745\n",
      "epoch: 69, loss: 97.32128, loss1: 0.74081, loss2_3: 96.58046\n",
      "\ttrain_acc: 0.8370, test_acc: \u001b[31m0.8329247000260039\u001b[0m, time: 52.39\n",
      "best_acc: 0.8329247000260039\n",
      "epoch: 70, loss: 97.26047, loss1: 0.73722, loss2_3: 96.52324\n",
      "\ttrain_acc: 0.8369, test_acc: \u001b[31m0.8332404621271221\u001b[0m, time: 52.41\n",
      "best_acc: 0.8332404621271221\n",
      "epoch: 71, loss: 97.00196, loss1: 0.73794, loss2_3: 96.26402\n",
      "\ttrain_acc: 0.8374, test_acc: \u001b[31m0.8333704818158179\u001b[0m, time: 52.46\n",
      "best_acc: 0.8333704818158179\n",
      "epoch: 72, loss: 97.09941, loss1: 0.73551, loss2_3: 96.36390\n",
      "\ttrain_acc: 0.8373, test_acc: \u001b[31m0.8319588394814071\u001b[0m, time: 52.42\n",
      "epoch: 73, loss: 96.99932, loss1: 0.73519, loss2_3: 96.26413\n",
      "\ttrain_acc: 0.8367, test_acc: \u001b[31m0.8308443850068725\u001b[0m, time: 52.47\n",
      "epoch: 74, loss: 96.99363, loss1: 0.73887, loss2_3: 96.25477\n",
      "\ttrain_acc: 0.8380, test_acc: \u001b[31m0.8321260076525874\u001b[0m, time: 52.50\n",
      "epoch: 75, loss: 96.86909, loss1: 0.73622, loss2_3: 96.13286\n",
      "\ttrain_acc: 0.8374, test_acc: \u001b[31m0.8314759092091089\u001b[0m, time: 52.49\n",
      "epoch: 76, loss: 96.78281, loss1: 0.73841, loss2_3: 96.04440\n",
      "\ttrain_acc: 0.8383, test_acc: \u001b[31m0.8328318288197927\u001b[0m, time: 52.47\n",
      "epoch: 77, loss: 96.85661, loss1: 0.73925, loss2_3: 96.11735\n",
      "\ttrain_acc: 0.8386, test_acc: \u001b[31m0.8326460864073703\u001b[0m, time: 52.46\n",
      "epoch: 78, loss: 96.73911, loss1: 0.73664, loss2_3: 96.00247\n",
      "\ttrain_acc: 0.8382, test_acc: \u001b[31m0.8334262045395445\u001b[0m, time: 52.45\n",
      "best_acc: 0.8334262045395445\n",
      "epoch: 79, loss: 96.75872, loss1: 0.73514, loss2_3: 96.02358\n",
      "\ttrain_acc: 0.8379, test_acc: \u001b[31m0.8320888591701029\u001b[0m, time: 52.51\n",
      "epoch: 80, loss: 96.72022, loss1: 0.73443, loss2_3: 95.98579\n",
      "\ttrain_acc: 0.8381, test_acc: \u001b[31m0.8328875515435195\u001b[0m, time: 52.46\n",
      "epoch: 81, loss: 96.75742, loss1: 0.73646, loss2_3: 96.02096\n",
      "\ttrain_acc: 0.8385, test_acc: \u001b[31m0.8325903636836435\u001b[0m, time: 52.45\n",
      "epoch: 82, loss: 96.68767, loss1: 0.73763, loss2_3: 95.95003\n",
      "\ttrain_acc: 0.8384, test_acc: \u001b[31m0.8329247000260039\u001b[0m, time: 52.51\n",
      "epoch: 83, loss: 96.53015, loss1: 0.73509, loss2_3: 95.79506\n",
      "\ttrain_acc: 0.8379, test_acc: \u001b[31m0.8319031167576805\u001b[0m, time: 55.91\n",
      "epoch: 84, loss: 96.70912, loss1: 0.73314, loss2_3: 95.97598\n",
      "\ttrain_acc: 0.8391, test_acc: \u001b[31m0.8325346409599168\u001b[0m, time: 55.95\n",
      "epoch: 85, loss: 96.25515, loss1: 0.73506, loss2_3: 95.52009\n",
      "\ttrain_acc: 0.8380, test_acc: \u001b[31m0.8309186819718415\u001b[0m, time: 52.54\n",
      "epoch: 86, loss: 96.36998, loss1: 0.73472, loss2_3: 95.63526\n",
      "\ttrain_acc: 0.8380, test_acc: \u001b[31m0.8333519075745756\u001b[0m, time: 52.50\n",
      "epoch: 87, loss: 96.28254, loss1: 0.73235, loss2_3: 95.55019\n",
      "\ttrain_acc: 0.8389, test_acc: \u001b[31m0.8339462832943274\u001b[0m, time: 52.59\n",
      "best_acc: 0.8339462832943274\n",
      "epoch: 88, loss: 96.38784, loss1: 0.73600, loss2_3: 95.65184\n",
      "\ttrain_acc: 0.8370, test_acc: \u001b[31m0.8325903636836435\u001b[0m, time: 52.53\n",
      "epoch: 89, loss: 96.25942, loss1: 0.73455, loss2_3: 95.52487\n",
      "\ttrain_acc: 0.8392, test_acc: \u001b[31m0.8339462832943274\u001b[0m, time: 52.52\n",
      "epoch: 90, loss: 96.19857, loss1: 0.73380, loss2_3: 95.46477\n",
      "\ttrain_acc: 0.8390, test_acc: \u001b[31m0.8331475909209108\u001b[0m, time: 52.74\n",
      "epoch: 91, loss: 96.24048, loss1: 0.73324, loss2_3: 95.50725\n",
      "\ttrain_acc: 0.8369, test_acc: \u001b[31m0.8305100486645121\u001b[0m, time: 52.61\n",
      "epoch: 92, loss: 96.05423, loss1: 0.73113, loss2_3: 95.32310\n",
      "\ttrain_acc: 0.8380, test_acc: \u001b[31m0.8328875515435195\u001b[0m, time: 52.55\n",
      "epoch: 93, loss: 96.14985, loss1: 0.73317, loss2_3: 95.41668\n",
      "\ttrain_acc: 0.8402, test_acc: \u001b[31m0.832850403061035\u001b[0m, time: 52.56\n",
      "epoch: 94, loss: 96.18420, loss1: 0.73408, loss2_3: 95.45012\n",
      "\ttrain_acc: 0.8371, test_acc: \u001b[31m0.8316245031390468\u001b[0m, time: 52.52\n",
      "epoch: 95, loss: 96.06983, loss1: 0.73140, loss2_3: 95.33844\n",
      "\ttrain_acc: 0.8397, test_acc: \u001b[31m0.8334076302983023\u001b[0m, time: 52.59\n",
      "epoch: 96, loss: 96.09753, loss1: 0.73265, loss2_3: 95.36487\n",
      "\ttrain_acc: 0.8389, test_acc: \u001b[31m0.8327389576135815\u001b[0m, time: 52.53\n",
      "epoch: 97, loss: 96.02184, loss1: 0.73480, loss2_3: 95.28704\n",
      "\ttrain_acc: 0.8385, test_acc: \u001b[31m0.8322374531000408\u001b[0m, time: 52.53\n",
      "epoch: 98, loss: 96.04901, loss1: 0.72863, loss2_3: 95.32038\n",
      "\ttrain_acc: 0.8391, test_acc: \u001b[31m0.8332033136446376\u001b[0m, time: 53.09\n",
      "epoch: 99, loss: 96.08040, loss1: 0.73323, loss2_3: 95.34717\n",
      "\ttrain_acc: 0.8391, test_acc: \u001b[31m0.832701809131097\u001b[0m, time: 57.32\n",
      "epoch: 100, loss: 95.86203, loss1: 0.73122, loss2_3: 95.13081\n",
      "\ttrain_acc: 0.8406, test_acc: \u001b[31m0.8337233923994205\u001b[0m, time: 57.77\n",
      "epoch: 101, loss: 95.90877, loss1: 0.73228, loss2_3: 95.17648\n",
      "\ttrain_acc: 0.8405, test_acc: \u001b[31m0.8342434711542034\u001b[0m, time: 57.81\n",
      "best_acc: 0.8342434711542034\n",
      "epoch: 102, loss: 95.78635, loss1: 0.73068, loss2_3: 95.05568\n",
      "\ttrain_acc: 0.8402, test_acc: \u001b[31m0.8324046212712211\u001b[0m, time: 57.73\n",
      "epoch: 103, loss: 95.91354, loss1: 0.72974, loss2_3: 95.18380\n",
      "\ttrain_acc: 0.8396, test_acc: \u001b[31m0.8307515138006613\u001b[0m, time: 57.77\n",
      "epoch: 104, loss: 95.93679, loss1: 0.73209, loss2_3: 95.20469\n",
      "\ttrain_acc: 0.8406, test_acc: \u001b[31m0.8328689773022772\u001b[0m, time: 59.33\n",
      "epoch: 105, loss: 95.84766, loss1: 0.72887, loss2_3: 95.11879\n",
      "\ttrain_acc: 0.8401, test_acc: \u001b[31m0.8331290166796687\u001b[0m, time: 57.75\n",
      "epoch: 106, loss: 95.62153, loss1: 0.73064, loss2_3: 94.89089\n",
      "\ttrain_acc: 0.8392, test_acc: \u001b[31m0.8321445818938297\u001b[0m, time: 57.80\n",
      "epoch: 107, loss: 95.71000, loss1: 0.73074, loss2_3: 94.97926\n",
      "\ttrain_acc: 0.8387, test_acc: \u001b[31m0.8311972955904752\u001b[0m, time: 57.76\n",
      "epoch: 108, loss: 95.61592, loss1: 0.73237, loss2_3: 94.88355\n",
      "\ttrain_acc: 0.8394, test_acc: \u001b[31m0.8319774137226494\u001b[0m, time: 53.67\n",
      "epoch: 109, loss: 95.50032, loss1: 0.72998, loss2_3: 94.77034\n",
      "\ttrain_acc: 0.8413, test_acc: \u001b[31m0.8334819272632713\u001b[0m, time: 52.47\n",
      "epoch: 110, loss: 95.48872, loss1: 0.72780, loss2_3: 94.76092\n",
      "\ttrain_acc: 0.8412, test_acc: \u001b[31m0.833686243916936\u001b[0m, time: 52.53\n",
      "epoch: 111, loss: 95.64311, loss1: 0.72733, loss2_3: 94.91578\n",
      "\ttrain_acc: 0.8361, test_acc: \u001b[31m0.8285783275753186\u001b[0m, time: 52.40\n",
      "epoch: 112, loss: 95.43595, loss1: 0.72778, loss2_3: 94.70816\n",
      "\ttrain_acc: 0.8406, test_acc: \u001b[31m0.8336490954344515\u001b[0m, time: 52.47\n",
      "epoch: 113, loss: 95.65360, loss1: 0.72934, loss2_3: 94.92426\n",
      "\ttrain_acc: 0.8407, test_acc: \u001b[31m0.8334447787807868\u001b[0m, time: 52.53\n",
      "epoch: 114, loss: 95.50430, loss1: 0.72881, loss2_3: 94.77550\n",
      "\ttrain_acc: 0.8390, test_acc: \u001b[31m0.8308443850068725\u001b[0m, time: 52.52\n",
      "epoch: 115, loss: 95.56274, loss1: 0.72758, loss2_3: 94.83515\n",
      "\ttrain_acc: 0.8418, test_acc: \u001b[31m0.833834837846874\u001b[0m, time: 52.42\n",
      "epoch: 116, loss: 95.46252, loss1: 0.72914, loss2_3: 94.73339\n",
      "\ttrain_acc: 0.8423, test_acc: \u001b[31m0.8340205802592964\u001b[0m, time: 53.25\n",
      "epoch: 117, loss: 95.48120, loss1: 0.73222, loss2_3: 94.74897\n",
      "\ttrain_acc: 0.8422, test_acc: \u001b[31m0.8340205802592964\u001b[0m, time: 54.24\n",
      "epoch: 118, loss: 95.40636, loss1: 0.72932, loss2_3: 94.67704\n",
      "\ttrain_acc: 0.8405, test_acc: \u001b[31m0.8322746015825253\u001b[0m, time: 52.61\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 119, loss: 95.33489, loss1: 0.72742, loss2_3: 94.60747\n",
      "\ttrain_acc: 0.8414, test_acc: \u001b[31m0.833463353022029\u001b[0m, time: 52.35\n",
      "epoch: 120, loss: 95.33681, loss1: 0.73146, loss2_3: 94.60535\n",
      "\ttrain_acc: 0.8420, test_acc: \u001b[31m0.8334447787807868\u001b[0m, time: 52.36\n",
      "epoch: 121, loss: 95.09032, loss1: 0.72746, loss2_3: 94.36286\n",
      "\ttrain_acc: 0.8417, test_acc: \u001b[31m0.8340020060180542\u001b[0m, time: 52.36\n",
      "epoch: 122, loss: 95.23723, loss1: 0.72858, loss2_3: 94.50865\n",
      "\ttrain_acc: 0.8420, test_acc: \u001b[31m0.8338719863293584\u001b[0m, time: 52.43\n",
      "epoch: 123, loss: 95.05418, loss1: 0.72834, loss2_3: 94.32585\n",
      "\ttrain_acc: 0.8394, test_acc: \u001b[31m0.8307143653181768\u001b[0m, time: 52.34\n",
      "epoch: 124, loss: 95.18640, loss1: 0.72980, loss2_3: 94.45660\n",
      "\ttrain_acc: 0.8427, test_acc: \u001b[31m0.8339277090530852\u001b[0m, time: 52.39\n",
      "epoch: 125, loss: 95.14426, loss1: 0.72512, loss2_3: 94.41913\n",
      "\ttrain_acc: 0.8421, test_acc: \u001b[31m0.8337791151231472\u001b[0m, time: 52.68\n",
      "epoch: 126, loss: 95.11126, loss1: 0.72836, loss2_3: 94.38290\n",
      "\ttrain_acc: 0.8417, test_acc: \u001b[31m0.8322188788587986\u001b[0m, time: 54.37\n",
      "epoch: 127, loss: 95.04984, loss1: 0.72629, loss2_3: 94.32355\n",
      "\ttrain_acc: 0.8413, test_acc: \u001b[31m0.832701809131097\u001b[0m, time: 54.25\n",
      "epoch: 128, loss: 95.08507, loss1: 0.72945, loss2_3: 94.35562\n",
      "\ttrain_acc: 0.8426, test_acc: \u001b[31m0.83338905605706\u001b[0m, time: 54.26\n",
      "epoch: 129, loss: 94.94318, loss1: 0.72702, loss2_3: 94.21616\n",
      "\ttrain_acc: 0.8428, test_acc: \u001b[31m0.8339277090530852\u001b[0m, time: 54.25\n",
      "epoch: 130, loss: 94.97938, loss1: 0.72799, loss2_3: 94.25139\n",
      "\ttrain_acc: 0.8407, test_acc: \u001b[31m0.8312901667966863\u001b[0m, time: 54.35\n",
      "epoch: 131, loss: 94.89668, loss1: 0.72523, loss2_3: 94.17145\n",
      "\ttrain_acc: 0.8412, test_acc: \u001b[31m0.831643077380289\u001b[0m, time: 54.25\n",
      "epoch: 132, loss: 94.76573, loss1: 0.72548, loss2_3: 94.04025\n",
      "\ttrain_acc: 0.8406, test_acc: \u001b[31m0.8306400683532078\u001b[0m, time: 54.28\n",
      "epoch: 133, loss: 94.81209, loss1: 0.72743, loss2_3: 94.08466\n",
      "\ttrain_acc: 0.8431, test_acc: \u001b[31m0.8332776106096066\u001b[0m, time: 53.25\n",
      "epoch: 134, loss: 94.82476, loss1: 0.73003, loss2_3: 94.09473\n",
      "\ttrain_acc: 0.8368, test_acc: \u001b[31m0.8286154760578031\u001b[0m, time: 56.41\n",
      "epoch: 135, loss: 94.90576, loss1: 0.72674, loss2_3: 94.17902\n",
      "\ttrain_acc: 0.8433, test_acc: \u001b[31m0.8330547197146997\u001b[0m, time: 54.56\n",
      "epoch: 136, loss: 94.77706, loss1: 0.72646, loss2_3: 94.05060\n",
      "\ttrain_acc: 0.8418, test_acc: \u001b[31m0.8332961848508489\u001b[0m, time: 52.47\n",
      "epoch: 137, loss: 94.82577, loss1: 0.72436, loss2_3: 94.10141\n",
      "\ttrain_acc: 0.8410, test_acc: \u001b[31m0.831940265240165\u001b[0m, time: 52.44\n",
      "epoch: 138, loss: 94.80904, loss1: 0.72380, loss2_3: 94.08524\n",
      "\ttrain_acc: 0.8437, test_acc: \u001b[31m0.834373490842899\u001b[0m, time: 52.52\n",
      "best_acc: 0.834373490842899\n",
      "epoch: 139, loss: 94.75224, loss1: 0.72575, loss2_3: 94.02649\n",
      "\ttrain_acc: 0.8431, test_acc: \u001b[31m0.8353579256287381\u001b[0m, time: 52.47\n",
      "best_acc: 0.8353579256287381\n",
      "epoch: 140, loss: 94.69401, loss1: 0.72627, loss2_3: 93.96774\n",
      "\ttrain_acc: 0.8434, test_acc: \u001b[31m0.8340391545005387\u001b[0m, time: 53.08\n",
      "epoch: 141, loss: 94.67321, loss1: 0.72680, loss2_3: 93.94642\n",
      "\ttrain_acc: 0.8435, test_acc: \u001b[31m0.833463353022029\u001b[0m, time: 54.33\n",
      "epoch: 142, loss: 94.56387, loss1: 0.72655, loss2_3: 93.83732\n",
      "\ttrain_acc: 0.8433, test_acc: \u001b[31m0.8348192726327129\u001b[0m, time: 54.33\n",
      "epoch: 143, loss: 94.61810, loss1: 0.72550, loss2_3: 93.89260\n",
      "\ttrain_acc: 0.8434, test_acc: \u001b[31m0.83247891823619\u001b[0m, time: 54.30\n",
      "epoch: 144, loss: 94.68913, loss1: 0.72737, loss2_3: 93.96177\n",
      "\ttrain_acc: 0.8435, test_acc: \u001b[31m0.8333704818158179\u001b[0m, time: 54.31\n",
      "epoch: 145, loss: 94.52749, loss1: 0.72626, loss2_3: 93.80123\n",
      "\ttrain_acc: 0.8433, test_acc: \u001b[31m0.83338905605706\u001b[0m, time: 53.52\n",
      "epoch: 146, loss: 94.55204, loss1: 0.72409, loss2_3: 93.82795\n",
      "\ttrain_acc: 0.8446, test_acc: \u001b[31m0.8337419666406627\u001b[0m, time: 52.47\n",
      "epoch: 147, loss: 94.54032, loss1: 0.72320, loss2_3: 93.81712\n",
      "\ttrain_acc: 0.8430, test_acc: \u001b[31m0.832776106096066\u001b[0m, time: 52.36\n",
      "epoch: 148, loss: 94.55565, loss1: 0.72449, loss2_3: 93.83116\n",
      "\ttrain_acc: 0.8434, test_acc: \u001b[31m0.8342620453954456\u001b[0m, time: 52.42\n",
      "epoch: 149, loss: 94.39657, loss1: 0.72179, loss2_3: 93.67478\n",
      "\ttrain_acc: 0.8398, test_acc: \u001b[31m0.8315502061740778\u001b[0m, time: 52.43\n",
      "epoch: 150, loss: 94.33267, loss1: 0.72525, loss2_3: 93.60742\n",
      "\ttrain_acc: 0.8433, test_acc: \u001b[31m0.8332590363683644\u001b[0m, time: 52.39\n",
      "epoch: 151, loss: 94.39896, loss1: 0.72387, loss2_3: 93.67510\n",
      "\ttrain_acc: 0.8432, test_acc: \u001b[31m0.8343549166016568\u001b[0m, time: 52.40\n",
      "epoch: 152, loss: 94.38510, loss1: 0.72175, loss2_3: 93.66336\n",
      "\ttrain_acc: 0.8445, test_acc: \u001b[31m0.8344292135666258\u001b[0m, time: 52.41\n",
      "epoch: 153, loss: 94.42646, loss1: 0.72458, loss2_3: 93.70188\n",
      "\ttrain_acc: 0.8437, test_acc: \u001b[31m0.833686243916936\u001b[0m, time: 52.41\n",
      "epoch: 154, loss: 94.30263, loss1: 0.72541, loss2_3: 93.57721\n",
      "\ttrain_acc: 0.8433, test_acc: \u001b[31m0.8329618485084884\u001b[0m, time: 52.41\n",
      "epoch: 155, loss: 94.30482, loss1: 0.72341, loss2_3: 93.58141\n",
      "\ttrain_acc: 0.8412, test_acc: \u001b[31m0.8308629592481147\u001b[0m, time: 52.39\n",
      "epoch: 156, loss: 94.35579, loss1: 0.72302, loss2_3: 93.63277\n",
      "\ttrain_acc: 0.8400, test_acc: \u001b[31m0.8298970987035179\u001b[0m, time: 52.41\n",
      "epoch: 157, loss: 94.20632, loss1: 0.71859, loss2_3: 93.48773\n",
      "\ttrain_acc: 0.8444, test_acc: \u001b[31m0.8335747984694826\u001b[0m, time: 52.41\n",
      "epoch: 158, loss: 94.22488, loss1: 0.72421, loss2_3: 93.50068\n",
      "\ttrain_acc: 0.8439, test_acc: \u001b[31m0.8345778074965637\u001b[0m, time: 52.43\n",
      "epoch: 159, loss: 94.23811, loss1: 0.72281, loss2_3: 93.51530\n",
      "\ttrain_acc: 0.8428, test_acc: \u001b[31m0.831865968275196\u001b[0m, time: 52.38\n",
      "epoch: 160, loss: 94.13886, loss1: 0.72085, loss2_3: 93.41801\n",
      "\ttrain_acc: 0.8439, test_acc: \u001b[31m0.8335190757457558\u001b[0m, time: 52.44\n",
      "epoch: 161, loss: 94.12059, loss1: 0.72058, loss2_3: 93.40001\n",
      "\ttrain_acc: 0.8444, test_acc: \u001b[31m0.8338905605706007\u001b[0m, time: 52.38\n",
      "epoch: 162, loss: 94.27898, loss1: 0.72450, loss2_3: 93.55448\n",
      "\ttrain_acc: 0.8447, test_acc: \u001b[31m0.8324046212712211\u001b[0m, time: 52.43\n",
      "epoch: 163, loss: 94.06173, loss1: 0.72174, loss2_3: 93.34000\n",
      "\ttrain_acc: 0.8440, test_acc: \u001b[31m0.8324231955124634\u001b[0m, time: 52.38\n",
      "epoch: 164, loss: 94.02340, loss1: 0.72127, loss2_3: 93.30213\n",
      "\ttrain_acc: 0.8444, test_acc: \u001b[31m0.8327203833723392\u001b[0m, time: 52.46\n",
      "epoch: 165, loss: 94.00985, loss1: 0.72213, loss2_3: 93.28771\n",
      "\ttrain_acc: 0.8451, test_acc: \u001b[31m0.8337419666406627\u001b[0m, time: 52.49\n",
      "epoch: 166, loss: 93.98303, loss1: 0.72110, loss2_3: 93.26193\n",
      "\ttrain_acc: 0.8432, test_acc: \u001b[31m0.8322374531000408\u001b[0m, time: 52.49\n",
      "epoch: 167, loss: 94.04860, loss1: 0.72422, loss2_3: 93.32438\n",
      "\ttrain_acc: 0.8451, test_acc: \u001b[31m0.8326089379248858\u001b[0m, time: 52.50\n",
      "epoch: 168, loss: 93.86122, loss1: 0.71985, loss2_3: 93.14137\n",
      "\ttrain_acc: 0.8450, test_acc: \u001b[31m0.8324974924774323\u001b[0m, time: 52.58\n",
      "epoch: 169, loss: 94.09880, loss1: 0.72348, loss2_3: 93.37532\n",
      "\ttrain_acc: 0.8432, test_acc: \u001b[31m0.8323860470299789\u001b[0m, time: 52.42\n",
      "epoch: 170, loss: 93.81921, loss1: 0.72143, loss2_3: 93.09777\n",
      "\ttrain_acc: 0.8451, test_acc: \u001b[31m0.8331104424384264\u001b[0m, time: 52.46\n",
      "epoch: 171, loss: 93.88815, loss1: 0.71927, loss2_3: 93.16888\n",
      "\ttrain_acc: 0.8440, test_acc: \u001b[31m0.8319774137226494\u001b[0m, time: 52.46\n",
      "epoch: 172, loss: 94.01088, loss1: 0.72232, loss2_3: 93.28856\n",
      "\ttrain_acc: 0.8452, test_acc: \u001b[31m0.8333704818158179\u001b[0m, time: 52.47\n",
      "epoch: 173, loss: 94.14170, loss1: 0.72343, loss2_3: 93.41827\n",
      "\ttrain_acc: 0.8447, test_acc: \u001b[31m0.8331104424384264\u001b[0m, time: 52.52\n",
      "epoch: 174, loss: 93.97086, loss1: 0.72015, loss2_3: 93.25071\n",
      "\ttrain_acc: 0.8441, test_acc: \u001b[31m0.8323674727887366\u001b[0m, time: 52.48\n",
      "epoch: 175, loss: 93.74365, loss1: 0.71897, loss2_3: 93.02468\n",
      "\ttrain_acc: 0.8437, test_acc: \u001b[31m0.8323117500650098\u001b[0m, time: 52.45\n",
      "epoch: 176, loss: 93.83653, loss1: 0.71949, loss2_3: 93.11704\n",
      "\ttrain_acc: 0.8454, test_acc: \u001b[31m0.8328132545785505\u001b[0m, time: 52.54\n",
      "epoch: 177, loss: 93.86746, loss1: 0.72258, loss2_3: 93.14488\n",
      "\ttrain_acc: 0.8448, test_acc: \u001b[31m0.8326832348898547\u001b[0m, time: 52.47\n",
      "epoch: 178, loss: 93.80838, loss1: 0.72259, loss2_3: 93.08578\n",
      "\ttrain_acc: 0.8452, test_acc: \u001b[31m0.8330547197146997\u001b[0m, time: 52.46\n",
      "epoch: 179, loss: 93.71669, loss1: 0.71902, loss2_3: 92.99767\n",
      "\ttrain_acc: 0.8452, test_acc: \u001b[31m0.8327389576135815\u001b[0m, time: 52.42\n",
      "epoch: 180, loss: 93.66033, loss1: 0.72206, loss2_3: 92.93827\n",
      "\ttrain_acc: 0.8462, test_acc: \u001b[31m0.8335747984694826\u001b[0m, time: 52.47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 181, loss: 93.72417, loss1: 0.72062, loss2_3: 93.00355\n",
      "\ttrain_acc: 0.8459, test_acc: \u001b[31m0.8329247000260039\u001b[0m, time: 52.43\n",
      "epoch: 182, loss: 93.65479, loss1: 0.71982, loss2_3: 92.93498\n",
      "\ttrain_acc: 0.8460, test_acc: \u001b[31m0.8332218878858798\u001b[0m, time: 52.41\n",
      "epoch: 183, loss: 93.58247, loss1: 0.72006, loss2_3: 92.86241\n",
      "\ttrain_acc: 0.8429, test_acc: \u001b[31m0.8301199895984249\u001b[0m, time: 52.40\n",
      "epoch: 184, loss: 93.58211, loss1: 0.71739, loss2_3: 92.86472\n",
      "\ttrain_acc: 0.8465, test_acc: \u001b[31m0.8331847394033953\u001b[0m, time: 52.47\n",
      "epoch: 185, loss: 93.47166, loss1: 0.71804, loss2_3: 92.75362\n",
      "\ttrain_acc: 0.8460, test_acc: \u001b[31m0.8339462832943274\u001b[0m, time: 52.41\n",
      "epoch: 186, loss: 93.43593, loss1: 0.71773, loss2_3: 92.71821\n",
      "\ttrain_acc: 0.8464, test_acc: \u001b[31m0.8341134514655076\u001b[0m, time: 52.53\n",
      "epoch: 187, loss: 93.60323, loss1: 0.72176, loss2_3: 92.88148\n",
      "\ttrain_acc: 0.8440, test_acc: \u001b[31m0.8302500092871207\u001b[0m, time: 52.55\n",
      "epoch: 188, loss: 93.46640, loss1: 0.71807, loss2_3: 92.74833\n",
      "\ttrain_acc: 0.8458, test_acc: \u001b[31m0.8342248969129611\u001b[0m, time: 52.52\n",
      "epoch: 189, loss: 93.42841, loss1: 0.71722, loss2_3: 92.71120\n",
      "\ttrain_acc: 0.8440, test_acc: \u001b[31m0.8309744046955682\u001b[0m, time: 52.51\n",
      "epoch: 190, loss: 93.55653, loss1: 0.71895, loss2_3: 92.83758\n",
      "\ttrain_acc: 0.8452, test_acc: \u001b[31m0.8321631561350719\u001b[0m, time: 52.50\n",
      "epoch: 191, loss: 93.56081, loss1: 0.71908, loss2_3: 92.84173\n",
      "\ttrain_acc: 0.8456, test_acc: \u001b[31m0.8334447787807868\u001b[0m, time: 52.47\n",
      "epoch: 192, loss: 93.52119, loss1: 0.71786, loss2_3: 92.80333\n",
      "\ttrain_acc: 0.8436, test_acc: \u001b[31m0.831940265240165\u001b[0m, time: 52.58\n",
      "epoch: 193, loss: 93.29860, loss1: 0.71859, loss2_3: 92.58001\n",
      "\ttrain_acc: 0.8456, test_acc: \u001b[31m0.833760540881905\u001b[0m, time: 52.46\n",
      "epoch: 194, loss: 93.44741, loss1: 0.71769, loss2_3: 92.72972\n",
      "\ttrain_acc: 0.8473, test_acc: \u001b[31m0.8354322225937071\u001b[0m, time: 52.51\n",
      "best_acc: 0.8354322225937071\n",
      "epoch: 195, loss: 93.45271, loss1: 0.71793, loss2_3: 92.73477\n",
      "\ttrain_acc: 0.8465, test_acc: \u001b[31m0.8354879453174338\u001b[0m, time: 57.76\n",
      "best_acc: 0.8354879453174338\n",
      "epoch: 196, loss: 93.39852, loss1: 0.72036, loss2_3: 92.67815\n",
      "\ttrain_acc: 0.8466, test_acc: \u001b[31m0.8330918681971842\u001b[0m, time: 59.44\n",
      "epoch: 197, loss: 93.36438, loss1: 0.71727, loss2_3: 92.64710\n",
      "\ttrain_acc: 0.8461, test_acc: \u001b[31m0.8320517106876184\u001b[0m, time: 59.41\n",
      "epoch: 198, loss: 93.26634, loss1: 0.71555, loss2_3: 92.55080\n",
      "\ttrain_acc: 0.8473, test_acc: \u001b[31m0.8335005015045135\u001b[0m, time: 57.75\n",
      "epoch: 199, loss: 93.16812, loss1: 0.71839, loss2_3: 92.44974\n",
      "\ttrain_acc: 0.8462, test_acc: \u001b[31m0.8337791151231472\u001b[0m, time: 53.70\n",
      "epoch: 200, loss: 93.32385, loss1: 0.71912, loss2_3: 92.60472\n",
      "\ttrain_acc: 0.8434, test_acc: \u001b[31m0.8305100486645121\u001b[0m, time: 52.46\n",
      "epoch: 201, loss: 93.12757, loss1: 0.71628, loss2_3: 92.41129\n",
      "\ttrain_acc: 0.8474, test_acc: \u001b[31m0.8335376499869981\u001b[0m, time: 52.41\n",
      "epoch: 202, loss: 93.07766, loss1: 0.71832, loss2_3: 92.35934\n",
      "\ttrain_acc: 0.8459, test_acc: \u001b[31m0.8325346409599168\u001b[0m, time: 52.40\n",
      "epoch: 203, loss: 93.28048, loss1: 0.71825, loss2_3: 92.56223\n",
      "\ttrain_acc: 0.8467, test_acc: \u001b[31m0.8336305211932092\u001b[0m, time: 52.41\n",
      "epoch: 204, loss: 93.10541, loss1: 0.71775, loss2_3: 92.38765\n",
      "\ttrain_acc: 0.8482, test_acc: \u001b[31m0.833760540881905\u001b[0m, time: 52.43\n",
      "epoch: 205, loss: 93.03826, loss1: 0.71687, loss2_3: 92.32139\n",
      "\ttrain_acc: 0.8462, test_acc: \u001b[31m0.8326646606486126\u001b[0m, time: 52.41\n",
      "epoch: 206, loss: 93.06372, loss1: 0.71697, loss2_3: 92.34675\n",
      "\ttrain_acc: 0.8480, test_acc: \u001b[31m0.8333333333333334\u001b[0m, time: 52.42\n",
      "epoch: 207, loss: 93.01958, loss1: 0.71231, loss2_3: 92.30727\n",
      "\ttrain_acc: 0.8471, test_acc: \u001b[31m0.8333147590920911\u001b[0m, time: 52.40\n",
      "epoch: 208, loss: 93.07243, loss1: 0.71460, loss2_3: 92.35784\n",
      "\ttrain_acc: 0.8474, test_acc: \u001b[31m0.8325903636836435\u001b[0m, time: 52.48\n",
      "epoch: 209, loss: 92.91583, loss1: 0.71373, loss2_3: 92.20209\n",
      "\ttrain_acc: 0.8470, test_acc: \u001b[31m0.8325903636836435\u001b[0m, time: 52.39\n",
      "epoch: 210, loss: 92.90214, loss1: 0.71743, loss2_3: 92.18472\n",
      "\ttrain_acc: 0.8457, test_acc: \u001b[31m0.8314201864853821\u001b[0m, time: 52.47\n",
      "epoch: 211, loss: 92.93850, loss1: 0.71723, loss2_3: 92.22127\n",
      "\ttrain_acc: 0.8476, test_acc: \u001b[31m0.8340391545005387\u001b[0m, time: 52.48\n",
      "epoch: 212, loss: 92.93066, loss1: 0.71419, loss2_3: 92.21647\n",
      "\ttrain_acc: 0.8479, test_acc: \u001b[31m0.8344477878078681\u001b[0m, time: 52.45\n",
      "epoch: 213, loss: 93.02541, loss1: 0.71597, loss2_3: 92.30945\n",
      "\ttrain_acc: 0.8472, test_acc: \u001b[31m0.834373490842899\u001b[0m, time: 52.39\n",
      "epoch: 214, loss: 92.88344, loss1: 0.71415, loss2_3: 92.16930\n",
      "\ttrain_acc: 0.8466, test_acc: \u001b[31m0.832776106096066\u001b[0m, time: 52.39\n",
      "epoch: 215, loss: 92.88472, loss1: 0.71684, loss2_3: 92.16788\n",
      "\ttrain_acc: 0.8467, test_acc: \u001b[31m0.8314387607266244\u001b[0m, time: 52.40\n",
      "epoch: 216, loss: 92.79627, loss1: 0.71498, loss2_3: 92.08129\n",
      "\ttrain_acc: 0.8471, test_acc: \u001b[31m0.8333333333333334\u001b[0m, time: 52.48\n",
      "epoch: 217, loss: 92.61399, loss1: 0.71271, loss2_3: 91.90128\n",
      "\ttrain_acc: 0.8486, test_acc: \u001b[31m0.8340577287417809\u001b[0m, time: 52.43\n",
      "epoch: 218, loss: 92.69980, loss1: 0.71482, loss2_3: 91.98499\n",
      "\ttrain_acc: 0.8474, test_acc: \u001b[31m0.833686243916936\u001b[0m, time: 52.52\n",
      "epoch: 219, loss: 92.60337, loss1: 0.71409, loss2_3: 91.88929\n",
      "\ttrain_acc: 0.8475, test_acc: \u001b[31m0.8342806196366879\u001b[0m, time: 52.51\n",
      "epoch: 220, loss: 92.68709, loss1: 0.71474, loss2_3: 91.97235\n",
      "\ttrain_acc: 0.8490, test_acc: \u001b[31m0.8336490954344515\u001b[0m, time: 52.49\n",
      "epoch: 221, loss: 92.80565, loss1: 0.71380, loss2_3: 92.09186\n",
      "\ttrain_acc: 0.8474, test_acc: \u001b[31m0.8345406590140793\u001b[0m, time: 52.45\n",
      "epoch: 222, loss: 92.74655, loss1: 0.71472, loss2_3: 92.03183\n",
      "\ttrain_acc: 0.8499, test_acc: \u001b[31m0.834596381737806\u001b[0m, time: 52.47\n",
      "epoch: 223, loss: 92.68037, loss1: 0.71243, loss2_3: 91.96794\n",
      "\ttrain_acc: 0.8484, test_acc: \u001b[31m0.8330361454734574\u001b[0m, time: 52.38\n",
      "epoch: 224, loss: 92.65444, loss1: 0.71269, loss2_3: 91.94176\n",
      "\ttrain_acc: 0.8454, test_acc: \u001b[31m0.8305100486645121\u001b[0m, time: 52.47\n",
      "epoch: 225, loss: 92.67102, loss1: 0.71383, loss2_3: 91.95719\n",
      "\ttrain_acc: 0.8474, test_acc: \u001b[31m0.8336305211932092\u001b[0m, time: 52.44\n",
      "epoch: 226, loss: 92.63501, loss1: 0.71421, loss2_3: 91.92080\n",
      "\ttrain_acc: 0.8484, test_acc: \u001b[31m0.8335190757457558\u001b[0m, time: 52.44\n",
      "epoch: 227, loss: 92.64542, loss1: 0.71797, loss2_3: 91.92744\n",
      "\ttrain_acc: 0.8489, test_acc: \u001b[31m0.8337419666406627\u001b[0m, time: 52.46\n",
      "epoch: 228, loss: 92.61348, loss1: 0.71316, loss2_3: 91.90033\n",
      "\ttrain_acc: 0.8483, test_acc: \u001b[31m0.8338534120881163\u001b[0m, time: 52.44\n",
      "epoch: 229, loss: 92.54264, loss1: 0.71419, loss2_3: 91.82845\n",
      "\ttrain_acc: 0.8478, test_acc: \u001b[31m0.8327203833723392\u001b[0m, time: 52.32\n",
      "epoch: 230, loss: 92.48453, loss1: 0.71097, loss2_3: 91.77356\n",
      "\ttrain_acc: 0.8481, test_acc: \u001b[31m0.8343177681191724\u001b[0m, time: 52.34\n",
      "epoch: 231, loss: 92.37837, loss1: 0.71201, loss2_3: 91.66636\n",
      "\ttrain_acc: 0.8493, test_acc: \u001b[31m0.83338905605706\u001b[0m, time: 52.37\n",
      "epoch: 232, loss: 92.48307, loss1: 0.71461, loss2_3: 91.76846\n",
      "\ttrain_acc: 0.8447, test_acc: \u001b[31m0.8292470002600394\u001b[0m, time: 52.48\n",
      "epoch: 233, loss: 92.53990, loss1: 0.71495, loss2_3: 91.82495\n",
      "\ttrain_acc: 0.8450, test_acc: \u001b[31m0.8285040306103496\u001b[0m, time: 52.49\n",
      "epoch: 234, loss: 92.25991, loss1: 0.71206, loss2_3: 91.54785\n",
      "\ttrain_acc: 0.8489, test_acc: \u001b[31m0.8329989969909729\u001b[0m, time: 52.54\n",
      "epoch: 235, loss: 92.32300, loss1: 0.71270, loss2_3: 91.61030\n",
      "\ttrain_acc: 0.8491, test_acc: \u001b[31m0.8322931758237676\u001b[0m, time: 52.43\n",
      "epoch: 236, loss: 92.31842, loss1: 0.71439, loss2_3: 91.60403\n",
      "\ttrain_acc: 0.8496, test_acc: \u001b[31m0.8335005015045135\u001b[0m, time: 52.36\n",
      "epoch: 237, loss: 92.21382, loss1: 0.71278, loss2_3: 91.50104\n",
      "\ttrain_acc: 0.8481, test_acc: \u001b[31m0.8336676696756937\u001b[0m, time: 52.44\n",
      "epoch: 238, loss: 92.24204, loss1: 0.71472, loss2_3: 91.52732\n",
      "\ttrain_acc: 0.8499, test_acc: \u001b[31m0.8340391545005387\u001b[0m, time: 52.46\n",
      "epoch: 239, loss: 92.22173, loss1: 0.71222, loss2_3: 91.50951\n",
      "\ttrain_acc: 0.8495, test_acc: \u001b[31m0.8335376499869981\u001b[0m, time: 52.47\n",
      "epoch: 240, loss: 92.16162, loss1: 0.71355, loss2_3: 91.44807\n",
      "\ttrain_acc: 0.8489, test_acc: \u001b[31m0.8332404621271221\u001b[0m, time: 52.48\n",
      "epoch: 241, loss: 91.99868, loss1: 0.71037, loss2_3: 91.28831\n",
      "\ttrain_acc: 0.8507, test_acc: \u001b[31m0.833834837846874\u001b[0m, time: 52.44\n",
      "epoch: 242, loss: 92.30057, loss1: 0.71210, loss2_3: 91.58847\n",
      "\ttrain_acc: 0.8488, test_acc: \u001b[31m0.8337048181581782\u001b[0m, time: 52.47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 243, loss: 92.16288, loss1: 0.71311, loss2_3: 91.44977\n",
      "\ttrain_acc: 0.8502, test_acc: \u001b[31m0.8332033136446376\u001b[0m, time: 52.51\n",
      "epoch: 244, loss: 92.16328, loss1: 0.71116, loss2_3: 91.45212\n",
      "\ttrain_acc: 0.8505, test_acc: \u001b[31m0.8340020060180542\u001b[0m, time: 52.44\n",
      "epoch: 245, loss: 92.10089, loss1: 0.71129, loss2_3: 91.38961\n",
      "\ttrain_acc: 0.8499, test_acc: \u001b[31m0.8326832348898547\u001b[0m, time: 52.40\n",
      "epoch: 246, loss: 92.10872, loss1: 0.71049, loss2_3: 91.39823\n",
      "\ttrain_acc: 0.8497, test_acc: \u001b[31m0.8335376499869981\u001b[0m, time: 52.52\n",
      "epoch: 247, loss: 92.10155, loss1: 0.71293, loss2_3: 91.38862\n",
      "\ttrain_acc: 0.8496, test_acc: \u001b[31m0.8324046212712211\u001b[0m, time: 52.41\n",
      "epoch: 248, loss: 91.97077, loss1: 0.71179, loss2_3: 91.25898\n",
      "\ttrain_acc: 0.8486, test_acc: \u001b[31m0.8322560273412831\u001b[0m, time: 52.44\n",
      "epoch: 249, loss: 91.87891, loss1: 0.70966, loss2_3: 91.16925\n",
      "\ttrain_acc: 0.8483, test_acc: \u001b[31m0.8313830380028976\u001b[0m, time: 52.43\n",
      "epoch: 250, loss: 91.99666, loss1: 0.71218, loss2_3: 91.28448\n",
      "\ttrain_acc: 0.8507, test_acc: \u001b[31m0.8332218878858798\u001b[0m, time: 52.43\n"
     ]
    }
   ],
   "source": [
    "for num_model in range(1):  # just one train\n",
    "    net=newModel().to(device)\n",
    "    lr = 0.0001\n",
    "    \n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr,weight_decay=5e-4)\n",
    "    criterion = ContrastiveLoss()\n",
    "    criterion_model = nn.CrossEntropyLoss(reduction='sum')\n",
    "    \n",
    "    best_acc=0\n",
    "    EPOCH=250\n",
    "    for epoch in range(EPOCH):\n",
    "        loss_ls=[]\n",
    "        loss1_ls=[]\n",
    "        loss2_3_ls=[]\n",
    "        t0=time.time()\n",
    "        net.train()\n",
    "        for seq1,seq2,label,label1,label2 in train_iter_cont:\n",
    "                output1=net(seq1)\n",
    "                output2=net(seq2)\n",
    "                output3=net.trainModel(seq1)\n",
    "                output4=net.trainModel(seq2)\n",
    "                \n",
    "                loss1=criterion(output1, output2, label)\n",
    "                loss2=criterion_model(output3,label1)\n",
    "                loss3=criterion_model(output4,label2)\n",
    "                loss=loss1+loss2+loss3\n",
    "    #             print(loss)\n",
    "                optimizer.zero_grad() \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                loss_ls.append(loss.item())\n",
    "                loss1_ls.append(loss1.item())\n",
    "                loss2_3_ls.append((loss2+loss3).item())\n",
    "\n",
    "\n",
    "        net.eval() \n",
    "        with torch.no_grad(): \n",
    "            train_acc=evaluate_accuracy(train_iter,net)\n",
    "            test_acc=evaluate_accuracy(val_iter,net)\n",
    "            \n",
    "        results=f\"epoch: {epoch+1}, loss: {np.mean(loss_ls):.5f}, loss1: {np.mean(loss1_ls):.5f}, loss2_3: {np.mean(loss2_3_ls):.5f}\\n\"\n",
    "        results+=f'\\ttrain_acc: {train_acc:.4f}, test_acc: {colored(test_acc,\"red\")}, time: {time.time()-t0:.2f}'\n",
    "        print(results)\n",
    "        to_log(results)\n",
    "        if test_acc>best_acc:\n",
    "            best_acc=test_acc\n",
    "            torch.save({\"best_acc\":best_acc,\"model\":net.state_dict()},f'compareModel/2021ACS_PepFormer/Model/{num_model}.pl')\n",
    "            print(f\"best_acc: {best_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T01:36:17.469265Z",
     "start_time": "2021-08-31T01:36:17.433429Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T02:00:14.432919Z",
     "start_time": "2021-08-31T02:00:14.429626Z"
    }
   },
   "outputs": [],
   "source": [
    "def pred(data_iter, net):\n",
    "    y_pred = []\n",
    "    for x, y in data_iter:\n",
    "        x,y=x.to(device),y.to(device)\n",
    "        outputs=net.trainModel(x)\n",
    "        for _ in outputs.argmax(dim=1):\n",
    "            y_pred.append(int(_))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T02:15:44.725526Z",
     "start_time": "2021-08-31T02:15:44.722214Z"
    }
   },
   "outputs": [],
   "source": [
    "def pred_prob(data_iter, net):\n",
    "    y_pred = []\n",
    "    for x, y in data_iter:\n",
    "        x,y=x.to(device),y.to(device)\n",
    "        outputs=net.trainModel(x)\n",
    "        for _ in outputs:\n",
    "            y_pred.append(list(map(float, _)))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T06:49:10.807011Z",
     "start_time": "2021-09-02T06:49:02.742919Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.832639900145621\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.81      0.83     33706\n",
      "           1       0.82      0.86      0.84     33592\n",
      "\n",
      "    accuracy                           0.83     67298\n",
      "   macro avg       0.83      0.83      0.83     67298\n",
      "weighted avg       0.83      0.83      0.83     67298\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_acc = evaluate_accuracy(test_iter,net)\n",
    "print('Test Accuracy: {}'.format(test_acc))\n",
    "\n",
    "# prediction\n",
    "y_pred = pred(test_iter, net)\n",
    "print(classification_report(test_label, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T06:49:35.584254Z",
     "start_time": "2021-09-02T06:49:29.066467Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf auc : 0.9064775173821029\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANMAAADCCAYAAADTjffnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAWJUlEQVR4nO3deXgUVdbA4d/pNEIWNllUFpEtZBBBBRc2EYdVUUAWJSKIYHQQlBlUQBwUmW9cQOdD3GAcUBYDiARBFAVcEIEBHBBQdgLoIAYCYdEgWe780UnsJE1XJ10dqsN5feqBrqpTdQv75FbdqjoRYwxKqeC5zncDlCotNJmUsokmk1I20WRSyiaaTErZRJNJKZu4Q72DyPYTdOw9x8ElY853ExyjWoxb/C2PvGaY5fcmfdOrfrdR0kKeTEoViyvifLegyDSZlDNJ+F2BaDIpZ9KeSSmbiKMuhwKiyaScSXsmpWyi10xK2UR7JqVsEobJFH59qbowiMt6stqESBcR2Skie0RktJ/1rhORLBHpXdRYb5pMypkiIqwnP0QkAngN6Ao0BvqJSONzrPcC8ElRYwvSZFLOJGI9+Xc9sMcYs88YcxaYC3T3sd5w4H0gpRix+WgyKWdyRVhOIpIgIhu9pgSvLdQEfvD6/GPOvDwiUhPoCbxZYO+Wsb7oAIRypgCuiYwx04Bp59qCr5ACn/8fGGWMyZL8PV0gsYVoMilnCn4070egttfnWsChAuu0AObmJFJV4FYRyQwwthBNJuVMwT9OtAFoKCJ1gf8CdwPx3isYY+r+vjt5G/jQGLNIRNxWsb5oMilnCrJnMsZkisgwPKN0EcB0Y8x3IvJQzvKC10mWsVb71GRSzuQK/qtpjPkI+KjAPJ9JZIy5zyrWiiaTciZ9alwpm4Th40SaTMqZ9KlxpewhLk0mpWwhes2klD3EpcmklC1cepqnlD30NE8pm+hpnlI20Z5JKZvoNZNSdgm/jkmTSTmT9kxK2SQcByDCL/3VBUFELKcAtuG3XJeIdBeRLSKyOaeGRBuvZftFZGvuskDarD2TcqRgT/O8ynV1xPMa+gYRWWyM+d5rtZXAYmOMEZGmwHwgzmt5e2PM0YDbHFSLlQoRG3omy3JdxpjTxpjcQinRBFA0xR9NJuVI4hLrKchSXwAi0lNEdgBLgfu9FhngUxH5psB2zyksTvNcLuHrN4dw6OhJej05j8rlyzFrXC/qXFqRA4dP0H/8+6SdPlMobkficE79epas7Gwys7Jp89C/8i0f0fdGnvtTR2p1n0TqyXRaNqnF5BG3cjYjiwETFrLv0HEqRpdl1tO9uOOJd0vqcAMy/91ZLFm0AGMMd/TsTd/4AfmW/2fjesb8ZTiX1fR8f9q178CghKEA9O7WkaioaFwRLiIi3Pxr9nwAXn/lJf799WoaNIrjr88+B8CypYs5eeIEfePvLcGjC+ymrQ2lvjDGJAFJInITMAHokLOotTHmkIhUB5aLyA5jzCp/7QmLZBrW63p2HjxK+aiLAHgsvjVf/CeZSYlreKxfKx6Lb81T01b6jO3y55mknkwvNL9WtQrc0qIeBw+n5c17tE9L+j29gDqXViShe3NGv7GCMQPa8uKc1SE5ruLat2c3SxYt4J/vzMVdpgwjhz9IyzbtqH15nXzrNbumOS9Oft3nNl6ZOoNKlSvnfT596hTbvt3MO/OSGD/2Cfbu3kWt2pfz8ZJFvDRlakiPxxcbRvOKVK7LGLNKROqLSFVjzFFjzKGc+SkikoTntNFvMlme5olInIiMEpFXRGRyzt//ENDh2KBm1fJ0ubEhM5ZuypvXrVUjZn+yBYDZn2zh9taNirzdFx/uxNipK/P9qMrIyiKyrJuocmXIyMymbo3K1KhagdXfHgz2MGy1P3kfVzZpRrnISNxuN9dc24JVn68Iapsul4uMjAyMMfz222+43W7enTmd3nf3x12mjE0tL1p7rCYLeaW+ROQiPOW6FnuvICINJKcLFJFrgYuAVBGJFpHyOfOjgU7ANss2+1soIqPwXLgJsD6ngQIkBvqbAYI1cVhnxk5dQXb271/76hdHc/jYaQAOHztNtcpRPmONMSyZeA9fTx3C/d2uyZt/W6tYDh09yda9P+ff15yveW3kbQzrdQNvJm1g/OD2jJ/+hf0HFaR6DRqwedNGTqSlcSY9nbVff0XKz4cLrbdt62YG3t2TkcMfZN/ePXnzRYS/PPwA99/Thw8Wek7xoqKjufmPHRkU34vLatQkOqY827/fRtubbymx4/IW7ACEMSYTyC3XtR2Yn1vqK7fcF9AL2CYim/GM/N2VMyBxCbBaRL7F871faoxZZtVmq9O8wcCVxpiMAgf6MvAd8LyvoJwLtgQAd+wduGu0sGqHT11vbEhK2i9s2nWYts3qWAcUcMvwt/kp9TTVKkXx4aT+7DyYyn92HmJU/zZ0e3xOofW37P2Zdg/PAKB108v5KfUUIjBr3J1kZGYz+o3lpBz/pVjHYqcr6tan/8DB/HnoECKjomgQ24iIAr8VolFcYxZ8uJyoqGjWrl7FkyOHM3fRxwC8MX02VatV5/ixVEYMHUKdK+px9bUtuGfgYO4ZOBiA558dx5CHhrMkaQHr162hfsNY7hvyUKG2hIodN22tSn0ZY17A8xswCsbtA5oVdX9WfWU2UMPH/MtylvlkjJlmjGlhjGlR3EQCaNmkNt1axbIjcTgzx93JzdfUZfqTPUg59guXXhwDwKUXx3Dk+K8+439K9fReR9J+ZfFXO7gurgb1alxMnUsrsf6tBHYkDqdmtQqsnfYAl1SOzhc7un8bnpv5FWMH3sSEGV+SuHwrQ++8vtjHYrduPXox/d0FvPbWTCpUqEit2vl/2ETHxBAV5Tmmlm1uIjMzk7TjxwGoWq06AJUvrsJN7Tvw/bat+WJ37dgOQO06dVi2dDETXniZ5L17+OHggVAfVh47btqWNKtkGgGsFJGPRWRazrQMz82uR0PduHFvfUaDvpOJ6zeFAc8u5ItNydz/90UsXbOT/p2bAtC/c1M+XLOzUGxUuTLERF6U9/cOLerxXfIRvktOoc6dLxPXbwpx/abw3yMnaZnwT3726nH6d27Ksn/vIe30GaLKliHbGLKNIaqsc8Zrjh9LBeDwT4f48rMVdOhya77lqUePkHsL5fttW8jOzqZipUqkp//Kr794jjU9/Vc2rFtDvQYN8sW+9cYUhvxpGJmZmWRnZwGeL/eZM4UHckLF5RLLyWn8fjuMMctEJBbPSEZNPNdLPwIbjDFZJdA+nyYlrmH2070YeOvV/JByknueWQDAZVVieP2xbvQcM5fqlaOZN6EvAO4IF/NWbGP5hr2W244s66Z/52Z5p4GvvLeOxPF9OJuZxcAJC0N3UEU09vERnDyRRoTbzV9GP0WFChVZtGAeAD1638UXKz8lacE8IiIiKFu2HOOfm4SIcCw1lScfewSArKwsOna5jRtbtc3b7qrPVxJ3ZZO83uvKq65mQN8e1G8YS8PYuMINCREn9jxW5PcbwKER2X5CaHcQRg4uGXO+m+AY1WLcfrOl0ahPLL83O1/o7KiMc855i1JeIiIclScB0WRSjhSGZ3maTMqZnDjAYEWTSTlSOA5AaDIpR9KeSSmbaM+klE20Z1LKJppMStkkDM/yNJmUM4Vjz6Q1IJQjOaDUl99YX7RnUo4UbM8UTKmvAGMLtzmoFisVIiLWk4VgSn1ZxvqiyaQcKZAaECEs9RVQbEF6mqccKZDTvBCW+gootiBNJuVINgyNF7vUV1Fjc+lpnnIkG15bL3apr0BifdGeSTmSK8iuyRiTKSK5pb4igOm5pb5ylr+Jp9TXABHJANL5vdSXz1irfWoyKUey46ZtcUt9nSvWiiaTcqQwfABCk0k5Uzg+TqTJpBwpIgyfdNVkUo6kLwcqZZMIPc1Tyh5h2DFpMiln0gEIpWwS7E3b80GTSTmSJpNSNtEBCKVsEoYdkyaTcibtmZSyid60Vcom4fg4kb4cqBzJhoIqgZT6uien1NcWEVkjIs28lu0Xka25ZcACabP2TMqRSqjUVzLQzhhzXES64qkncYPX8vbGmKOB7lOTSTmSDQMQeeW6AEQkt1xXXjIZY9Z4rb8OT62HYgt5Mh1f/tdQ7yJsVL5u2PlugmOkb3rV7/IAK7YmAN7lvablVCwC3+W6vHudggYDH3t9NsCnImKAqV7bPSftmZQjBTIAYUepLwARaY8nmdp4zW5tjDkkItWB5SKywxizyl97dABCOZJLrCcLAZXryimL/BbQ3RiTmjvfGHMo588UIAnPaaP/Nls2SanzIMIllpOFQEp9XQ4sBO41xuzymh8tIuVz/w50ArZZ7VBP85QjBTv+EGCpr3FAFeD1nGu0TGNMC+ASPFVewZMj7xpjllntU5NJOZIdjxMFUOprCDDER9w+oFnB+VY0mZQjRYTfAxCaTMqZ9H0mpWwSEYZDY5pMypG0Z1LKJtozKWUT8fkAg7NpMilHcmvPpJQ99LV1pWwShuMPmkzKmdzaMyllD+2ZlLJJOBZU0WRSjhSGZ3n6PpNyJhveZwq2OpHfWF+0Z1KOFOzjRMFUJwowtnCbg2qxUiESIdaThbzqRMaYs0BudaI8xpg1xpjjOR+9qxNZxvqiyaQcySViOVnwVZ2opp/1vasTFTUW0NM85VCBnOZZlPoKpjpRwLHeNJmUIwUymmdR6quo1Ym6elUnCii2UJutm6xUyRMRy8lCsasTBRLri/ZMypGCvWkbTHWic8Va7VOTSTmSHfdsi1ud6FyxVjSZlCPp40RK2URrQChlkzDMJU0m5Ux6mqeUTbSgilI20Z5JKZuEYS5pMiln0tE8pWwSjqd5YfVs3v7kffS9s3ve1Or6a5k98+186yTv28u98XfR4uomvDPjX/mWzZn1Dnd270bPO27LF/ePlybSu+ftjB3zRN68JYsXMWfWO6E8nGJxuYS1iaN4f/JDAPx9RA82L3yK9fPGMO+lB6gYE1kopmGd6qybOzpv+vmriQyLv9lvfMtm9Vg/bwyrZz9OvdpVAagYE8ni1x4ukeMUsZ6cJqyS6Yq69Zi/8APmL/yAxPcWUq5cJLd06JhvnQoVKzFqzFgGDhqcb/7u3bt4f8F7zJn7Hu8t/IBVX37BgQP7OXXqFN9u3sSCpCVkZ2Wxe9dOzpw5w+JFSfS9O74kDy8gw+LbszP557zPK9ftoHmfv3P9Xc+x+0AKj9/fqVDM7gMp3Hj389x49/O0in+BX89ksPjzb/3GP3rvLfR7/C3GTVlCQp+2AIxJ6MKL0z8pgaP09ExWk9OEVTJ5+/e6tdSuXZsaNfK/s1WlShWaXNUUtzv/GWzyvr00bdaMyMhI3G43zVtcx2crluNyCRkZGRhjOPPbb7jdbt6e/hbx/e+lTJkyJXlIlmpWr0SXNlcyI2lN3ryV63aQlZUNwPqtydS8pJLfbbS/vhHJPx7h4E/H/cZnZGYRWbYMUZFlyMjMom6tqtSoXonV3+yx/8B8kAD+c5piJ5OIDLKzIUW17OOldLm1W8DrN2gQyzcbN5KWdpz09HRWf7WKw4cPEx0dQ4eOnbirVw9q1qxFTPnyfLdtG+1v6RDC1hfPxMd7MXbyIrKzfb+nNqB7Sz752m+ZAvp0bs78Zd9Yxk+c/imvPdWPYfHteXPuKsYPu53xr38Y3AEUgQ2/bb3EBTMAMR6Y4WuB9xuQr74+lcEPJPhardgyzp7ly88/49ERIwOOqVe/PoMGD+HBIfcTFRVFbKNGuCMiABg0+AEGDX4AgGfGjWXo8EdYuOA91q5ZTcPYRiQ8NNTW9hdH17ZNSDl2ik3bf6Bt84aFlj8xuDNZWdnM/WjDObdRxh3Bbe2uYtyUwq/mFIzfsuu/tBv4EgCtr63PT0dOIAiznh9ERmYWo19OIuXYKZuOrrBwHM3z2zN5lUEqOG3F8xupfTLGTMt5L6SF3YkEsHr1KuIaX0mVqlWLFHdnrz7MW5DEjJlzqFixEpfXqZNv+fbtnp/KdepcwZLFi5j48mT27NnNgQP77Wp6sbW8uh7d2l3FjqXjmfn8IG6+LpbpfxsAwD2338CtNzXhvrFv+91G5zaN2bzjh0JJYBU/ekgXnpv2MWMf7MqENz8i8aMNDO13sw1HdW52DEAEUOorTkTWishvIvJYgWX7RWSriGwWkY2BtNmqZ7oE6AwcLzBfgDWFVy8ZH3+0lK633lbkuNTUVKpUqcJPhw6xcsWnzJozL9/y16ZMZtwzz5KZmUl2VhYALnFxJv2MLe0Oxrgpi/N6lLbNGzJiwB+5/6mZdGz1B0be14FOQyaTfibD7zb6dmlR6BTPKr7/7Tew7KvvSDuVTlS5i8jONmRnG6LKhfZ6soRKfR0DHgF6nGMz7Y0xRwPdp1UyfQjEGGM2+2jsF4HuxE7p6emsW7OGvz79bN68+fMSAeh7Vz+OHjlCv7t68cvp07hcLmbPeoekxR8RExPDyBHDOZGWhtvt5smnnqZCxYp52/hs5QqaNLmK6tU9HW7Tq6+hV4/biY2NpVFcXMkeZBH8Y1Rfyl7k5sM3hgGwfut+Hvm/uVxWrSKvj4un5/A3AIgsV4Zbbohj2N8SA4rPjel/+w10G/oqAK/M/ozESUM4m5HJwDFvh/S4bDjJyyvXBSAiueW68pLJGJMCpIhI0X8y+yDGWBZdCcqZTOuqLheKytcNO99NcIz0Ta/6zZeNySctvzct6lY45zZEpDfQJedtWkTkXuAGY0yh/wki8gxw2hgzyWteMp4zMgNM9ap6dE76BIRypACviWwp9XUOrY0xh0SkOrBcRHYYY1b5C9BkUo4USDLZUerLz7YP5fyZIiJJeE4b/SZT2N60VaWbDTdti1WuC0BEokWkfO7fgU7ANqs47ZmUIwV7UzaQUl8icimwEagAZIvICKAxUBVIyin/5QbeNcYss9qnJpNypACKTFoKoNTXYX4v1u/tJNDMx3y/NJmUI4XhAxCaTMqZNJmUsokTnwq3osmkHMmJT4Vb0WRSzqTJpJQ9wvEVDE0m5UhhmEuaTMqZdABCKZvoAIRSdtFkUsoeOgChlE3CL5U0mZRD2fGga0nTZFKOFI4DEPpyoHIkB5T68hvri/ZMypGCPc0LptRXgLGFaM+kHEkCmCzklfoyxpwFckt95THGpBhjNgAFCwZaxvqiyaQcySViOVmoCfzg9fnHnHmBKFasJpNypgC6JhFJEJGNXlNCgS0UFGipr2LF6jWTcqRARvNCWOqrWLHaMylHEhHLyUKxS30VN1Z7JuVIwd5mCqbUlzHmpK9YyzZrrfGSo7XGf2dVa/zYL1mW35uLoyMcdWtXeyblSGH4NJEmk3ImTSalbKJv2iplk3B80FWTSTmSvoKhlE3CMJc0mZQzaTIpZZNwHIAI+U1bpxCRhEB+ye+FQP8tQuNCejYvwXqVC4b+W4TAhZRMSoWUJpNSNrmQkkmvEX6n/xYhcMEMQCgVahdSz6RUSJX6ZCpO/bPSSkSmi0iKiGw7320pjUp1MnnVP+sKNAb6iUjj89uq8+ptoMv5bkRpVaqTiWLWPyutjDGr8BReVCFQ2pMpmNppShVJaU+mYGqnKVUkpT2ZgqmdplSRlPZkCqZ2mlJFUqqTyRiTCeTWP9sOzA+k/llpJSKJwFqgkYj8KCKDz3ebShN9AkIpm5TqnkmpkqTJpJRNNJmUsokmk1I20WRSyiaaTErZRJNJKZtoMillk/8B4XsKOMQgqM0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 216x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# AUC\n",
    "probs = np.array(pred_prob(test_iter, net))[:, 1]\n",
    "\n",
    "rf_auc = roc_auc_score(test_label, probs)\n",
    "print('rf auc : {}'.format(rf_auc))\n",
    "# plot the roc curve for the model81\n",
    "rf_fpr, rf_tpr, _ = roc_curve(test_label, probs)\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.plot(rf_fpr, rf_tpr, marker='.', label='RF AUC = {:.4f}'.format(rf_auc), color='orange')\n",
    "plt.title('ROC curve')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()\n",
    "\n",
    "cf_matrix = confusion_matrix(test_label, y_pred)\n",
    "plt.figure(figsize=(3, 3))\n",
    "sns.heatmap(cf_matrix, annot=True, fmt=',.0f')\n",
    "plt.show()\n",
    "plt.figure(figsize=(3, 3))\n",
    "sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, \n",
    "            fmt='.2%', cmap='Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
