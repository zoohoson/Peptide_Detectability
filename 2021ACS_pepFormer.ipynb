{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T21:28:22.103622Z",
     "start_time": "2021-08-30T21:28:21.201330Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import time\n",
    "import pickle\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw code\n",
    "  - paper's code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T21:28:22.113120Z",
     "start_time": "2021-08-30T21:28:22.105563Z"
    }
   },
   "outputs": [],
   "source": [
    "def genData(file,max_len):\n",
    "    aa_dict={'A':1,'R':2,'N':3,'D':4,'C':5,'Q':6,'E':7,'G':8,'H':9,'I':10,\n",
    "             'L':11,'K':12,'M':13,'F':14,'P':15,'O':16,'S':17,'U':18,'T':19,\n",
    "             'W':20,'Y':21,'V':22,'X':23}\n",
    "    with open(file, 'r') as inf:\n",
    "        lines = inf.read().splitlines()\n",
    "        \n",
    "    long_pep_counter=0\n",
    "    pep_codes=[]\n",
    "    labels=[]\n",
    "    for pep in lines:\n",
    "        pep,label=pep.split(\",\")\n",
    "        labels.append(int(label))\n",
    "        if not len(pep) > max_len:\n",
    "            current_pep=[]\n",
    "            for aa in pep:\n",
    "                current_pep.append(aa_dict[aa])\n",
    "            pep_codes.append(torch.tensor(current_pep))\n",
    "        else:\n",
    "            long_pep_counter += 1\n",
    "    print(\"length > {}:\".format(max_len),long_pep_counter)\n",
    "    data = rnn_utils.pad_sequence(pep_codes, batch_first=True)  # padding\n",
    "    return data,torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T21:28:22.705925Z",
     "start_time": "2021-08-30T21:28:22.688398Z"
    }
   },
   "outputs": [],
   "source": [
    "class newModel(nn.Module):\n",
    "    def __init__(self, vocab_size=24):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = 25\n",
    "        self.batch_size = 256\n",
    "        self.emb_dim = 512\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, self.emb_dim, padding_idx=0)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=1)\n",
    "        \n",
    "        self.gru = nn.GRU(self.emb_dim, self.hidden_dim, num_layers=2, \n",
    "                               bidirectional=True, dropout=0.2)\n",
    "        \n",
    "        \n",
    "        self.block1=nn.Sequential(nn.Linear(4050,1024),\n",
    "                                            nn.BatchNorm1d(1024),\n",
    "                                            nn.LeakyReLU(),\n",
    "                                            nn.Linear(1024,256),\n",
    "                                 )\n",
    "\n",
    "        self.block2=nn.Sequential(\n",
    "                                               nn.BatchNorm1d(256),\n",
    "                                               nn.LeakyReLU(),\n",
    "                                               nn.Linear(256,128),\n",
    "                                               nn.BatchNorm1d(128),\n",
    "                                               nn.LeakyReLU(),\n",
    "                                               nn.Linear(128,64),\n",
    "                                               nn.BatchNorm1d(64),\n",
    "                                               nn.LeakyReLU(),\n",
    "                                               nn.Linear(64,2)\n",
    "                                            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x=self.embedding(x)\n",
    "        output=self.transformer_encoder(x).permute(1, 0, 2)\n",
    "        output,hn=self.gru(output)\n",
    "        output=output.permute(1,0,2)\n",
    "        hn=hn.permute(1,0,2)\n",
    "        output=output.reshape(output.shape[0],-1)\n",
    "        hn=hn.reshape(output.shape[0],-1)\n",
    "        output=torch.cat([output,hn],1)\n",
    "        return self.block1(output)\n",
    "\n",
    "    def trainModel(self, x):\n",
    "        with torch.no_grad():\n",
    "            output=self.forward(x)\n",
    "        return self.block2(output)\n",
    "\n",
    "\n",
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        # euclidean_distance: [128]\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +     # calmp夹断用法\n",
    "                                      (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))     \n",
    "        \n",
    "        return loss_contrastive\n",
    "    \n",
    "    \n",
    "def collate(batch):\n",
    "    seq1_ls=[]\n",
    "    seq2_ls=[]\n",
    "    label1_ls=[]\n",
    "    label2_ls=[]\n",
    "    label_ls=[]\n",
    "    batch_size=len(batch)\n",
    "    for i in range(int(batch_size/2)):\n",
    "        seq1,label1=batch[i][0],batch[i][1]\n",
    "        seq2,label2=batch[i+int(batch_size/2)][0],batch[i+int(batch_size/2)][1]\n",
    "        label1_ls.append(label1.unsqueeze(0))\n",
    "        label2_ls.append(label2.unsqueeze(0))\n",
    "        label=(label1^label2)\n",
    "        seq1_ls.append(seq1.unsqueeze(0))\n",
    "        seq2_ls.append(seq2.unsqueeze(0))\n",
    "        label_ls.append(label.unsqueeze(0))\n",
    "    seq1=torch.cat(seq1_ls).to(device)\n",
    "    seq2=torch.cat(seq2_ls).to(device)\n",
    "    label=torch.cat(label_ls).to(device)\n",
    "    label1=torch.cat(label1_ls).to(device)\n",
    "    label2=torch.cat(label2_ls).to(device)\n",
    "    return seq1,seq2,label,label1,label2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T21:28:43.270230Z",
     "start_time": "2021-08-30T21:28:41.280021Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length > 81: 0\n",
      "torch.Size([90000, 79]) torch.Size([90000])\n"
     ]
    }
   ],
   "source": [
    "data,label=genData(\"compareModel/2021ACS_PepFormer/dataset/Homo_sapiens.csv\",81)\n",
    "print(data.shape,label.shape)\n",
    "\n",
    "train_data,train_label=data[:70000],label[:70000]\n",
    "test_data,test_label=data[70000:],label[70000:]\n",
    "\n",
    "train_dataset = Data.TensorDataset(train_data, train_label)\n",
    "test_dataset = Data.TensorDataset(test_data, test_label)\n",
    "batch_size=256\n",
    "train_iter = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_iter = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_iter_cont = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \n",
    "                                                                  shuffle=True, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T21:28:48.296830Z",
     "start_time": "2021-08-30T21:28:44.688350Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newModel(\n",
      "  (embedding): Embedding(24, 512, padding_idx=0)\n",
      "  (encoder_layer): TransformerEncoderLayer(\n",
      "    (self_attn): MultiheadAttention(\n",
      "      (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
      "    )\n",
      "    (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout1): Dropout(p=0.1, inplace=False)\n",
      "    (dropout2): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (gru): GRU(512, 25, num_layers=2, dropout=0.2, bidirectional=True)\n",
      "  (block1): Sequential(\n",
      "    (0): Linear(in_features=4050, out_features=1024, bias=True)\n",
      "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01)\n",
      "    (3): Linear(in_features=1024, out_features=256, bias=True)\n",
      "  )\n",
      "  (block2): Sequential(\n",
      "    (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.01)\n",
      "    (5): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=64, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "total param cnt : 10,864,306\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\",0)\n",
    "model = newModel().to(device)\n",
    "\n",
    "print(model)\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print('total param cnt : {:,}'.format(params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T21:29:04.764149Z",
     "start_time": "2021-08-30T21:29:04.759884Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net):\n",
    "    acc_sum, n = 0.0, 0\n",
    "    for x, y in data_iter:\n",
    "        x,y=x.to(device),y.to(device)\n",
    "        outputs=net.trainModel(x)\n",
    "        acc_sum += (outputs.argmax(dim=1) == y).float().sum().item()\n",
    "        n += y.shape[0]\n",
    "    return acc_sum / n\n",
    "\n",
    "def to_log(log):\n",
    "    with open(\"compareModel/2021ACS_PepFormer/modelLog.log\",\"a+\") as f:\n",
    "        f.write(log+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T20:44:00.250478Z",
     "start_time": "2021-08-30T20:37:55.493756Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 181.73344, loss1: 2.59739, loss2_3: 179.13606\n",
      "\ttrain_acc: 0.5578, test_acc: \u001b[31m0.5561\u001b[0m, time: 36.31\n",
      "best_acc: 0.5561\n",
      "epoch: 2, loss: 175.95414, loss1: 1.02178, loss2_3: 174.93236\n",
      "\ttrain_acc: 0.5487, test_acc: \u001b[31m0.54865\u001b[0m, time: 36.38\n",
      "epoch: 3, loss: 174.22681, loss1: 1.02853, loss2_3: 173.19829\n",
      "\ttrain_acc: 0.5860, test_acc: \u001b[31m0.58595\u001b[0m, time: 36.33\n",
      "best_acc: 0.58595\n",
      "epoch: 4, loss: 162.11159, loss1: 1.02037, loss2_3: 161.09122\n",
      "\ttrain_acc: 0.6842, test_acc: \u001b[31m0.68805\u001b[0m, time: 36.34\n",
      "best_acc: 0.68805\n",
      "epoch: 5, loss: 138.78763, loss1: 0.98040, loss2_3: 137.80723\n",
      "\ttrain_acc: 0.7542, test_acc: \u001b[31m0.75475\u001b[0m, time: 36.39\n",
      "best_acc: 0.75475\n",
      "epoch: 1, loss: 181.11985, loss1: 2.38522, loss2_3: 178.73463\n",
      "\ttrain_acc: 0.5336, test_acc: \u001b[31m0.5331\u001b[0m, time: 36.31\n",
      "best_acc: 0.5331\n",
      "epoch: 2, loss: 176.11953, loss1: 1.02278, loss2_3: 175.09675\n",
      "\ttrain_acc: 0.5570, test_acc: \u001b[31m0.5606\u001b[0m, time: 36.32\n",
      "best_acc: 0.5606\n",
      "epoch: 3, loss: 159.19088, loss1: 1.01531, loss2_3: 158.17557\n",
      "\ttrain_acc: 0.7211, test_acc: \u001b[31m0.72255\u001b[0m, time: 36.32\n",
      "best_acc: 0.72255\n",
      "epoch: 4, loss: 138.65569, loss1: 0.98186, loss2_3: 137.67383\n",
      "\ttrain_acc: 0.7479, test_acc: \u001b[31m0.7491\u001b[0m, time: 36.35\n",
      "best_acc: 0.7491\n",
      "epoch: 5, loss: 133.79938, loss1: 0.95522, loss2_3: 132.84416\n",
      "\ttrain_acc: 0.7479, test_acc: \u001b[31m0.7467\u001b[0m, time: 36.36\n"
     ]
    }
   ],
   "source": [
    "for num_model in range(2):\n",
    "    net=newModel().to(device)\n",
    "    lr = 0.0001\n",
    "    \n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr,weight_decay=5e-4)\n",
    "    criterion = ContrastiveLoss()\n",
    "    criterion_model = nn.CrossEntropyLoss(reduction='sum')\n",
    "    \n",
    "    best_acc=0\n",
    "    EPOCH=5\n",
    "    for epoch in range(EPOCH):\n",
    "        loss_ls=[]\n",
    "        loss1_ls=[]\n",
    "        loss2_3_ls=[]\n",
    "        t0=time.time()\n",
    "        net.train()\n",
    "        for seq1,seq2,label,label1,label2 in train_iter_cont:\n",
    "                output1=net(seq1)\n",
    "                output2=net(seq2)\n",
    "                output3=net.trainModel(seq1)\n",
    "                output4=net.trainModel(seq2)\n",
    "                \n",
    "                loss1=criterion(output1, output2, label)\n",
    "                loss2=criterion_model(output3,label1)\n",
    "                loss3=criterion_model(output4,label2)\n",
    "                loss=loss1+loss2+loss3\n",
    "    #             print(loss)\n",
    "                optimizer.zero_grad() \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                loss_ls.append(loss.item())\n",
    "                loss1_ls.append(loss1.item())\n",
    "                loss2_3_ls.append((loss2+loss3).item())\n",
    "\n",
    "\n",
    "        net.eval() \n",
    "        with torch.no_grad(): \n",
    "            train_acc=evaluate_accuracy(train_iter,net)\n",
    "            test_acc=evaluate_accuracy(test_iter,net)\n",
    "            \n",
    "        results=f\"epoch: {epoch+1}, loss: {np.mean(loss_ls):.5f}, loss1: {np.mean(loss1_ls):.5f}, loss2_3: {np.mean(loss2_3_ls):.5f}\\n\"\n",
    "        results+=f'\\ttrain_acc: {train_acc:.4f}, test_acc: {colored(test_acc,\"red\")}, time: {time.time()-t0:.2f}'\n",
    "        print(results)\n",
    "        to_log(results)\n",
    "        if test_acc>best_acc:\n",
    "            best_acc=test_acc\n",
    "            torch.save({\"best_acc\":best_acc,\"model\":net.state_dict()},f'compareModel/2021ACS_PepFormer/Model/{num_model}.pl')\n",
    "            print(f\"best_acc: {best_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Make Data X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T21:29:27.310382Z",
     "start_time": "2021-08-30T21:29:26.383533Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T21:29:32.561604Z",
     "start_time": "2021-08-30T21:29:30.743803Z"
    }
   },
   "outputs": [],
   "source": [
    "df_detect_peptide_train = pd.read_csv('data/df_detect_peptide_train.csv')\n",
    "df_detect_peptide_test = pd.read_csv('data/df_detect_peptide_test.csv')\n",
    "\n",
    "tra, val = train_test_split(df_detect_peptide_train[['PEP', 'ID']], test_size=0.2, random_state=7)\n",
    "tra.to_csv('compareModel/2021ACS_PepFormer/detect_peptide_train.csv', header=False, index=False)\n",
    "val.to_csv('compareModel/2021ACS_PepFormer/detect_peptide_val.csv', header=False, index=False)\n",
    "df_detect_peptide_test[['PEP', 'ID']].to_csv('compareModel/2021ACS_PepFormer/detect_peptide_test.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T21:30:56.460546Z",
     "start_time": "2021-08-30T21:30:49.144762Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length > 30: 0\n",
      "torch.Size([215352, 30]) torch.Size([215352])\n",
      "length > 30: 0\n",
      "torch.Size([53838, 30]) torch.Size([53838])\n",
      "length > 30: 0\n",
      "torch.Size([67298, 30]) torch.Size([67298])\n"
     ]
    }
   ],
   "source": [
    "train_data,train_label=genData(\"compareModel/2021ACS_PepFormer/detect_peptide_train.csv\",30)\n",
    "print(train_data.shape, train_label.shape)\n",
    "train_dataset = Data.TensorDataset(train_data, train_label)\n",
    "\n",
    "val_data,val_label=genData(\"compareModel/2021ACS_PepFormer/detect_peptide_val.csv\",30)\n",
    "print(val_data.shape, val_label.shape)\n",
    "val_dataset = Data.TensorDataset(val_data, val_label)\n",
    "\n",
    "test_data,test_label=genData(\"compareModel/2021ACS_PepFormer/detect_peptide_test.csv\",30)\n",
    "print(test_data.shape, test_label.shape)\n",
    "test_dataset = Data.TensorDataset(test_data, test_label)\n",
    "\n",
    "batch_size=256\n",
    "train_iter = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "train_iter_cont = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \n",
    "                                                                  shuffle=True, collate_fn=collate)\n",
    "val_iter = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_iter = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T21:38:23.235474Z",
     "start_time": "2021-08-30T21:38:23.217948Z"
    }
   },
   "outputs": [],
   "source": [
    "class newModel(nn.Module):\n",
    "    def __init__(self, vocab_size=24):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = 25\n",
    "        self.batch_size = 256\n",
    "        self.emb_dim = 512\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, self.emb_dim, padding_idx=0)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=1)\n",
    "        \n",
    "        self.gru = nn.GRU(self.emb_dim, self.hidden_dim, num_layers=2, \n",
    "                               bidirectional=True, dropout=0.2)\n",
    "        \n",
    "        \n",
    "        self.block1=nn.Sequential(nn.Linear(1600,1024),\n",
    "                                            nn.BatchNorm1d(1024),\n",
    "                                            nn.LeakyReLU(),\n",
    "                                            nn.Linear(1024,256),\n",
    "                                 )\n",
    "\n",
    "        self.block2=nn.Sequential(\n",
    "                                               nn.BatchNorm1d(256),\n",
    "                                               nn.LeakyReLU(),\n",
    "                                               nn.Linear(256,128),\n",
    "                                               nn.BatchNorm1d(128),\n",
    "                                               nn.LeakyReLU(),\n",
    "                                               nn.Linear(128,64),\n",
    "                                               nn.BatchNorm1d(64),\n",
    "                                               nn.LeakyReLU(),\n",
    "                                               nn.Linear(64,2)\n",
    "                                            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x=self.embedding(x)\n",
    "        output=self.transformer_encoder(x).permute(1, 0, 2)\n",
    "        output,hn=self.gru(output)\n",
    "        output=output.permute(1,0,2)\n",
    "        hn=hn.permute(1,0,2)\n",
    "        output=output.reshape(output.shape[0],-1)\n",
    "        hn=hn.reshape(output.shape[0],-1)\n",
    "        output=torch.cat([output,hn],1)\n",
    "        return self.block1(output)\n",
    "\n",
    "    def trainModel(self, x):\n",
    "        with torch.no_grad():\n",
    "            output=self.forward(x)\n",
    "        return self.block2(output)\n",
    "\n",
    "\n",
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        # euclidean_distance: [128]\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +     # calmp夹断用法\n",
    "                                      (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))     \n",
    "        \n",
    "        return loss_contrastive\n",
    "    \n",
    "    \n",
    "def collate(batch):\n",
    "    seq1_ls=[]\n",
    "    seq2_ls=[]\n",
    "    label1_ls=[]\n",
    "    label2_ls=[]\n",
    "    label_ls=[]\n",
    "    batch_size=len(batch)\n",
    "    for i in range(int(batch_size/2)):\n",
    "        seq1,label1=batch[i][0],batch[i][1]\n",
    "        seq2,label2=batch[i+int(batch_size/2)][0],batch[i+int(batch_size/2)][1]\n",
    "        label1_ls.append(label1.unsqueeze(0))\n",
    "        label2_ls.append(label2.unsqueeze(0))\n",
    "        label=(label1^label2)\n",
    "        seq1_ls.append(seq1.unsqueeze(0))\n",
    "        seq2_ls.append(seq2.unsqueeze(0))\n",
    "        label_ls.append(label.unsqueeze(0))\n",
    "    seq1=torch.cat(seq1_ls).to(device)\n",
    "    seq2=torch.cat(seq2_ls).to(device)\n",
    "    label=torch.cat(label_ls).to(device)\n",
    "    label1=torch.cat(label1_ls).to(device)\n",
    "    label2=torch.cat(label2_ls).to(device)\n",
    "    return seq1,seq2,label,label1,label2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T01:27:34.173866Z",
     "start_time": "2021-08-30T21:38:23.696487Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 157.79544, loss1: 1.45590, loss2_3: 156.33954\n",
      "\ttrain_acc: 0.7599, test_acc: \u001b[31m0.7590549426055946\u001b[0m, time: 64.05\n",
      "best_acc: 0.7590549426055946\n",
      "epoch: 2, loss: 124.56889, loss1: 0.92612, loss2_3: 123.64277\n",
      "\ttrain_acc: 0.7897, test_acc: \u001b[31m0.7894795497603923\u001b[0m, time: 63.93\n",
      "best_acc: 0.7894795497603923\n",
      "epoch: 3, loss: 118.81992, loss1: 0.90422, loss2_3: 117.91570\n",
      "\ttrain_acc: 0.7891, test_acc: \u001b[31m0.7878450165310747\u001b[0m, time: 65.07\n",
      "epoch: 4, loss: 115.51865, loss1: 0.88433, loss2_3: 114.63432\n",
      "\ttrain_acc: 0.7891, test_acc: \u001b[31m0.7902225194100821\u001b[0m, time: 64.20\n",
      "best_acc: 0.7902225194100821\n",
      "epoch: 5, loss: 114.16350, loss1: 0.87263, loss2_3: 113.29087\n",
      "\ttrain_acc: 0.7854, test_acc: \u001b[31m0.7858018499944277\u001b[0m, time: 63.56\n",
      "epoch: 6, loss: 111.80736, loss1: 0.86087, loss2_3: 110.94649\n",
      "\ttrain_acc: 0.8099, test_acc: \u001b[31m0.8092611166833835\u001b[0m, time: 64.21\n",
      "best_acc: 0.8092611166833835\n",
      "epoch: 7, loss: 109.85247, loss1: 0.84179, loss2_3: 109.01068\n",
      "\ttrain_acc: 0.8106, test_acc: \u001b[31m0.8107099075002786\u001b[0m, time: 63.84\n",
      "best_acc: 0.8107099075002786\n",
      "epoch: 8, loss: 109.44311, loss1: 0.83591, loss2_3: 108.60720\n",
      "\ttrain_acc: 0.8121, test_acc: \u001b[31m0.8101155317805268\u001b[0m, time: 63.35\n",
      "epoch: 9, loss: 108.64083, loss1: 0.83127, loss2_3: 107.80956\n",
      "\ttrain_acc: 0.8157, test_acc: \u001b[31m0.8125859058657454\u001b[0m, time: 63.35\n",
      "best_acc: 0.8125859058657454\n",
      "epoch: 10, loss: 107.68807, loss1: 0.82630, loss2_3: 106.86178\n",
      "\ttrain_acc: 0.8129, test_acc: \u001b[31m0.8102269772279802\u001b[0m, time: 64.41\n",
      "epoch: 11, loss: 106.59012, loss1: 0.81937, loss2_3: 105.77074\n",
      "\ttrain_acc: 0.8102, test_acc: \u001b[31m0.8079423455551841\u001b[0m, time: 63.33\n",
      "epoch: 12, loss: 106.29528, loss1: 0.81450, loss2_3: 105.48077\n",
      "\ttrain_acc: 0.8202, test_acc: \u001b[31m0.8178795646197853\u001b[0m, time: 63.95\n",
      "best_acc: 0.8178795646197853\n",
      "epoch: 13, loss: 105.69258, loss1: 0.81075, loss2_3: 104.88183\n",
      "\ttrain_acc: 0.8216, test_acc: \u001b[31m0.8202199190163082\u001b[0m, time: 63.72\n",
      "best_acc: 0.8202199190163082\n",
      "epoch: 14, loss: 105.56074, loss1: 0.80635, loss2_3: 104.75439\n",
      "\ttrain_acc: 0.8208, test_acc: \u001b[31m0.8203313644637616\u001b[0m, time: 63.49\n",
      "best_acc: 0.8203313644637616\n",
      "epoch: 15, loss: 104.82173, loss1: 0.80000, loss2_3: 104.02173\n",
      "\ttrain_acc: 0.8188, test_acc: \u001b[31m0.816876555592704\u001b[0m, time: 63.69\n",
      "epoch: 16, loss: 104.74118, loss1: 0.79869, loss2_3: 103.94249\n",
      "\ttrain_acc: 0.8210, test_acc: \u001b[31m0.8181210297559345\u001b[0m, time: 64.41\n",
      "epoch: 17, loss: 104.40565, loss1: 0.79728, loss2_3: 103.60837\n",
      "\ttrain_acc: 0.8226, test_acc: \u001b[31m0.8205542553586685\u001b[0m, time: 64.14\n",
      "best_acc: 0.8205542553586685\n",
      "epoch: 18, loss: 104.03009, loss1: 0.79043, loss2_3: 103.23965\n",
      "\ttrain_acc: 0.8220, test_acc: \u001b[31m0.8184925145807794\u001b[0m, time: 65.44\n",
      "epoch: 19, loss: 103.98417, loss1: 0.79414, loss2_3: 103.19003\n",
      "\ttrain_acc: 0.8208, test_acc: \u001b[31m0.8181396039971767\u001b[0m, time: 64.01\n",
      "epoch: 20, loss: 103.31940, loss1: 0.78592, loss2_3: 102.53348\n",
      "\ttrain_acc: 0.8170, test_acc: \u001b[31m0.8147219436086036\u001b[0m, time: 63.62\n",
      "epoch: 21, loss: 103.35687, loss1: 0.78521, loss2_3: 102.57166\n",
      "\ttrain_acc: 0.8247, test_acc: \u001b[31m0.8223373825179241\u001b[0m, time: 64.06\n",
      "best_acc: 0.8223373825179241\n",
      "epoch: 22, loss: 102.95613, loss1: 0.78336, loss2_3: 102.17277\n",
      "\ttrain_acc: 0.8254, test_acc: \u001b[31m0.8229874809614027\u001b[0m, time: 64.33\n",
      "best_acc: 0.8229874809614027\n",
      "epoch: 23, loss: 102.85393, loss1: 0.78581, loss2_3: 102.06812\n",
      "\ttrain_acc: 0.8255, test_acc: \u001b[31m0.8230432036851295\u001b[0m, time: 63.83\n",
      "best_acc: 0.8230432036851295\n",
      "epoch: 24, loss: 102.48990, loss1: 0.78079, loss2_3: 101.70910\n",
      "\ttrain_acc: 0.8251, test_acc: \u001b[31m0.8230246294438872\u001b[0m, time: 63.94\n",
      "epoch: 25, loss: 102.27278, loss1: 0.77890, loss2_3: 101.49389\n",
      "\ttrain_acc: 0.8257, test_acc: \u001b[31m0.8235075597161856\u001b[0m, time: 63.53\n",
      "best_acc: 0.8235075597161856\n",
      "epoch: 26, loss: 102.25274, loss1: 0.77592, loss2_3: 101.47682\n",
      "\ttrain_acc: 0.8278, test_acc: \u001b[31m0.8255321520115904\u001b[0m, time: 64.22\n",
      "best_acc: 0.8255321520115904\n",
      "epoch: 27, loss: 101.82918, loss1: 0.77328, loss2_3: 101.05590\n",
      "\ttrain_acc: 0.8252, test_acc: \u001b[31m0.8220587688992904\u001b[0m, time: 64.68\n",
      "epoch: 28, loss: 101.76433, loss1: 0.77129, loss2_3: 100.99304\n",
      "\ttrain_acc: 0.8262, test_acc: \u001b[31m0.8243805490545711\u001b[0m, time: 63.84\n",
      "epoch: 29, loss: 101.33173, loss1: 0.76861, loss2_3: 100.56312\n",
      "\ttrain_acc: 0.8241, test_acc: \u001b[31m0.8215386901445076\u001b[0m, time: 63.98\n",
      "epoch: 30, loss: 101.14114, loss1: 0.77040, loss2_3: 100.37074\n",
      "\ttrain_acc: 0.8302, test_acc: \u001b[31m0.827482447342026\u001b[0m, time: 65.00\n",
      "best_acc: 0.827482447342026\n",
      "epoch: 31, loss: 100.89879, loss1: 0.76650, loss2_3: 100.13229\n",
      "\ttrain_acc: 0.8274, test_acc: \u001b[31m0.8243991232958133\u001b[0m, time: 63.89\n",
      "epoch: 32, loss: 100.83164, loss1: 0.76545, loss2_3: 100.06619\n",
      "\ttrain_acc: 0.8300, test_acc: \u001b[31m0.8276867639956907\u001b[0m, time: 63.37\n",
      "best_acc: 0.8276867639956907\n",
      "epoch: 33, loss: 100.61932, loss1: 0.76453, loss2_3: 99.85479\n",
      "\ttrain_acc: 0.8285, test_acc: \u001b[31m0.8270552397934544\u001b[0m, time: 63.67\n",
      "epoch: 34, loss: 100.45152, loss1: 0.76553, loss2_3: 99.68599\n",
      "\ttrain_acc: 0.8289, test_acc: \u001b[31m0.8270180913109699\u001b[0m, time: 64.22\n",
      "epoch: 35, loss: 100.33909, loss1: 0.76277, loss2_3: 99.57633\n",
      "\ttrain_acc: 0.8255, test_acc: \u001b[31m0.8233403915450054\u001b[0m, time: 63.94\n",
      "epoch: 36, loss: 100.12471, loss1: 0.75728, loss2_3: 99.36743\n",
      "\ttrain_acc: 0.8320, test_acc: \u001b[31m0.8287454957464988\u001b[0m, time: 63.68\n",
      "best_acc: 0.8287454957464988\n",
      "epoch: 37, loss: 99.89057, loss1: 0.75921, loss2_3: 99.13136\n",
      "\ttrain_acc: 0.8300, test_acc: \u001b[31m0.8284483078866228\u001b[0m, time: 63.25\n",
      "epoch: 38, loss: 99.92323, loss1: 0.76034, loss2_3: 99.16289\n",
      "\ttrain_acc: 0.8311, test_acc: \u001b[31m0.8262751216612801\u001b[0m, time: 63.34\n",
      "epoch: 39, loss: 99.73129, loss1: 0.75574, loss2_3: 98.97555\n",
      "\ttrain_acc: 0.8313, test_acc: \u001b[31m0.8297113562910955\u001b[0m, time: 64.21\n",
      "best_acc: 0.8297113562910955\n",
      "epoch: 40, loss: 99.57385, loss1: 0.75391, loss2_3: 98.81994\n",
      "\ttrain_acc: 0.8328, test_acc: \u001b[31m0.829674207808611\u001b[0m, time: 63.95\n",
      "epoch: 41, loss: 99.30772, loss1: 0.75407, loss2_3: 98.55365\n",
      "\ttrain_acc: 0.8313, test_acc: \u001b[31m0.82974850477358\u001b[0m, time: 63.83\n",
      "best_acc: 0.82974850477358\n",
      "epoch: 42, loss: 99.18516, loss1: 0.75338, loss2_3: 98.43178\n",
      "\ttrain_acc: 0.8320, test_acc: \u001b[31m0.8296184850848842\u001b[0m, time: 63.60\n",
      "epoch: 43, loss: 99.16088, loss1: 0.75075, loss2_3: 98.41012\n",
      "\ttrain_acc: 0.8321, test_acc: \u001b[31m0.8292841487425239\u001b[0m, time: 63.72\n",
      "epoch: 44, loss: 99.08377, loss1: 0.75126, loss2_3: 98.33251\n",
      "\ttrain_acc: 0.8323, test_acc: \u001b[31m0.8286526245402875\u001b[0m, time: 63.67\n",
      "epoch: 45, loss: 98.94970, loss1: 0.75116, loss2_3: 98.19854\n",
      "\ttrain_acc: 0.8316, test_acc: \u001b[31m0.8295256138786731\u001b[0m, time: 52.80\n",
      "epoch: 46, loss: 98.95309, loss1: 0.75186, loss2_3: 98.20124\n",
      "\ttrain_acc: 0.8335, test_acc: \u001b[31m0.8311229986255061\u001b[0m, time: 52.51\n",
      "best_acc: 0.8311229986255061\n",
      "epoch: 47, loss: 98.79001, loss1: 0.74996, loss2_3: 98.04005\n",
      "\ttrain_acc: 0.8325, test_acc: \u001b[31m0.8286154760578031\u001b[0m, time: 52.56\n",
      "epoch: 48, loss: 98.72106, loss1: 0.74574, loss2_3: 97.97532\n",
      "\ttrain_acc: 0.8344, test_acc: \u001b[31m0.8302685835283629\u001b[0m, time: 53.32\n",
      "epoch: 49, loss: 98.63181, loss1: 0.74941, loss2_3: 97.88241\n",
      "\ttrain_acc: 0.8344, test_acc: \u001b[31m0.8304357516995431\u001b[0m, time: 53.52\n",
      "epoch: 50, loss: 98.48596, loss1: 0.74820, loss2_3: 97.73777\n",
      "\ttrain_acc: 0.8349, test_acc: \u001b[31m0.8319959879638916\u001b[0m, time: 54.36\n",
      "best_acc: 0.8319959879638916\n",
      "epoch: 51, loss: 98.53916, loss1: 0.74496, loss2_3: 97.79420\n",
      "\ttrain_acc: 0.8336, test_acc: \u001b[31m0.8296370593261265\u001b[0m, time: 52.53\n",
      "epoch: 52, loss: 98.40668, loss1: 0.74694, loss2_3: 97.65974\n",
      "\ttrain_acc: 0.8346, test_acc: \u001b[31m0.8324046212712211\u001b[0m, time: 52.56\n",
      "best_acc: 0.8324046212712211\n",
      "epoch: 53, loss: 98.37763, loss1: 0.74560, loss2_3: 97.63203\n",
      "\ttrain_acc: 0.8339, test_acc: \u001b[31m0.8301014153571826\u001b[0m, time: 52.52\n",
      "epoch: 54, loss: 98.38744, loss1: 0.74528, loss2_3: 97.64216\n",
      "\ttrain_acc: 0.8351, test_acc: \u001b[31m0.8305657713882388\u001b[0m, time: 52.43\n",
      "epoch: 55, loss: 98.11198, loss1: 0.74422, loss2_3: 97.36776\n",
      "\ttrain_acc: 0.8353, test_acc: \u001b[31m0.8319774137226494\u001b[0m, time: 52.49\n",
      "epoch: 56, loss: 98.02468, loss1: 0.74504, loss2_3: 97.27963\n",
      "\ttrain_acc: 0.8351, test_acc: \u001b[31m0.8318845425164382\u001b[0m, time: 52.43\n",
      "epoch: 57, loss: 97.92789, loss1: 0.74289, loss2_3: 97.18501\n",
      "\ttrain_acc: 0.8322, test_acc: \u001b[31m0.8304543259407853\u001b[0m, time: 52.43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 58, loss: 97.93607, loss1: 0.74302, loss2_3: 97.19305\n",
      "\ttrain_acc: 0.8348, test_acc: \u001b[31m0.8316059288978045\u001b[0m, time: 52.46\n",
      "epoch: 59, loss: 97.90876, loss1: 0.74537, loss2_3: 97.16339\n",
      "\ttrain_acc: 0.8345, test_acc: \u001b[31m0.8321445818938297\u001b[0m, time: 54.71\n",
      "epoch: 60, loss: 97.88411, loss1: 0.74297, loss2_3: 97.14114\n",
      "\ttrain_acc: 0.8354, test_acc: \u001b[31m0.8314201864853821\u001b[0m, time: 57.76\n",
      "epoch: 61, loss: 97.77314, loss1: 0.74127, loss2_3: 97.03187\n",
      "\ttrain_acc: 0.8361, test_acc: \u001b[31m0.8315130576915933\u001b[0m, time: 52.58\n",
      "epoch: 62, loss: 97.64517, loss1: 0.74038, loss2_3: 96.90479\n",
      "\ttrain_acc: 0.8362, test_acc: \u001b[31m0.8320888591701029\u001b[0m, time: 52.42\n",
      "epoch: 63, loss: 97.55289, loss1: 0.74046, loss2_3: 96.81243\n",
      "\ttrain_acc: 0.8361, test_acc: \u001b[31m0.8321445818938297\u001b[0m, time: 52.51\n",
      "epoch: 64, loss: 97.61019, loss1: 0.73968, loss2_3: 96.87050\n",
      "\ttrain_acc: 0.8347, test_acc: \u001b[31m0.8298228017385489\u001b[0m, time: 52.42\n",
      "epoch: 65, loss: 97.58285, loss1: 0.73981, loss2_3: 96.84304\n",
      "\ttrain_acc: 0.8356, test_acc: \u001b[31m0.8322003046175563\u001b[0m, time: 52.46\n",
      "epoch: 66, loss: 97.39326, loss1: 0.74050, loss2_3: 96.65276\n",
      "\ttrain_acc: 0.8361, test_acc: \u001b[31m0.8316616516215313\u001b[0m, time: 52.40\n",
      "epoch: 67, loss: 97.35506, loss1: 0.74038, loss2_3: 96.61468\n",
      "\ttrain_acc: 0.8354, test_acc: \u001b[31m0.8307143653181768\u001b[0m, time: 52.46\n",
      "epoch: 68, loss: 97.32439, loss1: 0.73874, loss2_3: 96.58566\n",
      "\ttrain_acc: 0.8364, test_acc: \u001b[31m0.8325160667186745\u001b[0m, time: 52.40\n",
      "best_acc: 0.8325160667186745\n",
      "epoch: 69, loss: 97.32128, loss1: 0.74081, loss2_3: 96.58046\n",
      "\ttrain_acc: 0.8370, test_acc: \u001b[31m0.8329247000260039\u001b[0m, time: 52.39\n",
      "best_acc: 0.8329247000260039\n",
      "epoch: 70, loss: 97.26047, loss1: 0.73722, loss2_3: 96.52324\n",
      "\ttrain_acc: 0.8369, test_acc: \u001b[31m0.8332404621271221\u001b[0m, time: 52.41\n",
      "best_acc: 0.8332404621271221\n",
      "epoch: 71, loss: 97.00196, loss1: 0.73794, loss2_3: 96.26402\n",
      "\ttrain_acc: 0.8374, test_acc: \u001b[31m0.8333704818158179\u001b[0m, time: 52.46\n",
      "best_acc: 0.8333704818158179\n",
      "epoch: 72, loss: 97.09941, loss1: 0.73551, loss2_3: 96.36390\n",
      "\ttrain_acc: 0.8373, test_acc: \u001b[31m0.8319588394814071\u001b[0m, time: 52.42\n",
      "epoch: 73, loss: 96.99932, loss1: 0.73519, loss2_3: 96.26413\n",
      "\ttrain_acc: 0.8367, test_acc: \u001b[31m0.8308443850068725\u001b[0m, time: 52.47\n",
      "epoch: 74, loss: 96.99363, loss1: 0.73887, loss2_3: 96.25477\n",
      "\ttrain_acc: 0.8380, test_acc: \u001b[31m0.8321260076525874\u001b[0m, time: 52.50\n",
      "epoch: 75, loss: 96.86909, loss1: 0.73622, loss2_3: 96.13286\n",
      "\ttrain_acc: 0.8374, test_acc: \u001b[31m0.8314759092091089\u001b[0m, time: 52.49\n",
      "epoch: 76, loss: 96.78281, loss1: 0.73841, loss2_3: 96.04440\n",
      "\ttrain_acc: 0.8383, test_acc: \u001b[31m0.8328318288197927\u001b[0m, time: 52.47\n",
      "epoch: 77, loss: 96.85661, loss1: 0.73925, loss2_3: 96.11735\n",
      "\ttrain_acc: 0.8386, test_acc: \u001b[31m0.8326460864073703\u001b[0m, time: 52.46\n",
      "epoch: 78, loss: 96.73911, loss1: 0.73664, loss2_3: 96.00247\n",
      "\ttrain_acc: 0.8382, test_acc: \u001b[31m0.8334262045395445\u001b[0m, time: 52.45\n",
      "best_acc: 0.8334262045395445\n",
      "epoch: 79, loss: 96.75872, loss1: 0.73514, loss2_3: 96.02358\n",
      "\ttrain_acc: 0.8379, test_acc: \u001b[31m0.8320888591701029\u001b[0m, time: 52.51\n",
      "epoch: 80, loss: 96.72022, loss1: 0.73443, loss2_3: 95.98579\n",
      "\ttrain_acc: 0.8381, test_acc: \u001b[31m0.8328875515435195\u001b[0m, time: 52.46\n",
      "epoch: 81, loss: 96.75742, loss1: 0.73646, loss2_3: 96.02096\n",
      "\ttrain_acc: 0.8385, test_acc: \u001b[31m0.8325903636836435\u001b[0m, time: 52.45\n",
      "epoch: 82, loss: 96.68767, loss1: 0.73763, loss2_3: 95.95003\n",
      "\ttrain_acc: 0.8384, test_acc: \u001b[31m0.8329247000260039\u001b[0m, time: 52.51\n",
      "epoch: 83, loss: 96.53015, loss1: 0.73509, loss2_3: 95.79506\n",
      "\ttrain_acc: 0.8379, test_acc: \u001b[31m0.8319031167576805\u001b[0m, time: 55.91\n",
      "epoch: 84, loss: 96.70912, loss1: 0.73314, loss2_3: 95.97598\n",
      "\ttrain_acc: 0.8391, test_acc: \u001b[31m0.8325346409599168\u001b[0m, time: 55.95\n",
      "epoch: 85, loss: 96.25515, loss1: 0.73506, loss2_3: 95.52009\n",
      "\ttrain_acc: 0.8380, test_acc: \u001b[31m0.8309186819718415\u001b[0m, time: 52.54\n",
      "epoch: 86, loss: 96.36998, loss1: 0.73472, loss2_3: 95.63526\n",
      "\ttrain_acc: 0.8380, test_acc: \u001b[31m0.8333519075745756\u001b[0m, time: 52.50\n",
      "epoch: 87, loss: 96.28254, loss1: 0.73235, loss2_3: 95.55019\n",
      "\ttrain_acc: 0.8389, test_acc: \u001b[31m0.8339462832943274\u001b[0m, time: 52.59\n",
      "best_acc: 0.8339462832943274\n",
      "epoch: 88, loss: 96.38784, loss1: 0.73600, loss2_3: 95.65184\n",
      "\ttrain_acc: 0.8370, test_acc: \u001b[31m0.8325903636836435\u001b[0m, time: 52.53\n",
      "epoch: 89, loss: 96.25942, loss1: 0.73455, loss2_3: 95.52487\n",
      "\ttrain_acc: 0.8392, test_acc: \u001b[31m0.8339462832943274\u001b[0m, time: 52.52\n",
      "epoch: 90, loss: 96.19857, loss1: 0.73380, loss2_3: 95.46477\n",
      "\ttrain_acc: 0.8390, test_acc: \u001b[31m0.8331475909209108\u001b[0m, time: 52.74\n",
      "epoch: 91, loss: 96.24048, loss1: 0.73324, loss2_3: 95.50725\n",
      "\ttrain_acc: 0.8369, test_acc: \u001b[31m0.8305100486645121\u001b[0m, time: 52.61\n",
      "epoch: 92, loss: 96.05423, loss1: 0.73113, loss2_3: 95.32310\n",
      "\ttrain_acc: 0.8380, test_acc: \u001b[31m0.8328875515435195\u001b[0m, time: 52.55\n",
      "epoch: 93, loss: 96.14985, loss1: 0.73317, loss2_3: 95.41668\n",
      "\ttrain_acc: 0.8402, test_acc: \u001b[31m0.832850403061035\u001b[0m, time: 52.56\n",
      "epoch: 94, loss: 96.18420, loss1: 0.73408, loss2_3: 95.45012\n",
      "\ttrain_acc: 0.8371, test_acc: \u001b[31m0.8316245031390468\u001b[0m, time: 52.52\n",
      "epoch: 95, loss: 96.06983, loss1: 0.73140, loss2_3: 95.33844\n",
      "\ttrain_acc: 0.8397, test_acc: \u001b[31m0.8334076302983023\u001b[0m, time: 52.59\n",
      "epoch: 96, loss: 96.09753, loss1: 0.73265, loss2_3: 95.36487\n",
      "\ttrain_acc: 0.8389, test_acc: \u001b[31m0.8327389576135815\u001b[0m, time: 52.53\n",
      "epoch: 97, loss: 96.02184, loss1: 0.73480, loss2_3: 95.28704\n",
      "\ttrain_acc: 0.8385, test_acc: \u001b[31m0.8322374531000408\u001b[0m, time: 52.53\n",
      "epoch: 98, loss: 96.04901, loss1: 0.72863, loss2_3: 95.32038\n",
      "\ttrain_acc: 0.8391, test_acc: \u001b[31m0.8332033136446376\u001b[0m, time: 53.09\n",
      "epoch: 99, loss: 96.08040, loss1: 0.73323, loss2_3: 95.34717\n",
      "\ttrain_acc: 0.8391, test_acc: \u001b[31m0.832701809131097\u001b[0m, time: 57.32\n",
      "epoch: 100, loss: 95.86203, loss1: 0.73122, loss2_3: 95.13081\n",
      "\ttrain_acc: 0.8406, test_acc: \u001b[31m0.8337233923994205\u001b[0m, time: 57.77\n",
      "epoch: 101, loss: 95.90877, loss1: 0.73228, loss2_3: 95.17648\n",
      "\ttrain_acc: 0.8405, test_acc: \u001b[31m0.8342434711542034\u001b[0m, time: 57.81\n",
      "best_acc: 0.8342434711542034\n",
      "epoch: 102, loss: 95.78635, loss1: 0.73068, loss2_3: 95.05568\n",
      "\ttrain_acc: 0.8402, test_acc: \u001b[31m0.8324046212712211\u001b[0m, time: 57.73\n",
      "epoch: 103, loss: 95.91354, loss1: 0.72974, loss2_3: 95.18380\n",
      "\ttrain_acc: 0.8396, test_acc: \u001b[31m0.8307515138006613\u001b[0m, time: 57.77\n",
      "epoch: 104, loss: 95.93679, loss1: 0.73209, loss2_3: 95.20469\n",
      "\ttrain_acc: 0.8406, test_acc: \u001b[31m0.8328689773022772\u001b[0m, time: 59.33\n",
      "epoch: 105, loss: 95.84766, loss1: 0.72887, loss2_3: 95.11879\n",
      "\ttrain_acc: 0.8401, test_acc: \u001b[31m0.8331290166796687\u001b[0m, time: 57.75\n",
      "epoch: 106, loss: 95.62153, loss1: 0.73064, loss2_3: 94.89089\n",
      "\ttrain_acc: 0.8392, test_acc: \u001b[31m0.8321445818938297\u001b[0m, time: 57.80\n",
      "epoch: 107, loss: 95.71000, loss1: 0.73074, loss2_3: 94.97926\n",
      "\ttrain_acc: 0.8387, test_acc: \u001b[31m0.8311972955904752\u001b[0m, time: 57.76\n",
      "epoch: 108, loss: 95.61592, loss1: 0.73237, loss2_3: 94.88355\n",
      "\ttrain_acc: 0.8394, test_acc: \u001b[31m0.8319774137226494\u001b[0m, time: 53.67\n",
      "epoch: 109, loss: 95.50032, loss1: 0.72998, loss2_3: 94.77034\n",
      "\ttrain_acc: 0.8413, test_acc: \u001b[31m0.8334819272632713\u001b[0m, time: 52.47\n",
      "epoch: 110, loss: 95.48872, loss1: 0.72780, loss2_3: 94.76092\n",
      "\ttrain_acc: 0.8412, test_acc: \u001b[31m0.833686243916936\u001b[0m, time: 52.53\n",
      "epoch: 111, loss: 95.64311, loss1: 0.72733, loss2_3: 94.91578\n",
      "\ttrain_acc: 0.8361, test_acc: \u001b[31m0.8285783275753186\u001b[0m, time: 52.40\n",
      "epoch: 112, loss: 95.43595, loss1: 0.72778, loss2_3: 94.70816\n",
      "\ttrain_acc: 0.8406, test_acc: \u001b[31m0.8336490954344515\u001b[0m, time: 52.47\n",
      "epoch: 113, loss: 95.65360, loss1: 0.72934, loss2_3: 94.92426\n",
      "\ttrain_acc: 0.8407, test_acc: \u001b[31m0.8334447787807868\u001b[0m, time: 52.53\n",
      "epoch: 114, loss: 95.50430, loss1: 0.72881, loss2_3: 94.77550\n",
      "\ttrain_acc: 0.8390, test_acc: \u001b[31m0.8308443850068725\u001b[0m, time: 52.52\n",
      "epoch: 115, loss: 95.56274, loss1: 0.72758, loss2_3: 94.83515\n",
      "\ttrain_acc: 0.8418, test_acc: \u001b[31m0.833834837846874\u001b[0m, time: 52.42\n",
      "epoch: 116, loss: 95.46252, loss1: 0.72914, loss2_3: 94.73339\n",
      "\ttrain_acc: 0.8423, test_acc: \u001b[31m0.8340205802592964\u001b[0m, time: 53.25\n",
      "epoch: 117, loss: 95.48120, loss1: 0.73222, loss2_3: 94.74897\n",
      "\ttrain_acc: 0.8422, test_acc: \u001b[31m0.8340205802592964\u001b[0m, time: 54.24\n",
      "epoch: 118, loss: 95.40636, loss1: 0.72932, loss2_3: 94.67704\n",
      "\ttrain_acc: 0.8405, test_acc: \u001b[31m0.8322746015825253\u001b[0m, time: 52.61\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 119, loss: 95.33489, loss1: 0.72742, loss2_3: 94.60747\n",
      "\ttrain_acc: 0.8414, test_acc: \u001b[31m0.833463353022029\u001b[0m, time: 52.35\n",
      "epoch: 120, loss: 95.33681, loss1: 0.73146, loss2_3: 94.60535\n",
      "\ttrain_acc: 0.8420, test_acc: \u001b[31m0.8334447787807868\u001b[0m, time: 52.36\n",
      "epoch: 121, loss: 95.09032, loss1: 0.72746, loss2_3: 94.36286\n",
      "\ttrain_acc: 0.8417, test_acc: \u001b[31m0.8340020060180542\u001b[0m, time: 52.36\n",
      "epoch: 122, loss: 95.23723, loss1: 0.72858, loss2_3: 94.50865\n",
      "\ttrain_acc: 0.8420, test_acc: \u001b[31m0.8338719863293584\u001b[0m, time: 52.43\n",
      "epoch: 123, loss: 95.05418, loss1: 0.72834, loss2_3: 94.32585\n",
      "\ttrain_acc: 0.8394, test_acc: \u001b[31m0.8307143653181768\u001b[0m, time: 52.34\n",
      "epoch: 124, loss: 95.18640, loss1: 0.72980, loss2_3: 94.45660\n",
      "\ttrain_acc: 0.8427, test_acc: \u001b[31m0.8339277090530852\u001b[0m, time: 52.39\n",
      "epoch: 125, loss: 95.14426, loss1: 0.72512, loss2_3: 94.41913\n",
      "\ttrain_acc: 0.8421, test_acc: \u001b[31m0.8337791151231472\u001b[0m, time: 52.68\n",
      "epoch: 126, loss: 95.11126, loss1: 0.72836, loss2_3: 94.38290\n",
      "\ttrain_acc: 0.8417, test_acc: \u001b[31m0.8322188788587986\u001b[0m, time: 54.37\n",
      "epoch: 127, loss: 95.04984, loss1: 0.72629, loss2_3: 94.32355\n",
      "\ttrain_acc: 0.8413, test_acc: \u001b[31m0.832701809131097\u001b[0m, time: 54.25\n",
      "epoch: 128, loss: 95.08507, loss1: 0.72945, loss2_3: 94.35562\n",
      "\ttrain_acc: 0.8426, test_acc: \u001b[31m0.83338905605706\u001b[0m, time: 54.26\n",
      "epoch: 129, loss: 94.94318, loss1: 0.72702, loss2_3: 94.21616\n",
      "\ttrain_acc: 0.8428, test_acc: \u001b[31m0.8339277090530852\u001b[0m, time: 54.25\n",
      "epoch: 130, loss: 94.97938, loss1: 0.72799, loss2_3: 94.25139\n",
      "\ttrain_acc: 0.8407, test_acc: \u001b[31m0.8312901667966863\u001b[0m, time: 54.35\n",
      "epoch: 131, loss: 94.89668, loss1: 0.72523, loss2_3: 94.17145\n",
      "\ttrain_acc: 0.8412, test_acc: \u001b[31m0.831643077380289\u001b[0m, time: 54.25\n",
      "epoch: 132, loss: 94.76573, loss1: 0.72548, loss2_3: 94.04025\n",
      "\ttrain_acc: 0.8406, test_acc: \u001b[31m0.8306400683532078\u001b[0m, time: 54.28\n",
      "epoch: 133, loss: 94.81209, loss1: 0.72743, loss2_3: 94.08466\n",
      "\ttrain_acc: 0.8431, test_acc: \u001b[31m0.8332776106096066\u001b[0m, time: 53.25\n",
      "epoch: 134, loss: 94.82476, loss1: 0.73003, loss2_3: 94.09473\n",
      "\ttrain_acc: 0.8368, test_acc: \u001b[31m0.8286154760578031\u001b[0m, time: 56.41\n",
      "epoch: 135, loss: 94.90576, loss1: 0.72674, loss2_3: 94.17902\n",
      "\ttrain_acc: 0.8433, test_acc: \u001b[31m0.8330547197146997\u001b[0m, time: 54.56\n",
      "epoch: 136, loss: 94.77706, loss1: 0.72646, loss2_3: 94.05060\n",
      "\ttrain_acc: 0.8418, test_acc: \u001b[31m0.8332961848508489\u001b[0m, time: 52.47\n",
      "epoch: 137, loss: 94.82577, loss1: 0.72436, loss2_3: 94.10141\n",
      "\ttrain_acc: 0.8410, test_acc: \u001b[31m0.831940265240165\u001b[0m, time: 52.44\n",
      "epoch: 138, loss: 94.80904, loss1: 0.72380, loss2_3: 94.08524\n",
      "\ttrain_acc: 0.8437, test_acc: \u001b[31m0.834373490842899\u001b[0m, time: 52.52\n",
      "best_acc: 0.834373490842899\n",
      "epoch: 139, loss: 94.75224, loss1: 0.72575, loss2_3: 94.02649\n",
      "\ttrain_acc: 0.8431, test_acc: \u001b[31m0.8353579256287381\u001b[0m, time: 52.47\n",
      "best_acc: 0.8353579256287381\n",
      "epoch: 140, loss: 94.69401, loss1: 0.72627, loss2_3: 93.96774\n",
      "\ttrain_acc: 0.8434, test_acc: \u001b[31m0.8340391545005387\u001b[0m, time: 53.08\n",
      "epoch: 141, loss: 94.67321, loss1: 0.72680, loss2_3: 93.94642\n",
      "\ttrain_acc: 0.8435, test_acc: \u001b[31m0.833463353022029\u001b[0m, time: 54.33\n",
      "epoch: 142, loss: 94.56387, loss1: 0.72655, loss2_3: 93.83732\n",
      "\ttrain_acc: 0.8433, test_acc: \u001b[31m0.8348192726327129\u001b[0m, time: 54.33\n",
      "epoch: 143, loss: 94.61810, loss1: 0.72550, loss2_3: 93.89260\n",
      "\ttrain_acc: 0.8434, test_acc: \u001b[31m0.83247891823619\u001b[0m, time: 54.30\n",
      "epoch: 144, loss: 94.68913, loss1: 0.72737, loss2_3: 93.96177\n",
      "\ttrain_acc: 0.8435, test_acc: \u001b[31m0.8333704818158179\u001b[0m, time: 54.31\n",
      "epoch: 145, loss: 94.52749, loss1: 0.72626, loss2_3: 93.80123\n",
      "\ttrain_acc: 0.8433, test_acc: \u001b[31m0.83338905605706\u001b[0m, time: 53.52\n",
      "epoch: 146, loss: 94.55204, loss1: 0.72409, loss2_3: 93.82795\n",
      "\ttrain_acc: 0.8446, test_acc: \u001b[31m0.8337419666406627\u001b[0m, time: 52.47\n",
      "epoch: 147, loss: 94.54032, loss1: 0.72320, loss2_3: 93.81712\n",
      "\ttrain_acc: 0.8430, test_acc: \u001b[31m0.832776106096066\u001b[0m, time: 52.36\n",
      "epoch: 148, loss: 94.55565, loss1: 0.72449, loss2_3: 93.83116\n",
      "\ttrain_acc: 0.8434, test_acc: \u001b[31m0.8342620453954456\u001b[0m, time: 52.42\n",
      "epoch: 149, loss: 94.39657, loss1: 0.72179, loss2_3: 93.67478\n",
      "\ttrain_acc: 0.8398, test_acc: \u001b[31m0.8315502061740778\u001b[0m, time: 52.43\n",
      "epoch: 150, loss: 94.33267, loss1: 0.72525, loss2_3: 93.60742\n",
      "\ttrain_acc: 0.8433, test_acc: \u001b[31m0.8332590363683644\u001b[0m, time: 52.39\n",
      "epoch: 151, loss: 94.39896, loss1: 0.72387, loss2_3: 93.67510\n",
      "\ttrain_acc: 0.8432, test_acc: \u001b[31m0.8343549166016568\u001b[0m, time: 52.40\n",
      "epoch: 152, loss: 94.38510, loss1: 0.72175, loss2_3: 93.66336\n",
      "\ttrain_acc: 0.8445, test_acc: \u001b[31m0.8344292135666258\u001b[0m, time: 52.41\n",
      "epoch: 153, loss: 94.42646, loss1: 0.72458, loss2_3: 93.70188\n",
      "\ttrain_acc: 0.8437, test_acc: \u001b[31m0.833686243916936\u001b[0m, time: 52.41\n",
      "epoch: 154, loss: 94.30263, loss1: 0.72541, loss2_3: 93.57721\n",
      "\ttrain_acc: 0.8433, test_acc: \u001b[31m0.8329618485084884\u001b[0m, time: 52.41\n",
      "epoch: 155, loss: 94.30482, loss1: 0.72341, loss2_3: 93.58141\n",
      "\ttrain_acc: 0.8412, test_acc: \u001b[31m0.8308629592481147\u001b[0m, time: 52.39\n",
      "epoch: 156, loss: 94.35579, loss1: 0.72302, loss2_3: 93.63277\n",
      "\ttrain_acc: 0.8400, test_acc: \u001b[31m0.8298970987035179\u001b[0m, time: 52.41\n",
      "epoch: 157, loss: 94.20632, loss1: 0.71859, loss2_3: 93.48773\n",
      "\ttrain_acc: 0.8444, test_acc: \u001b[31m0.8335747984694826\u001b[0m, time: 52.41\n",
      "epoch: 158, loss: 94.22488, loss1: 0.72421, loss2_3: 93.50068\n",
      "\ttrain_acc: 0.8439, test_acc: \u001b[31m0.8345778074965637\u001b[0m, time: 52.43\n",
      "epoch: 159, loss: 94.23811, loss1: 0.72281, loss2_3: 93.51530\n",
      "\ttrain_acc: 0.8428, test_acc: \u001b[31m0.831865968275196\u001b[0m, time: 52.38\n",
      "epoch: 160, loss: 94.13886, loss1: 0.72085, loss2_3: 93.41801\n",
      "\ttrain_acc: 0.8439, test_acc: \u001b[31m0.8335190757457558\u001b[0m, time: 52.44\n",
      "epoch: 161, loss: 94.12059, loss1: 0.72058, loss2_3: 93.40001\n",
      "\ttrain_acc: 0.8444, test_acc: \u001b[31m0.8338905605706007\u001b[0m, time: 52.38\n",
      "epoch: 162, loss: 94.27898, loss1: 0.72450, loss2_3: 93.55448\n",
      "\ttrain_acc: 0.8447, test_acc: \u001b[31m0.8324046212712211\u001b[0m, time: 52.43\n",
      "epoch: 163, loss: 94.06173, loss1: 0.72174, loss2_3: 93.34000\n",
      "\ttrain_acc: 0.8440, test_acc: \u001b[31m0.8324231955124634\u001b[0m, time: 52.38\n",
      "epoch: 164, loss: 94.02340, loss1: 0.72127, loss2_3: 93.30213\n",
      "\ttrain_acc: 0.8444, test_acc: \u001b[31m0.8327203833723392\u001b[0m, time: 52.46\n",
      "epoch: 165, loss: 94.00985, loss1: 0.72213, loss2_3: 93.28771\n",
      "\ttrain_acc: 0.8451, test_acc: \u001b[31m0.8337419666406627\u001b[0m, time: 52.49\n",
      "epoch: 166, loss: 93.98303, loss1: 0.72110, loss2_3: 93.26193\n",
      "\ttrain_acc: 0.8432, test_acc: \u001b[31m0.8322374531000408\u001b[0m, time: 52.49\n",
      "epoch: 167, loss: 94.04860, loss1: 0.72422, loss2_3: 93.32438\n",
      "\ttrain_acc: 0.8451, test_acc: \u001b[31m0.8326089379248858\u001b[0m, time: 52.50\n",
      "epoch: 168, loss: 93.86122, loss1: 0.71985, loss2_3: 93.14137\n",
      "\ttrain_acc: 0.8450, test_acc: \u001b[31m0.8324974924774323\u001b[0m, time: 52.58\n",
      "epoch: 169, loss: 94.09880, loss1: 0.72348, loss2_3: 93.37532\n",
      "\ttrain_acc: 0.8432, test_acc: \u001b[31m0.8323860470299789\u001b[0m, time: 52.42\n",
      "epoch: 170, loss: 93.81921, loss1: 0.72143, loss2_3: 93.09777\n",
      "\ttrain_acc: 0.8451, test_acc: \u001b[31m0.8331104424384264\u001b[0m, time: 52.46\n",
      "epoch: 171, loss: 93.88815, loss1: 0.71927, loss2_3: 93.16888\n",
      "\ttrain_acc: 0.8440, test_acc: \u001b[31m0.8319774137226494\u001b[0m, time: 52.46\n",
      "epoch: 172, loss: 94.01088, loss1: 0.72232, loss2_3: 93.28856\n",
      "\ttrain_acc: 0.8452, test_acc: \u001b[31m0.8333704818158179\u001b[0m, time: 52.47\n",
      "epoch: 173, loss: 94.14170, loss1: 0.72343, loss2_3: 93.41827\n",
      "\ttrain_acc: 0.8447, test_acc: \u001b[31m0.8331104424384264\u001b[0m, time: 52.52\n",
      "epoch: 174, loss: 93.97086, loss1: 0.72015, loss2_3: 93.25071\n",
      "\ttrain_acc: 0.8441, test_acc: \u001b[31m0.8323674727887366\u001b[0m, time: 52.48\n",
      "epoch: 175, loss: 93.74365, loss1: 0.71897, loss2_3: 93.02468\n",
      "\ttrain_acc: 0.8437, test_acc: \u001b[31m0.8323117500650098\u001b[0m, time: 52.45\n",
      "epoch: 176, loss: 93.83653, loss1: 0.71949, loss2_3: 93.11704\n",
      "\ttrain_acc: 0.8454, test_acc: \u001b[31m0.8328132545785505\u001b[0m, time: 52.54\n",
      "epoch: 177, loss: 93.86746, loss1: 0.72258, loss2_3: 93.14488\n",
      "\ttrain_acc: 0.8448, test_acc: \u001b[31m0.8326832348898547\u001b[0m, time: 52.47\n",
      "epoch: 178, loss: 93.80838, loss1: 0.72259, loss2_3: 93.08578\n",
      "\ttrain_acc: 0.8452, test_acc: \u001b[31m0.8330547197146997\u001b[0m, time: 52.46\n",
      "epoch: 179, loss: 93.71669, loss1: 0.71902, loss2_3: 92.99767\n",
      "\ttrain_acc: 0.8452, test_acc: \u001b[31m0.8327389576135815\u001b[0m, time: 52.42\n",
      "epoch: 180, loss: 93.66033, loss1: 0.72206, loss2_3: 92.93827\n",
      "\ttrain_acc: 0.8462, test_acc: \u001b[31m0.8335747984694826\u001b[0m, time: 52.47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 181, loss: 93.72417, loss1: 0.72062, loss2_3: 93.00355\n",
      "\ttrain_acc: 0.8459, test_acc: \u001b[31m0.8329247000260039\u001b[0m, time: 52.43\n",
      "epoch: 182, loss: 93.65479, loss1: 0.71982, loss2_3: 92.93498\n",
      "\ttrain_acc: 0.8460, test_acc: \u001b[31m0.8332218878858798\u001b[0m, time: 52.41\n",
      "epoch: 183, loss: 93.58247, loss1: 0.72006, loss2_3: 92.86241\n",
      "\ttrain_acc: 0.8429, test_acc: \u001b[31m0.8301199895984249\u001b[0m, time: 52.40\n",
      "epoch: 184, loss: 93.58211, loss1: 0.71739, loss2_3: 92.86472\n",
      "\ttrain_acc: 0.8465, test_acc: \u001b[31m0.8331847394033953\u001b[0m, time: 52.47\n",
      "epoch: 185, loss: 93.47166, loss1: 0.71804, loss2_3: 92.75362\n",
      "\ttrain_acc: 0.8460, test_acc: \u001b[31m0.8339462832943274\u001b[0m, time: 52.41\n",
      "epoch: 186, loss: 93.43593, loss1: 0.71773, loss2_3: 92.71821\n",
      "\ttrain_acc: 0.8464, test_acc: \u001b[31m0.8341134514655076\u001b[0m, time: 52.53\n",
      "epoch: 187, loss: 93.60323, loss1: 0.72176, loss2_3: 92.88148\n",
      "\ttrain_acc: 0.8440, test_acc: \u001b[31m0.8302500092871207\u001b[0m, time: 52.55\n",
      "epoch: 188, loss: 93.46640, loss1: 0.71807, loss2_3: 92.74833\n",
      "\ttrain_acc: 0.8458, test_acc: \u001b[31m0.8342248969129611\u001b[0m, time: 52.52\n",
      "epoch: 189, loss: 93.42841, loss1: 0.71722, loss2_3: 92.71120\n",
      "\ttrain_acc: 0.8440, test_acc: \u001b[31m0.8309744046955682\u001b[0m, time: 52.51\n",
      "epoch: 190, loss: 93.55653, loss1: 0.71895, loss2_3: 92.83758\n",
      "\ttrain_acc: 0.8452, test_acc: \u001b[31m0.8321631561350719\u001b[0m, time: 52.50\n",
      "epoch: 191, loss: 93.56081, loss1: 0.71908, loss2_3: 92.84173\n",
      "\ttrain_acc: 0.8456, test_acc: \u001b[31m0.8334447787807868\u001b[0m, time: 52.47\n",
      "epoch: 192, loss: 93.52119, loss1: 0.71786, loss2_3: 92.80333\n",
      "\ttrain_acc: 0.8436, test_acc: \u001b[31m0.831940265240165\u001b[0m, time: 52.58\n",
      "epoch: 193, loss: 93.29860, loss1: 0.71859, loss2_3: 92.58001\n",
      "\ttrain_acc: 0.8456, test_acc: \u001b[31m0.833760540881905\u001b[0m, time: 52.46\n",
      "epoch: 194, loss: 93.44741, loss1: 0.71769, loss2_3: 92.72972\n",
      "\ttrain_acc: 0.8473, test_acc: \u001b[31m0.8354322225937071\u001b[0m, time: 52.51\n",
      "best_acc: 0.8354322225937071\n",
      "epoch: 195, loss: 93.45271, loss1: 0.71793, loss2_3: 92.73477\n",
      "\ttrain_acc: 0.8465, test_acc: \u001b[31m0.8354879453174338\u001b[0m, time: 57.76\n",
      "best_acc: 0.8354879453174338\n",
      "epoch: 196, loss: 93.39852, loss1: 0.72036, loss2_3: 92.67815\n",
      "\ttrain_acc: 0.8466, test_acc: \u001b[31m0.8330918681971842\u001b[0m, time: 59.44\n",
      "epoch: 197, loss: 93.36438, loss1: 0.71727, loss2_3: 92.64710\n",
      "\ttrain_acc: 0.8461, test_acc: \u001b[31m0.8320517106876184\u001b[0m, time: 59.41\n",
      "epoch: 198, loss: 93.26634, loss1: 0.71555, loss2_3: 92.55080\n",
      "\ttrain_acc: 0.8473, test_acc: \u001b[31m0.8335005015045135\u001b[0m, time: 57.75\n",
      "epoch: 199, loss: 93.16812, loss1: 0.71839, loss2_3: 92.44974\n",
      "\ttrain_acc: 0.8462, test_acc: \u001b[31m0.8337791151231472\u001b[0m, time: 53.70\n",
      "epoch: 200, loss: 93.32385, loss1: 0.71912, loss2_3: 92.60472\n",
      "\ttrain_acc: 0.8434, test_acc: \u001b[31m0.8305100486645121\u001b[0m, time: 52.46\n",
      "epoch: 201, loss: 93.12757, loss1: 0.71628, loss2_3: 92.41129\n",
      "\ttrain_acc: 0.8474, test_acc: \u001b[31m0.8335376499869981\u001b[0m, time: 52.41\n",
      "epoch: 202, loss: 93.07766, loss1: 0.71832, loss2_3: 92.35934\n",
      "\ttrain_acc: 0.8459, test_acc: \u001b[31m0.8325346409599168\u001b[0m, time: 52.40\n",
      "epoch: 203, loss: 93.28048, loss1: 0.71825, loss2_3: 92.56223\n",
      "\ttrain_acc: 0.8467, test_acc: \u001b[31m0.8336305211932092\u001b[0m, time: 52.41\n",
      "epoch: 204, loss: 93.10541, loss1: 0.71775, loss2_3: 92.38765\n",
      "\ttrain_acc: 0.8482, test_acc: \u001b[31m0.833760540881905\u001b[0m, time: 52.43\n",
      "epoch: 205, loss: 93.03826, loss1: 0.71687, loss2_3: 92.32139\n",
      "\ttrain_acc: 0.8462, test_acc: \u001b[31m0.8326646606486126\u001b[0m, time: 52.41\n",
      "epoch: 206, loss: 93.06372, loss1: 0.71697, loss2_3: 92.34675\n",
      "\ttrain_acc: 0.8480, test_acc: \u001b[31m0.8333333333333334\u001b[0m, time: 52.42\n",
      "epoch: 207, loss: 93.01958, loss1: 0.71231, loss2_3: 92.30727\n",
      "\ttrain_acc: 0.8471, test_acc: \u001b[31m0.8333147590920911\u001b[0m, time: 52.40\n",
      "epoch: 208, loss: 93.07243, loss1: 0.71460, loss2_3: 92.35784\n",
      "\ttrain_acc: 0.8474, test_acc: \u001b[31m0.8325903636836435\u001b[0m, time: 52.48\n",
      "epoch: 209, loss: 92.91583, loss1: 0.71373, loss2_3: 92.20209\n",
      "\ttrain_acc: 0.8470, test_acc: \u001b[31m0.8325903636836435\u001b[0m, time: 52.39\n",
      "epoch: 210, loss: 92.90214, loss1: 0.71743, loss2_3: 92.18472\n",
      "\ttrain_acc: 0.8457, test_acc: \u001b[31m0.8314201864853821\u001b[0m, time: 52.47\n",
      "epoch: 211, loss: 92.93850, loss1: 0.71723, loss2_3: 92.22127\n",
      "\ttrain_acc: 0.8476, test_acc: \u001b[31m0.8340391545005387\u001b[0m, time: 52.48\n",
      "epoch: 212, loss: 92.93066, loss1: 0.71419, loss2_3: 92.21647\n",
      "\ttrain_acc: 0.8479, test_acc: \u001b[31m0.8344477878078681\u001b[0m, time: 52.45\n",
      "epoch: 213, loss: 93.02541, loss1: 0.71597, loss2_3: 92.30945\n",
      "\ttrain_acc: 0.8472, test_acc: \u001b[31m0.834373490842899\u001b[0m, time: 52.39\n",
      "epoch: 214, loss: 92.88344, loss1: 0.71415, loss2_3: 92.16930\n",
      "\ttrain_acc: 0.8466, test_acc: \u001b[31m0.832776106096066\u001b[0m, time: 52.39\n",
      "epoch: 215, loss: 92.88472, loss1: 0.71684, loss2_3: 92.16788\n",
      "\ttrain_acc: 0.8467, test_acc: \u001b[31m0.8314387607266244\u001b[0m, time: 52.40\n",
      "epoch: 216, loss: 92.79627, loss1: 0.71498, loss2_3: 92.08129\n",
      "\ttrain_acc: 0.8471, test_acc: \u001b[31m0.8333333333333334\u001b[0m, time: 52.48\n",
      "epoch: 217, loss: 92.61399, loss1: 0.71271, loss2_3: 91.90128\n",
      "\ttrain_acc: 0.8486, test_acc: \u001b[31m0.8340577287417809\u001b[0m, time: 52.43\n",
      "epoch: 218, loss: 92.69980, loss1: 0.71482, loss2_3: 91.98499\n",
      "\ttrain_acc: 0.8474, test_acc: \u001b[31m0.833686243916936\u001b[0m, time: 52.52\n",
      "epoch: 219, loss: 92.60337, loss1: 0.71409, loss2_3: 91.88929\n",
      "\ttrain_acc: 0.8475, test_acc: \u001b[31m0.8342806196366879\u001b[0m, time: 52.51\n",
      "epoch: 220, loss: 92.68709, loss1: 0.71474, loss2_3: 91.97235\n",
      "\ttrain_acc: 0.8490, test_acc: \u001b[31m0.8336490954344515\u001b[0m, time: 52.49\n",
      "epoch: 221, loss: 92.80565, loss1: 0.71380, loss2_3: 92.09186\n",
      "\ttrain_acc: 0.8474, test_acc: \u001b[31m0.8345406590140793\u001b[0m, time: 52.45\n",
      "epoch: 222, loss: 92.74655, loss1: 0.71472, loss2_3: 92.03183\n",
      "\ttrain_acc: 0.8499, test_acc: \u001b[31m0.834596381737806\u001b[0m, time: 52.47\n",
      "epoch: 223, loss: 92.68037, loss1: 0.71243, loss2_3: 91.96794\n",
      "\ttrain_acc: 0.8484, test_acc: \u001b[31m0.8330361454734574\u001b[0m, time: 52.38\n",
      "epoch: 224, loss: 92.65444, loss1: 0.71269, loss2_3: 91.94176\n",
      "\ttrain_acc: 0.8454, test_acc: \u001b[31m0.8305100486645121\u001b[0m, time: 52.47\n",
      "epoch: 225, loss: 92.67102, loss1: 0.71383, loss2_3: 91.95719\n",
      "\ttrain_acc: 0.8474, test_acc: \u001b[31m0.8336305211932092\u001b[0m, time: 52.44\n",
      "epoch: 226, loss: 92.63501, loss1: 0.71421, loss2_3: 91.92080\n",
      "\ttrain_acc: 0.8484, test_acc: \u001b[31m0.8335190757457558\u001b[0m, time: 52.44\n",
      "epoch: 227, loss: 92.64542, loss1: 0.71797, loss2_3: 91.92744\n",
      "\ttrain_acc: 0.8489, test_acc: \u001b[31m0.8337419666406627\u001b[0m, time: 52.46\n",
      "epoch: 228, loss: 92.61348, loss1: 0.71316, loss2_3: 91.90033\n",
      "\ttrain_acc: 0.8483, test_acc: \u001b[31m0.8338534120881163\u001b[0m, time: 52.44\n",
      "epoch: 229, loss: 92.54264, loss1: 0.71419, loss2_3: 91.82845\n",
      "\ttrain_acc: 0.8478, test_acc: \u001b[31m0.8327203833723392\u001b[0m, time: 52.32\n",
      "epoch: 230, loss: 92.48453, loss1: 0.71097, loss2_3: 91.77356\n",
      "\ttrain_acc: 0.8481, test_acc: \u001b[31m0.8343177681191724\u001b[0m, time: 52.34\n",
      "epoch: 231, loss: 92.37837, loss1: 0.71201, loss2_3: 91.66636\n",
      "\ttrain_acc: 0.8493, test_acc: \u001b[31m0.83338905605706\u001b[0m, time: 52.37\n",
      "epoch: 232, loss: 92.48307, loss1: 0.71461, loss2_3: 91.76846\n",
      "\ttrain_acc: 0.8447, test_acc: \u001b[31m0.8292470002600394\u001b[0m, time: 52.48\n",
      "epoch: 233, loss: 92.53990, loss1: 0.71495, loss2_3: 91.82495\n",
      "\ttrain_acc: 0.8450, test_acc: \u001b[31m0.8285040306103496\u001b[0m, time: 52.49\n",
      "epoch: 234, loss: 92.25991, loss1: 0.71206, loss2_3: 91.54785\n",
      "\ttrain_acc: 0.8489, test_acc: \u001b[31m0.8329989969909729\u001b[0m, time: 52.54\n",
      "epoch: 235, loss: 92.32300, loss1: 0.71270, loss2_3: 91.61030\n",
      "\ttrain_acc: 0.8491, test_acc: \u001b[31m0.8322931758237676\u001b[0m, time: 52.43\n",
      "epoch: 236, loss: 92.31842, loss1: 0.71439, loss2_3: 91.60403\n",
      "\ttrain_acc: 0.8496, test_acc: \u001b[31m0.8335005015045135\u001b[0m, time: 52.36\n",
      "epoch: 237, loss: 92.21382, loss1: 0.71278, loss2_3: 91.50104\n",
      "\ttrain_acc: 0.8481, test_acc: \u001b[31m0.8336676696756937\u001b[0m, time: 52.44\n",
      "epoch: 238, loss: 92.24204, loss1: 0.71472, loss2_3: 91.52732\n",
      "\ttrain_acc: 0.8499, test_acc: \u001b[31m0.8340391545005387\u001b[0m, time: 52.46\n",
      "epoch: 239, loss: 92.22173, loss1: 0.71222, loss2_3: 91.50951\n",
      "\ttrain_acc: 0.8495, test_acc: \u001b[31m0.8335376499869981\u001b[0m, time: 52.47\n",
      "epoch: 240, loss: 92.16162, loss1: 0.71355, loss2_3: 91.44807\n",
      "\ttrain_acc: 0.8489, test_acc: \u001b[31m0.8332404621271221\u001b[0m, time: 52.48\n",
      "epoch: 241, loss: 91.99868, loss1: 0.71037, loss2_3: 91.28831\n",
      "\ttrain_acc: 0.8507, test_acc: \u001b[31m0.833834837846874\u001b[0m, time: 52.44\n",
      "epoch: 242, loss: 92.30057, loss1: 0.71210, loss2_3: 91.58847\n",
      "\ttrain_acc: 0.8488, test_acc: \u001b[31m0.8337048181581782\u001b[0m, time: 52.47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 243, loss: 92.16288, loss1: 0.71311, loss2_3: 91.44977\n",
      "\ttrain_acc: 0.8502, test_acc: \u001b[31m0.8332033136446376\u001b[0m, time: 52.51\n",
      "epoch: 244, loss: 92.16328, loss1: 0.71116, loss2_3: 91.45212\n",
      "\ttrain_acc: 0.8505, test_acc: \u001b[31m0.8340020060180542\u001b[0m, time: 52.44\n",
      "epoch: 245, loss: 92.10089, loss1: 0.71129, loss2_3: 91.38961\n",
      "\ttrain_acc: 0.8499, test_acc: \u001b[31m0.8326832348898547\u001b[0m, time: 52.40\n",
      "epoch: 246, loss: 92.10872, loss1: 0.71049, loss2_3: 91.39823\n",
      "\ttrain_acc: 0.8497, test_acc: \u001b[31m0.8335376499869981\u001b[0m, time: 52.52\n",
      "epoch: 247, loss: 92.10155, loss1: 0.71293, loss2_3: 91.38862\n",
      "\ttrain_acc: 0.8496, test_acc: \u001b[31m0.8324046212712211\u001b[0m, time: 52.41\n",
      "epoch: 248, loss: 91.97077, loss1: 0.71179, loss2_3: 91.25898\n",
      "\ttrain_acc: 0.8486, test_acc: \u001b[31m0.8322560273412831\u001b[0m, time: 52.44\n",
      "epoch: 249, loss: 91.87891, loss1: 0.70966, loss2_3: 91.16925\n",
      "\ttrain_acc: 0.8483, test_acc: \u001b[31m0.8313830380028976\u001b[0m, time: 52.43\n",
      "epoch: 250, loss: 91.99666, loss1: 0.71218, loss2_3: 91.28448\n",
      "\ttrain_acc: 0.8507, test_acc: \u001b[31m0.8332218878858798\u001b[0m, time: 52.43\n"
     ]
    }
   ],
   "source": [
    "for num_model in range(1):  # just one train\n",
    "    net=newModel().to(device)\n",
    "    lr = 0.0001\n",
    "    \n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr,weight_decay=5e-4)\n",
    "    criterion = ContrastiveLoss()\n",
    "    criterion_model = nn.CrossEntropyLoss(reduction='sum')\n",
    "    \n",
    "    best_acc=0\n",
    "    EPOCH=250\n",
    "    for epoch in range(EPOCH):\n",
    "        loss_ls=[]\n",
    "        loss1_ls=[]\n",
    "        loss2_3_ls=[]\n",
    "        t0=time.time()\n",
    "        net.train()\n",
    "        for seq1,seq2,label,label1,label2 in train_iter_cont:\n",
    "                output1=net(seq1)\n",
    "                output2=net(seq2)\n",
    "                output3=net.trainModel(seq1)\n",
    "                output4=net.trainModel(seq2)\n",
    "                \n",
    "                loss1=criterion(output1, output2, label)\n",
    "                loss2=criterion_model(output3,label1)\n",
    "                loss3=criterion_model(output4,label2)\n",
    "                loss=loss1+loss2+loss3\n",
    "    #             print(loss)\n",
    "                optimizer.zero_grad() \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                loss_ls.append(loss.item())\n",
    "                loss1_ls.append(loss1.item())\n",
    "                loss2_3_ls.append((loss2+loss3).item())\n",
    "\n",
    "\n",
    "        net.eval() \n",
    "        with torch.no_grad(): \n",
    "            train_acc=evaluate_accuracy(train_iter,net)\n",
    "            test_acc=evaluate_accuracy(val_iter,net)\n",
    "            \n",
    "        results=f\"epoch: {epoch+1}, loss: {np.mean(loss_ls):.5f}, loss1: {np.mean(loss1_ls):.5f}, loss2_3: {np.mean(loss2_3_ls):.5f}\\n\"\n",
    "        results+=f'\\ttrain_acc: {train_acc:.4f}, test_acc: {colored(test_acc,\"red\")}, time: {time.time()-t0:.2f}'\n",
    "        print(results)\n",
    "        to_log(results)\n",
    "        if test_acc>best_acc:\n",
    "            best_acc=test_acc\n",
    "            torch.save({\"best_acc\":best_acc,\"model\":net.state_dict()},f'compareModel/2021ACS_PepFormer/Model/{num_model}.pl')\n",
    "            print(f\"best_acc: {best_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T01:36:17.469265Z",
     "start_time": "2021-08-31T01:36:17.433429Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T02:00:14.432919Z",
     "start_time": "2021-08-31T02:00:14.429626Z"
    }
   },
   "outputs": [],
   "source": [
    "def pred(data_iter, net):\n",
    "    y_pred = []\n",
    "    for x, y in data_iter:\n",
    "        x,y=x.to(device),y.to(device)\n",
    "        outputs=net.trainModel(x)\n",
    "        for _ in outputs.argmax(dim=1):\n",
    "            y_pred.append(int(_))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T02:15:44.725526Z",
     "start_time": "2021-08-31T02:15:44.722214Z"
    }
   },
   "outputs": [],
   "source": [
    "def pred_prob(data_iter, net):\n",
    "    y_pred = []\n",
    "    for x, y in data_iter:\n",
    "        x,y=x.to(device),y.to(device)\n",
    "        outputs=net.trainModel(x)\n",
    "        for _ in outputs:\n",
    "            y_pred.append(list(map(float, _)))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T01:43:19.844962Z",
     "start_time": "2021-08-31T01:43:17.303611Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.832639900145621\n"
     ]
    }
   ],
   "source": [
    "test_acc = evaluate_accuracy(test_iter,net)\n",
    "print('Test Accuracy: {}'.format(test_acc))\n",
    "\n",
    "# prediction\n",
    "y_pred = pred(test_iter, net)\n",
    "print(classification_report(test_label, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T02:15:52.383194Z",
     "start_time": "2021-08-31T02:15:46.461649Z"
    }
   },
   "outputs": [],
   "source": [
    "# AUC\n",
    "probs = pred_prob(test_iter, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T03:04:08.336765Z",
     "start_time": "2021-08-31T03:04:08.305720Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3.5717508792877197, -0.768585205078125],\n",
       " [4.277831077575684, -1.2637131214141846],\n",
       " [-1.3084465265274048, 1.2894442081451416],\n",
       " [-0.3948383629322052, 0.6678199768066406],\n",
       " [-1.1757993698120117, 0.9540104866027832],\n",
       " [3.397508144378662, -0.7419110536575317],\n",
       " [-1.059721827507019, 0.547842800617218],\n",
       " [2.4729669094085693, -0.5109972953796387],\n",
       " [-1.04049813747406, 1.47611403465271],\n",
       " [4.409674167633057, -1.2055996656417847],\n",
       " [0.06145118921995163, 0.4930192828178406],\n",
       " [-0.5151676535606384, 0.815303385257721],\n",
       " [0.15468844771385193, 0.15777094662189484],\n",
       " [0.39211246371269226, 0.2917012572288513],\n",
       " [-1.320945143699646, 1.35123610496521],\n",
       " [-1.1329760551452637, 1.308112621307373],\n",
       " [0.2669239938259125, 0.3428284823894501],\n",
       " [-1.2966212034225464, 1.6092833280563354],\n",
       " [3.7887861728668213, -0.9150843620300293],\n",
       " [-1.1075180768966675, 1.4790408611297607],\n",
       " [-1.280547022819519, 1.3432307243347168],\n",
       " [1.2611033916473389, -0.22941510379314423],\n",
       " [-0.7845730185508728, 1.1495734453201294],\n",
       " [3.40763521194458, -0.7575128674507141],\n",
       " [0.8373018503189087, -0.07220391929149628],\n",
       " [0.9762983322143555, 0.08469706773757935],\n",
       " [0.13925164937973022, 0.5309916734695435],\n",
       " [3.13737154006958, -0.6263453960418701],\n",
       " [-0.6469212174415588, 0.9467688798904419],\n",
       " [3.6986913681030273, -1.0030386447906494],\n",
       " [-0.7101163268089294, 1.0961236953735352],\n",
       " [2.9555563926696777, -0.5994091629981995],\n",
       " [-0.471279501914978, 0.6483691930770874],\n",
       " [0.5597541332244873, 0.3161604404449463],\n",
       " [-1.201227068901062, 1.660539150238037],\n",
       " [1.149226188659668, 0.08029372990131378],\n",
       " [-1.1079057455062866, 0.9854441285133362],\n",
       " [3.007485866546631, -0.9521466493606567],\n",
       " [1.7342756986618042, -0.18879415094852448],\n",
       " [-1.1622254848480225, 1.027858853340149],\n",
       " [1.60464346408844, -0.1648809164762497],\n",
       " [2.3100967407226562, -0.701789140701294],\n",
       " [-1.0155296325683594, 1.230812668800354],\n",
       " [3.4127607345581055, -0.7685592174530029],\n",
       " [-0.6336107850074768, 0.6576189994812012],\n",
       " [-1.1282845735549927, 1.5194246768951416],\n",
       " [0.6222929358482361, 0.1525774449110031],\n",
       " [1.893196940422058, -0.3918161988258362],\n",
       " [-0.7823783755302429, 1.0914565324783325],\n",
       " [-0.27424874901771545, 0.9397855997085571],\n",
       " [0.22201165556907654, 0.3336779773235321],\n",
       " [-0.35120147466659546, 0.5844739079475403],\n",
       " [-0.6683455109596252, 0.6062473058700562],\n",
       " [3.8059329986572266, -0.9869353771209717],\n",
       " [-1.221321940422058, 1.3564962148666382],\n",
       " [-0.9708492159843445, 1.2459275722503662],\n",
       " [0.8698599934577942, 0.07466832548379898],\n",
       " [-1.5300002098083496, 1.405430793762207],\n",
       " [4.0036702156066895, -1.1424729824066162],\n",
       " [-1.1186494827270508, 1.253894567489624],\n",
       " [-0.21579191088676453, 0.7279998064041138],\n",
       " [-0.5240694880485535, 0.5180383920669556],\n",
       " [-1.1682748794555664, 1.5305625200271606],\n",
       " [1.6079670190811157, -0.18496204912662506],\n",
       " [0.698914647102356, -0.24747495353221893],\n",
       " [-1.5794209241867065, 1.470426082611084],\n",
       " [4.019944190979004, -0.9963064193725586],\n",
       " [1.6351085901260376, -0.16188345849514008],\n",
       " [-1.3270546197891235, 1.499856948852539],\n",
       " [3.003551959991455, -0.6052177548408508],\n",
       " [-0.8197687268257141, 1.0005656480789185],\n",
       " [-0.514884889125824, 0.8674254417419434],\n",
       " [-1.196529507637024, 0.8619027137756348],\n",
       " [0.9022889137268066, -0.1269228607416153],\n",
       " [-1.5994077920913696, 1.324904203414917],\n",
       " [-0.5453400611877441, 0.7476853132247925],\n",
       " [3.983785629272461, -1.0982754230499268],\n",
       " [-1.188836932182312, 1.4447455406188965],\n",
       " [2.884824275970459, -0.5712525844573975],\n",
       " [4.355086803436279, -1.2058069705963135],\n",
       " [1.746864676475525, -0.6281753778457642],\n",
       " [-1.344159483909607, 1.4760793447494507],\n",
       " [-0.7414317727088928, 0.716801643371582],\n",
       " [4.345819473266602, -1.169762372970581],\n",
       " [-1.3439239263534546, 1.7017147541046143],\n",
       " [1.397565245628357, 0.05937504768371582],\n",
       " [-0.9362675547599792, 1.3166348934173584],\n",
       " [3.2534093856811523, -0.8177480697631836],\n",
       " [0.17071738839149475, 0.2969653606414795],\n",
       " [-0.9120654463768005, 0.4935460686683655],\n",
       " [1.0879117250442505, -0.15619073808193207],\n",
       " [-1.2709053754806519, 1.2948992252349854],\n",
       " [-0.9773431420326233, 1.0027369260787964],\n",
       " [3.1967310905456543, -1.0240590572357178],\n",
       " [3.917689323425293, -1.0397377014160156],\n",
       " [2.368436813354492, -0.5063892006874084],\n",
       " [0.5291796326637268, 0.14844197034835815],\n",
       " [3.7861990928649902, -0.8214731216430664],\n",
       " [0.13432031869888306, 0.3837323784828186],\n",
       " [-0.6579623222351074, 0.7660813331604004],\n",
       " [-0.6525061130523682, 0.9449614882469177],\n",
       " [1.2958272695541382, -0.2670525908470154],\n",
       " [4.211945533752441, -1.1483384370803833],\n",
       " [-1.2392791509628296, 1.5530149936676025],\n",
       " [2.3173367977142334, -0.34549611806869507],\n",
       " [-0.03499015048146248, 0.267659455537796],\n",
       " [-0.7171451449394226, 0.5902353525161743],\n",
       " [3.3019514083862305, -1.0201910734176636],\n",
       " [2.8415255546569824, -0.5731394290924072],\n",
       " [0.14685356616973877, 0.18032974004745483],\n",
       " [-0.8483656048774719, 0.9825057983398438],\n",
       " [-0.7723654508590698, 0.8026295900344849],\n",
       " [0.6021900177001953, -0.23610486090183258],\n",
       " [-0.8608174324035645, 1.105861783027649],\n",
       " [0.9979022145271301, 0.19062988460063934],\n",
       " [0.9632050395011902, 0.06983146071434021],\n",
       " [0.4045063257217407, 0.1796875298023224],\n",
       " [-1.1578880548477173, 1.0938775539398193],\n",
       " [-1.202065110206604, 1.2547739744186401],\n",
       " [3.9952330589294434, -1.1113440990447998],\n",
       " [-1.1538076400756836, 1.4130444526672363],\n",
       " [-0.8321968913078308, 0.9996553659439087],\n",
       " [-0.49292415380477905, 0.47535961866378784],\n",
       " [5.137462139129639, -1.1072921752929688],\n",
       " [2.8839619159698486, -0.8044058084487915],\n",
       " [3.4769999980926514, -0.7764915227890015],\n",
       " [0.6006373763084412, 0.3700758218765259],\n",
       " [3.0609517097473145, -0.5063489675521851],\n",
       " [2.5743894577026367, -0.530767560005188],\n",
       " [0.5566907525062561, 0.40427345037460327],\n",
       " [-1.29417085647583, 1.102640151977539],\n",
       " [0.381988525390625, 0.4171902537345886],\n",
       " [-0.7989938259124756, 1.2753416299819946],\n",
       " [1.2789236307144165, -0.21939142048358917],\n",
       " [0.9842610955238342, 0.04263301566243172],\n",
       " [-1.043340802192688, 1.3724520206451416],\n",
       " [-1.1898527145385742, 1.4977567195892334],\n",
       " [3.709486484527588, -0.9872934818267822],\n",
       " [3.2145514488220215, -0.8729695081710815],\n",
       " [-0.12926605343818665, 0.5331472754478455],\n",
       " [0.2116968035697937, 0.0030133798718452454],\n",
       " [3.7564549446105957, -0.26957541704177856],\n",
       " [-0.8233678936958313, 1.243091106414795],\n",
       " [-0.07444560527801514, 0.5543712973594666],\n",
       " [0.07199262827634811, 0.5222690105438232],\n",
       " [-1.3411496877670288, 0.941802978515625],\n",
       " [4.2910566329956055, -1.0619642734527588],\n",
       " [-0.8976870775222778, 1.224292278289795],\n",
       " [-1.4300613403320312, 1.2718737125396729],\n",
       " [2.1365010738372803, -0.13195744156837463],\n",
       " [-1.0410354137420654, 1.3825161457061768],\n",
       " [-1.405637502670288, 1.6365387439727783],\n",
       " [2.133554220199585, -0.48904550075531006],\n",
       " [1.1911427974700928, -0.2526245713233948],\n",
       " [-0.38643911480903625, 0.49985575675964355],\n",
       " [1.6737641096115112, -0.213826522231102],\n",
       " [3.3701577186584473, -0.5265402793884277],\n",
       " [3.516857385635376, -0.8113189935684204],\n",
       " [-1.140647292137146, 1.3783276081085205],\n",
       " [4.359186172485352, -1.0244367122650146],\n",
       " [1.6604971885681152, -0.2510308623313904],\n",
       " [4.026425838470459, -1.1318044662475586],\n",
       " [3.664553165435791, -0.6850883960723877],\n",
       " [-0.9703623056411743, 1.4474241733551025],\n",
       " [3.4904470443725586, -0.9805692434310913],\n",
       " [1.5830637216567993, -0.455322802066803],\n",
       " [-0.7944315671920776, 1.2396972179412842],\n",
       " [-1.567966341972351, 1.4136066436767578],\n",
       " [-0.1971931755542755, 0.7416888475418091],\n",
       " [1.1150020360946655, 0.02163827419281006],\n",
       " [0.06583259999752045, 0.1311080902814865],\n",
       " [0.6747179627418518, 0.2719182074069977],\n",
       " [4.06593132019043, -1.0404928922653198],\n",
       " [4.2205891609191895, -1.0779465436935425],\n",
       " [-1.273023247718811, 1.351694941520691],\n",
       " [0.1606549322605133, 0.4529910087585449],\n",
       " [1.455193042755127, -0.195550337433815],\n",
       " [-1.0105229616165161, 1.2349305152893066],\n",
       " [-1.068458914756775, 1.4456795454025269],\n",
       " [-1.0289790630340576, 1.1479840278625488],\n",
       " [4.3938984870910645, -1.2550888061523438],\n",
       " [-0.031185902655124664, 0.10554514825344086],\n",
       " [0.7444118857383728, -0.021515145897865295],\n",
       " [-1.3378585577011108, 1.4249120950698853],\n",
       " [-0.11651364713907242, 0.3212968111038208],\n",
       " [-0.8749290704727173, 1.256291389465332],\n",
       " [0.47974979877471924, 0.178524449467659],\n",
       " [2.291393995285034, -0.4040282368659973],\n",
       " [3.9910051822662354, -1.1323764324188232],\n",
       " [2.4846129417419434, -0.206678107380867],\n",
       " [-0.9896958470344543, 1.2117547988891602],\n",
       " [-1.1466950178146362, 0.8612319231033325],\n",
       " [-1.1338655948638916, 1.3927080631256104],\n",
       " [-1.4412521123886108, 1.4508674144744873],\n",
       " [0.6168172359466553, 0.5202733278274536],\n",
       " [-1.3201136589050293, 1.435448169708252],\n",
       " [-0.11225680261850357, 0.8414038419723511],\n",
       " [2.517371892929077, -0.3304462432861328],\n",
       " [1.1136943101882935, -0.13602133095264435],\n",
       " [-0.7915442585945129, 0.688001275062561],\n",
       " [1.0085082054138184, -0.021707937121391296],\n",
       " [0.3391200602054596, 0.2020726054906845],\n",
       " [1.1662529706954956, -0.109815314412117],\n",
       " [3.8131542205810547, -0.9807379245758057],\n",
       " [0.38689881563186646, 0.20994964241981506],\n",
       " [0.08911127597093582, 0.611098051071167],\n",
       " [4.152514457702637, -1.2696619033813477],\n",
       " [0.5126069784164429, 0.151528000831604],\n",
       " [-1.0039925575256348, 1.2184059619903564],\n",
       " [1.1371921300888062, 0.1799127757549286],\n",
       " [-1.1116228103637695, 1.0723903179168701],\n",
       " [0.05991289019584656, 0.42348700761795044],\n",
       " [-0.9611273407936096, 1.4840067625045776],\n",
       " [-0.2406514286994934, 0.41141390800476074],\n",
       " [0.30245617032051086, 0.19633477926254272],\n",
       " [-0.7581287026405334, 0.910149872303009],\n",
       " [0.0772223025560379, 0.5022485852241516],\n",
       " [-1.2921217679977417, 1.0349093675613403],\n",
       " [4.360283374786377, -1.0873522758483887],\n",
       " [-0.9647685885429382, 1.1982030868530273],\n",
       " [-0.4664981961250305, 0.7708209156990051],\n",
       " [-1.1791304349899292, 0.8532782793045044],\n",
       " [2.2191519737243652, -0.6940329074859619],\n",
       " [3.494375228881836, -0.9657998085021973],\n",
       " [-0.2659282088279724, 0.7500417232513428],\n",
       " [-1.3719857931137085, 1.6521077156066895],\n",
       " [3.988208532333374, -0.8855879902839661],\n",
       " [-0.7632223963737488, 0.6502374410629272],\n",
       " [-1.0475234985351562, 1.2708450555801392],\n",
       " [-1.09055757522583, 1.1428375244140625],\n",
       " [-0.03024671971797943, 0.7234234809875488],\n",
       " [0.6563042402267456, 0.06363722681999207],\n",
       " [0.0018954873085021973, 0.030654430389404297],\n",
       " [1.305769681930542, -0.04021714627742767],\n",
       " [3.980630874633789, -1.199821949005127],\n",
       " [3.269235372543335, -0.9075669050216675],\n",
       " [-1.3693041801452637, 1.1596845388412476],\n",
       " [1.6863566637039185, -0.27473318576812744],\n",
       " [2.859121799468994, -0.9556959867477417],\n",
       " [-1.3566316366195679, 1.5841927528381348],\n",
       " [-1.2569562196731567, 1.4562418460845947],\n",
       " [3.415198802947998, -0.9371496438980103],\n",
       " [2.779664993286133, -0.3449028730392456],\n",
       " [-0.3129523992538452, 1.144813060760498],\n",
       " [-0.5727505087852478, 0.4706111550331116],\n",
       " [-1.3609122037887573, 1.389899492263794],\n",
       " [1.9768751859664917, -0.5559358596801758],\n",
       " [-0.5710270404815674, 0.7849360108375549],\n",
       " [1.0862271785736084, 0.069277822971344],\n",
       " [0.30282342433929443, 0.48195314407348633],\n",
       " [-1.2175122499465942, 1.324364185333252],\n",
       " [1.2259985208511353, -0.04604075849056244],\n",
       " [-1.2846176624298096, 1.3520296812057495],\n",
       " [1.6695154905319214, -0.233705535531044],\n",
       " [4.15565299987793, -1.0615344047546387],\n",
       " [1.1167348623275757, -0.23786453902721405],\n",
       " [-0.4394860863685608, 0.7453794479370117],\n",
       " [3.8760766983032227, -1.0993863344192505],\n",
       " [-0.48693645000457764, 1.0128239393234253],\n",
       " [-1.4726718664169312, 1.6195341348648071],\n",
       " [-1.2858868837356567, 1.1518161296844482],\n",
       " [1.7069941759109497, -0.22882871329784393],\n",
       " [-1.1085420846939087, 1.1730338335037231],\n",
       " [-1.4270493984222412, 1.557901382446289],\n",
       " [3.0012807846069336, -0.6895443201065063],\n",
       " [-0.7334261536598206, 0.9755207300186157],\n",
       " [3.7598986625671387, -1.203992247581482],\n",
       " [-1.0131354331970215, 1.2961206436157227],\n",
       " [2.557821750640869, -0.157271608710289],\n",
       " [3.5780189037323, -0.8075807690620422],\n",
       " [4.3927836418151855, -1.184876799583435],\n",
       " [1.5230833292007446, -0.058320850133895874],\n",
       " [-1.178286075592041, 1.3495714664459229],\n",
       " [3.6342082023620605, -0.9340145587921143],\n",
       " [-1.2862114906311035, 1.3503799438476562],\n",
       " [0.6703364253044128, 0.5964555740356445],\n",
       " [-0.9659174084663391, 1.3406559228897095],\n",
       " [-0.9487574696540833, 0.8892507553100586],\n",
       " [3.5073063373565674, -0.6691783666610718],\n",
       " [-1.1639755964279175, 1.0103991031646729],\n",
       " [-0.6528190970420837, 0.9197679758071899],\n",
       " [-1.1910263299942017, 1.744128942489624],\n",
       " [0.018498849123716354, 0.6205734610557556],\n",
       " [-0.18524935841560364, 0.7115768194198608],\n",
       " [-0.19800904393196106, 0.548466145992279],\n",
       " [2.589141607284546, -0.7237913608551025],\n",
       " [-1.5134543180465698, 1.4075000286102295],\n",
       " [3.5481598377227783, -0.8243995308876038],\n",
       " [4.154023170471191, -1.0778188705444336],\n",
       " [1.3086663484573364, 0.24204275012016296],\n",
       " [3.286133289337158, -1.0091160535812378],\n",
       " [-1.0476444959640503, 1.192787528038025],\n",
       " [-0.012070909142494202, 0.5087579488754272],\n",
       " [-1.460262656211853, 1.6023207902908325],\n",
       " [-0.02626003697514534, 0.4641619920730591],\n",
       " [0.7152655124664307, -0.04599599540233612],\n",
       " [3.6044986248016357, -1.0950324535369873],\n",
       " [3.546464681625366, -0.9207011461257935],\n",
       " [0.6125632524490356, -0.0009034126996994019],\n",
       " [0.33273616433143616, 0.524881899356842],\n",
       " [-1.0588728189468384, 1.416576862335205],\n",
       " [-0.9409692883491516, 1.0932294130325317],\n",
       " [0.3971261978149414, 0.14568448066711426],\n",
       " [-0.27877283096313477, 0.8580695986747742],\n",
       " [-0.9419222474098206, 1.3309643268585205],\n",
       " [1.1267447471618652, -0.23328323662281036],\n",
       " [0.8496884107589722, 0.12825587391853333],\n",
       " [4.704291343688965, -1.2241525650024414],\n",
       " [0.38215726613998413, 0.08791722357273102],\n",
       " [2.415083646774292, -0.4118587374687195],\n",
       " [-0.7333301901817322, 1.1878669261932373],\n",
       " [-1.671413540840149, 1.244199275970459],\n",
       " [1.2647258043289185, -0.5757530331611633],\n",
       " [4.452789783477783, -1.2132441997528076],\n",
       " [-0.5663242936134338, 0.7747998237609863],\n",
       " [0.46630072593688965, -0.09466692805290222],\n",
       " [-0.7533326148986816, 0.8332569599151611],\n",
       " [3.272778034210205, -0.6969345808029175],\n",
       " [3.6415374279022217, -0.9149556159973145],\n",
       " [3.1215832233428955, -0.7570750713348389],\n",
       " [3.018944263458252, -0.3305155038833618],\n",
       " [-0.37167030572891235, 1.1606297492980957],\n",
       " [-1.04472815990448, 1.4806327819824219],\n",
       " [-1.2552803754806519, 1.432837724685669],\n",
       " [-0.9688512682914734, 1.193487524986267],\n",
       " [2.7038185596466064, -0.5069288015365601],\n",
       " [2.1058056354522705, -0.3934823274612427],\n",
       " [-1.5036091804504395, 1.5117013454437256],\n",
       " [3.084033966064453, -0.6477996110916138],\n",
       " [-0.5365212559700012, 0.6288616061210632],\n",
       " [1.7762256860733032, -0.5596641302108765],\n",
       " [-0.727860152721405, 1.31996750831604],\n",
       " [-1.0352548360824585, 1.5295352935791016],\n",
       " [-1.2352683544158936, 1.4774737358093262],\n",
       " [-0.24763867259025574, 0.47747963666915894],\n",
       " [3.961775541305542, -0.9505139589309692],\n",
       " [-0.994720995426178, 0.9534294009208679],\n",
       " [-1.0474082231521606, 1.4636465311050415],\n",
       " [-0.9651868343353271, 1.151120662689209],\n",
       " [-1.3884180784225464, 1.47013521194458],\n",
       " [-0.22434455156326294, 0.6699191331863403],\n",
       " [-1.097589373588562, 0.9848891496658325],\n",
       " [3.805379629135132, -0.9904682636260986],\n",
       " [0.6803232431411743, -0.09867659211158752],\n",
       " [4.326530933380127, -1.0630097389221191],\n",
       " [2.3689088821411133, -0.2773740887641907],\n",
       " [-0.1451500952243805, 0.6301537752151489],\n",
       " [0.8032332062721252, -0.13651731610298157],\n",
       " [-0.18274551630020142, 0.8158553838729858],\n",
       " [-1.0306004285812378, 1.5926830768585205],\n",
       " [-0.231673002243042, 0.5411063432693481],\n",
       " [3.6458187103271484, -1.0814675092697144],\n",
       " [-0.8019118905067444, 1.2364943027496338],\n",
       " [-1.2416030168533325, 1.5309271812438965],\n",
       " [4.0320892333984375, -1.1224322319030762],\n",
       " [3.6538915634155273, -0.6972905397415161],\n",
       " [0.1749650239944458, -0.012489177286624908],\n",
       " [-0.9883629679679871, 0.5881273746490479],\n",
       " [3.427203416824341, -0.7244851589202881],\n",
       " [0.4627147912979126, 0.04774756729602814],\n",
       " [0.988299548625946, -0.15933839976787567],\n",
       " [0.8903650641441345, 0.033562153577804565],\n",
       " [2.613908052444458, -0.6301866769790649],\n",
       " [-1.1747325658798218, 1.248389482498169],\n",
       " [-0.7427142262458801, 1.1553962230682373],\n",
       " [2.453341484069824, -0.4639614224433899],\n",
       " [0.5358229875564575, 0.2413962185382843],\n",
       " [0.9153275489807129, -0.2010120302438736],\n",
       " [1.928501009941101, -0.2472958117723465],\n",
       " [-0.8120822906494141, 1.5253634452819824],\n",
       " [0.47755783796310425, 0.532894492149353],\n",
       " [-0.4773961901664734, 0.4929502606391907],\n",
       " [-1.2703946828842163, 1.5046052932739258],\n",
       " [2.9477057456970215, -0.46872657537460327],\n",
       " [-0.8289187550544739, 0.42381614446640015],\n",
       " [4.120778560638428, -1.134950876235962],\n",
       " [-0.9752817749977112, 1.2080168724060059],\n",
       " [4.769867897033691, -1.0157499313354492],\n",
       " [1.095254898071289, -0.16837485134601593],\n",
       " [0.9256232380867004, -0.08307196199893951],\n",
       " [2.1229560375213623, -0.3083203434944153],\n",
       " [1.1931780576705933, -0.05047158896923065],\n",
       " [1.100401520729065, 0.15307343006134033],\n",
       " [2.2541873455047607, -0.3957807421684265],\n",
       " [4.658056259155273, -0.9230682849884033],\n",
       " [-1.252238392829895, 1.2659072875976562],\n",
       " [-1.2378333806991577, 0.9189422130584717],\n",
       " [0.2870546579360962, 0.02535487711429596],\n",
       " [0.020645834505558014, 0.24952833354473114],\n",
       " [-1.0730746984481812, 1.0279312133789062],\n",
       " [1.5222053527832031, -0.10181556642055511],\n",
       " [5.103909492492676, -1.2108508348464966],\n",
       " [0.6789175868034363, 0.5559176206588745],\n",
       " [-1.097015142440796, 1.4835273027420044],\n",
       " [4.296441078186035, -0.9634082317352295],\n",
       " [-0.04123842716217041, 0.305519700050354],\n",
       " [1.309705138206482, -0.35806775093078613],\n",
       " [-1.0527108907699585, 0.8092113733291626],\n",
       " [1.118994116783142, -0.36647796630859375],\n",
       " [-0.8700255155563354, 1.018481969833374],\n",
       " [1.8810349702835083, -0.47256267070770264],\n",
       " [0.012310363352298737, 0.2571049928665161],\n",
       " [-0.2554932236671448, 0.8851578235626221],\n",
       " [0.1377076804637909, 0.2791843116283417],\n",
       " [3.797501802444458, -1.0661461353302002],\n",
       " [-0.9878333210945129, 1.0608241558074951],\n",
       " [-0.07078257948160172, 0.016171962022781372],\n",
       " [2.354524850845337, -0.27899396419525146],\n",
       " [-1.0280500650405884, 1.4217522144317627],\n",
       " [3.262587308883667, -1.0590853691101074],\n",
       " [0.7541646361351013, -0.11297912895679474],\n",
       " [-0.8846234679222107, 1.2165974378585815],\n",
       " [3.9407641887664795, -1.1826351881027222],\n",
       " [-1.0362290143966675, 0.8400446772575378],\n",
       " [1.222368597984314, -0.19458334147930145],\n",
       " [1.9666653871536255, -0.3975159525871277],\n",
       " [-1.36589515209198, 1.646289587020874],\n",
       " [-0.138179749250412, 0.8415798544883728],\n",
       " [3.4020018577575684, -0.9980213642120361],\n",
       " [-1.0152171850204468, 1.1395779848098755],\n",
       " [1.0214009284973145, -0.46137678623199463],\n",
       " [-1.0978176593780518, 1.3062169551849365],\n",
       " [0.2989264130592346, 0.39083898067474365],\n",
       " [-0.4283762276172638, 0.5738506317138672],\n",
       " [-1.0243724584579468, 0.5355392098426819],\n",
       " [0.3747054636478424, 0.6505913734436035],\n",
       " [1.5622608661651611, -0.49504756927490234],\n",
       " [0.9030117988586426, -0.45423728227615356],\n",
       " [4.040501594543457, -0.9451695680618286],\n",
       " [3.118467330932617, -0.6378775835037231],\n",
       " [0.4530971348285675, 0.16869376599788666],\n",
       " [3.127706289291382, -0.6459238529205322],\n",
       " [0.5150551795959473, -0.0307571142911911],\n",
       " [2.7811365127563477, -0.715896725654602],\n",
       " [-0.9385717511177063, 1.340928554534912],\n",
       " [3.898029088973999, -1.0286977291107178],\n",
       " [0.061782728880643845, 0.8164837956428528],\n",
       " [1.3872088193893433, 0.08980757743120193],\n",
       " [1.3070064783096313, -0.30675750970840454],\n",
       " [-0.30783969163894653, 0.8335309028625488],\n",
       " [-0.48693305253982544, 1.0517438650131226],\n",
       " [-1.0640758275985718, 1.4041272401809692],\n",
       " [4.448985576629639, -1.4438855648040771],\n",
       " [-0.6012804508209229, 1.0379918813705444],\n",
       " [-1.042802095413208, 1.4635729789733887],\n",
       " [0.20595231652259827, 0.055455803871154785],\n",
       " [3.166576623916626, -0.6673481464385986],\n",
       " [1.2192044258117676, -0.39930427074432373],\n",
       " [-0.3975154459476471, 0.5704014897346497],\n",
       " [-0.9413384795188904, 1.1341291666030884],\n",
       " [3.976527214050293, -1.111358404159546],\n",
       " [-1.2323096990585327, 1.4030085802078247],\n",
       " [-1.2163482904434204, 1.257641315460205],\n",
       " [-0.6422404646873474, 1.2411391735076904],\n",
       " [-0.8567923307418823, 1.3053853511810303],\n",
       " [0.4506257176399231, -0.17684073746204376],\n",
       " [-1.2168956995010376, 1.6086359024047852],\n",
       " [-1.1806727647781372, 1.4586718082427979],\n",
       " [-0.6920306086540222, 1.1475324630737305],\n",
       " [0.2743660509586334, 0.4053693413734436],\n",
       " [-0.5477510690689087, 0.9558926820755005],\n",
       " [2.9329934120178223, -0.6034382581710815],\n",
       " [1.4907013177871704, 0.089589923620224],\n",
       " [-0.8991597294807434, 1.0917292833328247],\n",
       " [3.6954309940338135, -0.9917277097702026],\n",
       " [-1.3913651704788208, 1.2881460189819336],\n",
       " [3.0135202407836914, -0.6000562906265259],\n",
       " [2.93277645111084, -0.539519190788269],\n",
       " [-1.1871250867843628, 1.429018259048462],\n",
       " [0.1298588365316391, 0.11917744576931],\n",
       " [0.6493546366691589, 0.048893436789512634],\n",
       " [1.2037322521209717, -0.45754534006118774],\n",
       " [-0.12190935760736465, 0.5345011949539185],\n",
       " [3.4706079959869385, -0.9310784339904785],\n",
       " [-0.9704506993293762, 1.0211436748504639],\n",
       " [-0.9818907380104065, 1.2756644487380981],\n",
       " [1.6362578868865967, 0.2262004017829895],\n",
       " [-1.1114708185195923, 1.2464523315429688],\n",
       " [-1.2713291645050049, 1.5155839920043945],\n",
       " [3.733858585357666, -0.9463882446289062],\n",
       " [-0.3209940791130066, 0.22883377969264984],\n",
       " [0.042951978743076324, 0.36808884143829346],\n",
       " [1.334916353225708, 0.06383998692035675],\n",
       " [2.7470784187316895, -0.7870718836784363],\n",
       " [-1.1655917167663574, 1.2516762018203735],\n",
       " [-0.7809233069419861, 1.2941397428512573],\n",
       " [-1.2303547859191895, 1.4627063274383545],\n",
       " [4.060075759887695, -1.047067403793335],\n",
       " [0.7484961748123169, -0.2022676020860672],\n",
       " [1.2464393377304077, -0.3474047780036926],\n",
       " [-1.2198039293289185, 1.197615146636963],\n",
       " [-1.0185338258743286, 1.3560872077941895],\n",
       " [1.997904896736145, -0.3702547550201416],\n",
       " [-1.2424123287200928, 1.435164213180542],\n",
       " [1.0536415576934814, -0.025751546025276184],\n",
       " [-1.2279608249664307, 1.0239529609680176],\n",
       " [0.882887065410614, -0.21042345464229584],\n",
       " [2.229435920715332, -0.29194021224975586],\n",
       " [1.5173109769821167, -0.20636092126369476],\n",
       " [-0.9119004607200623, 1.1715235710144043],\n",
       " [1.138236165046692, -0.3137189745903015],\n",
       " [-0.9704467058181763, 1.228062391281128],\n",
       " [-1.1432230472564697, 1.4809209108352661],\n",
       " [1.3263462781906128, -0.2662208676338196],\n",
       " [-1.194179892539978, 1.2134323120117188],\n",
       " [-1.127720832824707, 1.3367620706558228],\n",
       " [-1.60224187374115, 1.4676131010055542],\n",
       " [-0.033650510013103485, 0.5649492740631104],\n",
       " [1.4617418050765991, -0.10496817529201508],\n",
       " [1.4493372440338135, -0.1079864650964737],\n",
       " [1.4160517454147339, -0.23415444791316986],\n",
       " [-0.9210107922554016, 1.263313889503479],\n",
       " [-1.3797900676727295, 1.4022362232208252],\n",
       " [-0.898952066898346, 1.2986071109771729],\n",
       " [-0.599854052066803, 0.8538289070129395],\n",
       " [0.08163471519947052, 0.5268811583518982],\n",
       " [-0.9378858208656311, 1.0141148567199707],\n",
       " [0.1999746859073639, 0.34503063559532166],\n",
       " [-0.06263177841901779, 0.4969434142112732],\n",
       " [4.308310508728027, -1.3910281658172607],\n",
       " [0.4488048851490021, 0.017560049891471863],\n",
       " [1.5029842853546143, -0.20017315447330475],\n",
       " [-1.1078534126281738, 1.4456028938293457],\n",
       " [2.0478878021240234, -0.35278671979904175],\n",
       " [3.047211170196533, -0.6150292158126831],\n",
       " [4.0560526847839355, -1.0243027210235596],\n",
       " [-1.3102428913116455, 1.4509029388427734],\n",
       " [0.8465889096260071, 0.33726829290390015],\n",
       " [-0.7004846930503845, 1.0484956502914429],\n",
       " [2.614884614944458, -0.2840893864631653],\n",
       " [4.486403465270996, -1.4197691679000854],\n",
       " [-1.0466359853744507, 1.3766018152236938],\n",
       " [-1.3715811967849731, 1.3389256000518799],\n",
       " [-0.5695395469665527, 1.0311253070831299],\n",
       " [0.6681028604507446, 0.11184339225292206],\n",
       " [-0.7613039612770081, 0.393363893032074],\n",
       " [2.1826932430267334, -0.21941067278385162],\n",
       " [2.2137291431427, -0.11732913553714752],\n",
       " [1.0409406423568726, 0.26250046491622925],\n",
       " [0.4273990988731384, 0.4218480587005615],\n",
       " [-0.9528505206108093, 1.297257423400879],\n",
       " [-0.8559610843658447, 0.7547612190246582],\n",
       " [-0.6299453377723694, 0.46968382596969604],\n",
       " [0.5611571073532104, 0.17463506758213043],\n",
       " [2.068671703338623, -0.5333808660507202],\n",
       " [3.7921977043151855, -1.1268891096115112],\n",
       " [3.8508291244506836, -1.0685781240463257],\n",
       " [-0.7758914828300476, 1.2884764671325684],\n",
       " [0.9141202569007874, -0.11289390921592712],\n",
       " [-1.1050406694412231, 1.2460572719573975],\n",
       " [-1.3979984521865845, 1.6610983610153198],\n",
       " [-1.4921289682388306, 1.2963292598724365],\n",
       " [-1.0860786437988281, 1.2317886352539062],\n",
       " [3.579948902130127, -0.7286899089813232],\n",
       " [-1.4436694383621216, 1.5433416366577148],\n",
       " [-1.1942697763442993, 1.3213601112365723],\n",
       " [2.8891773223876953, -0.5692948698997498],\n",
       " [3.757537603378296, -0.9463901519775391],\n",
       " [-1.4014934301376343, 1.7094371318817139],\n",
       " [0.0676722601056099, 0.33384478092193604],\n",
       " [-1.204870343208313, 1.3533382415771484],\n",
       " [0.21645230054855347, 0.32314103841781616],\n",
       " [-1.3026456832885742, 1.2527618408203125],\n",
       " [-0.029996566474437714, 0.6165618300437927],\n",
       " [0.04605971276760101, 0.5216962695121765],\n",
       " [0.5010353922843933, -0.05732850730419159],\n",
       " [1.6126338243484497, -0.2756696939468384],\n",
       " [0.29438453912734985, 0.870682954788208],\n",
       " [-1.2079198360443115, 1.3612284660339355],\n",
       " [-1.1563422679901123, 1.2754274606704712],\n",
       " [3.170837879180908, -0.5956918001174927],\n",
       " [-0.1683996319770813, 0.5144309997558594],\n",
       " [-0.12513220310211182, 0.6344695687294006],\n",
       " [-1.2987359762191772, 1.052493929862976],\n",
       " [1.3310714960098267, -0.10060350596904755],\n",
       " [3.582486152648926, -0.67657470703125],\n",
       " [-1.0708314180374146, 1.3879709243774414],\n",
       " [1.8821905851364136, -0.22762490808963776],\n",
       " [0.26950186491012573, 0.22386573255062103],\n",
       " [0.6023792028427124, 0.25850963592529297],\n",
       " [1.3287285566329956, -0.1334466189146042],\n",
       " [-0.7901296615600586, 1.1780424118041992],\n",
       " [-0.8945318460464478, 1.1054999828338623],\n",
       " [2.7599542140960693, -0.21526946127414703],\n",
       " [-1.505947470664978, 1.6613268852233887],\n",
       " [-0.9760436415672302, 1.2234383821487427],\n",
       " [-0.9889093041419983, 1.4577012062072754],\n",
       " [2.091289520263672, -0.3754426836967468],\n",
       " [2.5020015239715576, -0.6449403762817383],\n",
       " [3.857487678527832, -1.0959813594818115],\n",
       " [-1.5171476602554321, 1.5538814067840576],\n",
       " [0.3636168837547302, 0.22632700204849243],\n",
       " [-1.2246185541152954, 1.5942518711090088],\n",
       " [-1.4378958940505981, 1.487737774848938],\n",
       " [4.0795578956604, -1.1115449666976929],\n",
       " [-1.1068552732467651, 1.3592699766159058],\n",
       " [-0.7254090905189514, 1.1587553024291992],\n",
       " [3.183072090148926, -0.8056460618972778],\n",
       " [1.9642945528030396, -0.23022578656673431],\n",
       " [4.141008377075195, -1.017439842224121],\n",
       " [2.269780397415161, -0.1674065738916397],\n",
       " [1.2496088743209839, 0.013096071779727936],\n",
       " [-0.9319354295730591, 0.6842329502105713],\n",
       " [1.513926386833191, -0.3567710518836975],\n",
       " [-1.263378381729126, 1.215308666229248],\n",
       " [2.4489855766296387, -0.6610832214355469],\n",
       " [2.2675023078918457, -0.32948774099349976],\n",
       " [-1.1035202741622925, 1.5927860736846924],\n",
       " [3.5344338417053223, -0.7263343930244446],\n",
       " [3.646322727203369, -0.8894539475440979],\n",
       " [-1.1953117847442627, 1.4730623960494995],\n",
       " [2.910336971282959, -0.533843457698822],\n",
       " [0.09751185774803162, 0.4584183692932129],\n",
       " [-1.0601170063018799, 1.18307363986969],\n",
       " [-0.036055438220500946, 0.47486239671707153],\n",
       " [0.23631227016448975, 0.11115863919258118],\n",
       " [0.40724819898605347, 0.3536022901535034],\n",
       " [-1.0162512063980103, 0.8599319458007812],\n",
       " [-1.3047460317611694, 1.4944937229156494],\n",
       " [-0.021811559796333313, 0.537108838558197],\n",
       " [-1.2276371717453003, 1.451395869255066],\n",
       " [1.3026460409164429, -0.5045737624168396],\n",
       " [3.2277464866638184, -0.7588119506835938],\n",
       " [-0.7754481434822083, 0.5159516930580139],\n",
       " [0.7770426869392395, -0.13382209837436676],\n",
       " [-1.0487323999404907, 0.8124322891235352],\n",
       " [-1.1169317960739136, 1.5355782508850098],\n",
       " [0.30239951610565186, 0.3739612102508545],\n",
       " [1.8036093711853027, -0.03967766463756561],\n",
       " [-1.0974171161651611, 1.3027056455612183],\n",
       " [-0.2323412299156189, 0.5656508207321167],\n",
       " [3.3572161197662354, -0.8375256657600403],\n",
       " [-1.1735517978668213, 1.4192618131637573],\n",
       " [0.7286017537117004, -0.17597858607769012],\n",
       " [3.762160062789917, -0.9659097194671631],\n",
       " [1.3952686786651611, -0.07691571116447449],\n",
       " [0.7910766005516052, -0.06202678382396698],\n",
       " [-0.09843830019235611, 0.5623460412025452],\n",
       " [-0.32952553033828735, 0.6586267948150635],\n",
       " [1.4076682329177856, -0.3575841188430786],\n",
       " [-1.1159566640853882, 1.5527291297912598],\n",
       " [-0.8840637803077698, 1.4840006828308105],\n",
       " [0.7502445578575134, -0.16625548899173737],\n",
       " [3.603502035140991, -0.8809477686882019],\n",
       " [-1.3542777299880981, 1.4843357801437378],\n",
       " [0.8657159209251404, -0.22604261338710785],\n",
       " [-0.45733505487442017, 1.017127513885498],\n",
       " [0.06439638137817383, 0.2390998750925064],\n",
       " [3.658402442932129, -0.9705755710601807],\n",
       " [0.1668810248374939, 0.5444603562355042],\n",
       " [-0.8624395728111267, 1.0737751722335815],\n",
       " [0.27609410881996155, 0.10704322904348373],\n",
       " [4.20345401763916, -1.0920644998550415],\n",
       " [0.7023031711578369, 0.294407457113266],\n",
       " [1.740221381187439, -0.1746596246957779],\n",
       " [3.3442375659942627, -1.018394947052002],\n",
       " [1.2496522665023804, -0.4958779811859131],\n",
       " [1.5209726095199585, -0.24936719238758087],\n",
       " [-0.02627146989107132, 0.37937963008880615],\n",
       " [0.6983776688575745, -0.05215124785900116],\n",
       " [-1.2293339967727661, 1.4771747589111328],\n",
       " [3.5869827270507812, -0.9407352209091187],\n",
       " [3.3279733657836914, -0.71895432472229],\n",
       " [0.04914427548646927, 0.33404427766799927],\n",
       " [2.5981831550598145, -0.5792597532272339],\n",
       " [-0.7828983664512634, 1.194481372833252],\n",
       " [0.8842090964317322, 0.2782256007194519],\n",
       " [-1.1208940744400024, 1.1645846366882324],\n",
       " [-0.6953809261322021, 0.603095293045044],\n",
       " [1.9676395654678345, -0.06692267954349518],\n",
       " [-0.6494320631027222, 0.974609375],\n",
       " [4.289101600646973, -1.1402010917663574],\n",
       " [3.5553510189056396, -0.9059216976165771],\n",
       " [-0.2135365605354309, 0.8193677663803101],\n",
       " [-0.7632067799568176, 0.7710293531417847],\n",
       " [-0.8525916934013367, 0.9116976261138916],\n",
       " [4.556116104125977, -1.1942789554595947],\n",
       " [-0.44620466232299805, 0.8278065323829651],\n",
       " [0.9921687841415405, 0.3186085820198059],\n",
       " [3.934436798095703, -0.9264471530914307],\n",
       " [-0.4051898121833801, 0.7579994201660156],\n",
       " [1.4180078506469727, 0.024634815752506256],\n",
       " [-0.8608652353286743, 0.6899536848068237],\n",
       " [4.5121259689331055, -1.188037633895874],\n",
       " [0.7300353646278381, 0.09324672073125839],\n",
       " [-0.7389106154441833, 1.3019742965698242],\n",
       " [0.7648879885673523, 0.2722409963607788],\n",
       " [-0.32709792256355286, 0.40166175365448],\n",
       " [1.34390127658844, -0.22312359511852264],\n",
       " [-0.8947998881340027, 0.9880149364471436],\n",
       " [-0.8738479018211365, 0.8259317874908447],\n",
       " [2.6032543182373047, -0.3721497654914856],\n",
       " [-1.4330085515975952, 1.575185775756836],\n",
       " [-1.2013362646102905, 1.8561344146728516],\n",
       " [-1.1148980855941772, 1.3580975532531738],\n",
       " [-0.6268330216407776, 0.962313175201416],\n",
       " [-0.983206570148468, 1.2339892387390137],\n",
       " [1.1420787572860718, -0.24976105988025665],\n",
       " [0.06308042258024216, -0.205781951546669],\n",
       " [-0.9252418875694275, 1.2043604850769043],\n",
       " [1.8570247888565063, -0.15094168484210968],\n",
       " [3.2273001670837402, -0.614840567111969],\n",
       " [0.963756799697876, 0.003939978778362274],\n",
       " [0.5462982058525085, 0.005239859223365784],\n",
       " [-0.7890977263450623, 0.5741715431213379],\n",
       " [-0.9440315961837769, 1.2632559537887573],\n",
       " [0.8218325972557068, -0.05030496418476105],\n",
       " [-1.345335602760315, 1.490983009338379],\n",
       " [-0.8531447052955627, 1.0899882316589355],\n",
       " [-0.4696071445941925, 0.6711481809616089],\n",
       " [-0.9643530249595642, 1.264625072479248],\n",
       " [4.248036861419678, -1.2733187675476074],\n",
       " [0.7135688662528992, -0.11440889537334442],\n",
       " [0.29482758045196533, 0.00982491672039032],\n",
       " [3.4357528686523438, -0.9488914012908936],\n",
       " [1.8047631978988647, -0.5542972683906555],\n",
       " [1.5935598611831665, -0.18929298222064972],\n",
       " [-1.1118701696395874, 0.943280816078186],\n",
       " [-0.8337554335594177, 0.6179183125495911],\n",
       " [-0.09545661509037018, 0.3431934714317322],\n",
       " [0.257289856672287, 0.4472687840461731],\n",
       " [-0.8803600668907166, 1.0892502069473267],\n",
       " [4.209817886352539, -1.1298452615737915],\n",
       " [0.6470757126808167, 0.17089436948299408],\n",
       " [-1.3093411922454834, 1.4864890575408936],\n",
       " [-0.404766708612442, 0.8526144027709961],\n",
       " [1.6714942455291748, -0.15040560066699982],\n",
       " [-0.29450973868370056, 0.302310585975647],\n",
       " [0.4114142656326294, 0.10982626676559448],\n",
       " [-1.2155743837356567, 1.6116647720336914],\n",
       " [1.1933656930923462, 0.023426175117492676],\n",
       " [-1.0147291421890259, 1.1103465557098389],\n",
       " [-0.3461717367172241, 0.7059712409973145],\n",
       " [-0.9878881573677063, 0.5433309674263],\n",
       " [2.9165680408477783, -0.424859881401062],\n",
       " [-1.086036205291748, 1.08559250831604],\n",
       " [0.4746234714984894, -0.08553220331668854],\n",
       " [-1.263311505317688, 1.3355419635772705],\n",
       " [2.948145627975464, -0.27882009744644165],\n",
       " [-0.8151083588600159, 0.6303723454475403],\n",
       " [1.7611409425735474, -0.3897159695625305],\n",
       " [-0.24060273170471191, 0.5707359910011292],\n",
       " [-0.8951215744018555, 1.0303833484649658],\n",
       " [4.68233585357666, -1.3559436798095703],\n",
       " [0.2988818287849426, -0.17249120771884918],\n",
       " [4.055695533752441, -1.1001726388931274],\n",
       " [4.177116870880127, -1.2618324756622314],\n",
       " [0.27802881598472595, 0.17045751214027405],\n",
       " [-0.693631649017334, 0.8759461641311646],\n",
       " [-1.2446595430374146, 1.4960510730743408],\n",
       " [3.901350259780884, -1.1956446170806885],\n",
       " [3.5107498168945312, -0.9950661659240723],\n",
       " [3.741823673248291, -0.9537492990493774],\n",
       " [-0.23849910497665405, 0.33775076270103455],\n",
       " [-1.3832577466964722, 1.3355966806411743],\n",
       " [-0.8474863171577454, 0.8776842951774597],\n",
       " [-0.7291088700294495, 1.144364356994629],\n",
       " [0.7371359467506409, 0.14328192174434662],\n",
       " [-0.11791379004716873, 0.4949796199798584],\n",
       " [2.991511344909668, -0.6615594029426575],\n",
       " [1.655283808708191, -0.007241949439048767],\n",
       " [0.0658029317855835, 0.057262346148490906],\n",
       " [0.1238681748509407, 0.09263484179973602],\n",
       " [0.35322341322898865, 0.23478065431118011],\n",
       " [0.32058465480804443, 0.11789633333683014],\n",
       " [2.5029759407043457, -0.6511738896369934],\n",
       " [-1.2932838201522827, 1.458831787109375],\n",
       " [2.8747751712799072, -0.5360682010650635],\n",
       " [2.0539050102233887, -0.6208021640777588],\n",
       " [1.6585006713867188, -0.026899918913841248],\n",
       " [1.7563856840133667, -0.17273621261119843],\n",
       " [-0.6492932438850403, 1.015969157218933],\n",
       " [3.6704537868499756, -1.0881174802780151],\n",
       " [-1.6563115119934082, 1.6581461429595947],\n",
       " [-0.3649316430091858, 0.8412891626358032],\n",
       " [0.022900409996509552, 0.6873664259910583],\n",
       " [1.7230815887451172, -0.5166622400283813],\n",
       " [3.5330545902252197, -0.6914112567901611],\n",
       " [-0.7219527959823608, 1.0392431020736694],\n",
       " [-0.9343147873878479, 1.3825569152832031],\n",
       " [3.5822997093200684, -1.0349233150482178],\n",
       " [0.38224565982818604, 0.02927973121404648],\n",
       " [-0.5313526391983032, 0.9911285042762756],\n",
       " [-0.8856297135353088, 1.108412504196167],\n",
       " [-1.2000023126602173, 1.286231517791748],\n",
       " [3.723771095275879, -1.012509822845459],\n",
       " [0.27892550826072693, 0.4095035791397095],\n",
       " [-0.6405751705169678, 0.506230354309082],\n",
       " [0.7379772663116455, 0.24269923567771912],\n",
       " [-1.4271665811538696, 1.5498878955841064],\n",
       " [4.5614800453186035, -1.12131667137146],\n",
       " [-0.12117352336645126, 0.46109509468078613],\n",
       " [-1.0059505701065063, 1.018352746963501],\n",
       " [-0.24183252453804016, 0.7969925403594971],\n",
       " [0.9920935034751892, -0.19490738213062286],\n",
       " [-0.41260695457458496, 0.24170367419719696],\n",
       " [0.44655248522758484, 0.7947393655776978],\n",
       " [0.33607882261276245, 0.23520950973033905],\n",
       " [-1.2526048421859741, 1.514002799987793],\n",
       " [-0.38932597637176514, 0.6768365502357483],\n",
       " [0.3475835919380188, 0.07713019847869873],\n",
       " [-1.1203078031539917, 1.3263367414474487],\n",
       " [-0.42182987928390503, 1.2478362321853638],\n",
       " [0.2267993986606598, 0.3081200122833252],\n",
       " [1.8279749155044556, 0.14698797464370728],\n",
       " [-1.4085544347763062, 1.2338694334030151],\n",
       " [0.743158757686615, 0.31709522008895874],\n",
       " [-1.216173529624939, 1.1352765560150146],\n",
       " [4.893372535705566, -0.687854528427124],\n",
       " [-0.8188439607620239, 1.1214007139205933],\n",
       " [0.6083199381828308, -0.23530252277851105],\n",
       " [-1.09980046749115, 1.3547282218933105],\n",
       " [-0.6939240097999573, 1.2177006006240845],\n",
       " [-1.2203893661499023, 0.9718368053436279],\n",
       " [-0.4583036005496979, 0.6667870879173279],\n",
       " [-0.7518234252929688, 0.9488736987113953],\n",
       " [1.3820246458053589, -0.2653220295906067],\n",
       " [1.0637699365615845, 0.026444211602211],\n",
       " [-1.0203592777252197, 1.414667010307312],\n",
       " [0.6621183753013611, 0.47922784090042114],\n",
       " [-0.48291367292404175, 0.45788395404815674],\n",
       " [-1.264999508857727, 1.430220365524292],\n",
       " [-0.49288398027420044, 1.0338149070739746],\n",
       " [3.993129253387451, -1.0648307800292969],\n",
       " [0.7557269930839539, 0.2196449339389801],\n",
       " [2.2478039264678955, -0.27219074964523315],\n",
       " [-0.4391899108886719, 0.8649642467498779],\n",
       " [0.33037781715393066, 0.22346748411655426],\n",
       " [-0.42487862706184387, 0.46121370792388916],\n",
       " [-0.2996038496494293, 0.6387441158294678],\n",
       " [1.4056700468063354, -0.21185190975666046],\n",
       " [2.8498013019561768, -0.6633250713348389],\n",
       " [-1.0112313032150269, 1.1055494546890259],\n",
       " [4.771511077880859, -1.2498772144317627],\n",
       " [4.404642105102539, -1.1716153621673584],\n",
       " [-1.026721477508545, 1.3024206161499023],\n",
       " [0.6809197664260864, 0.139695942401886],\n",
       " [-1.0737074613571167, 1.1781468391418457],\n",
       " [-0.49026376008987427, 1.1142579317092896],\n",
       " [-0.7626772522926331, 0.7625453472137451],\n",
       " [-1.2415589094161987, 1.4206856489181519],\n",
       " [1.275546908378601, -0.35710418224334717],\n",
       " [-0.533022940158844, 0.5083268284797668],\n",
       " [-0.25776994228363037, 0.426023006439209],\n",
       " [-1.2451746463775635, 1.303680419921875],\n",
       " [0.5866592526435852, 0.2547649145126343],\n",
       " [-0.6049396395683289, 1.1633564233779907],\n",
       " [-0.48349905014038086, 0.5422182083129883],\n",
       " [1.4313884973526, -0.6262883543968201],\n",
       " [-1.477033257484436, 0.8303438425064087],\n",
       " [-0.8999511003494263, 1.222407579421997],\n",
       " [1.5426706075668335, -0.457334041595459],\n",
       " [-1.2847790718078613, 1.2899909019470215],\n",
       " [1.950698733329773, -0.21813581883907318],\n",
       " [2.5043692588806152, -0.5899351239204407],\n",
       " [-1.2075024843215942, 1.4252837896347046],\n",
       " [-0.6031802296638489, 0.8708654642105103],\n",
       " [-1.2039836645126343, 1.4108598232269287],\n",
       " [-0.025609023869037628, 0.30469101667404175],\n",
       " [1.3050578832626343, -0.309800386428833],\n",
       " [1.3538905382156372, -0.5115159153938293],\n",
       " [-1.3391269445419312, 1.4194693565368652],\n",
       " [0.6883092522621155, -0.14384852349758148],\n",
       " [0.1677144169807434, 0.14743712544441223],\n",
       " [2.8086469173431396, -0.21026040613651276],\n",
       " [3.2818782329559326, -0.8059215545654297],\n",
       " [-0.8085868954658508, 1.005044937133789],\n",
       " [-0.10436271876096725, 0.6454594731330872],\n",
       " [-0.2919105589389801, 0.6252094507217407],\n",
       " [1.8106335401535034, -0.6265480518341064],\n",
       " [-0.7445353865623474, 1.0647512674331665],\n",
       " [-0.1635032594203949, 0.7127984762191772],\n",
       " [-0.634650468826294, 0.7771339416503906],\n",
       " [2.675222873687744, -0.4201306104660034],\n",
       " [-1.0389472246170044, 1.3950443267822266],\n",
       " [-1.140779972076416, 1.3511778116226196],\n",
       " [-1.2993696928024292, 1.4205458164215088],\n",
       " [3.9925918579101562, -0.9521485567092896],\n",
       " [-0.4408077895641327, 1.0744296312332153],\n",
       " [-0.5836449861526489, 0.6946923732757568],\n",
       " [-1.0087884664535522, 1.6077864170074463],\n",
       " [-0.39880356192588806, 0.6540499925613403],\n",
       " [-1.1470005512237549, 1.5719082355499268],\n",
       " [-0.6197087168693542, 1.0599579811096191],\n",
       " [-1.2142976522445679, 1.4474241733551025],\n",
       " [4.315210342407227, -1.2377665042877197],\n",
       " [-1.1525031328201294, 1.4884132146835327],\n",
       " [-1.2171133756637573, 1.586838722229004],\n",
       " [-1.1890149116516113, 1.1089824438095093],\n",
       " [-0.946174681186676, 1.3859992027282715],\n",
       " [4.753222942352295, -1.3646008968353271],\n",
       " [-0.6472197771072388, 0.7842075824737549],\n",
       " [2.6562538146972656, -0.5150963068008423],\n",
       " [-0.46886053681373596, 1.090317726135254],\n",
       " [1.9717882871627808, -0.25772154331207275],\n",
       " [0.3740221858024597, -0.021546311676502228],\n",
       " [2.7243857383728027, -0.3273162841796875],\n",
       " [-1.3048917055130005, 1.332911729812622],\n",
       " [0.48884791135787964, 0.49199944734573364],\n",
       " [-0.9668861627578735, 1.2654855251312256],\n",
       " [-0.9764744639396667, 0.987165093421936],\n",
       " [1.0407373905181885, 0.0017463713884353638],\n",
       " [4.0343918800354, -0.9244918823242188],\n",
       " [-0.9895835518836975, 1.1744319200515747],\n",
       " [-0.28600916266441345, 1.0069488286972046],\n",
       " [3.895993232727051, -0.9855625629425049],\n",
       " [-0.6843380331993103, 1.1818666458129883],\n",
       " [-1.1242104768753052, 1.356924057006836],\n",
       " [0.6610516905784607, 0.08352385461330414],\n",
       " [2.0839858055114746, -0.07524929940700531],\n",
       " [-0.9482194781303406, 1.3294177055358887],\n",
       " [2.3143527507781982, -0.418623685836792],\n",
       " [3.165717124938965, -0.5478609800338745],\n",
       " [3.1704611778259277, -0.38078707456588745],\n",
       " [-1.2443715333938599, 1.2638314962387085],\n",
       " [0.9447880387306213, -0.005389593541622162],\n",
       " [-0.919882595539093, 1.2721068859100342],\n",
       " [-0.6412329077720642, 0.9239855408668518],\n",
       " [1.6222279071807861, -0.20375744998455048],\n",
       " [-0.10835341364145279, 0.5467725396156311],\n",
       " [2.4225194454193115, -0.4703174829483032],\n",
       " [-1.289858102798462, 1.4498748779296875],\n",
       " [2.2185606956481934, -0.3656390905380249],\n",
       " [-1.2689718008041382, 1.4512958526611328],\n",
       " [-1.2436691522598267, 1.4917247295379639],\n",
       " [0.28257688879966736, 0.45132875442504883],\n",
       " [1.1467875242233276, -0.08135609328746796],\n",
       " [-0.4887022376060486, 0.38991624116897583],\n",
       " [2.6742255687713623, -0.6829465627670288],\n",
       " [0.1623404324054718, 0.42772847414016724],\n",
       " [0.18821075558662415, 0.41105276346206665],\n",
       " [-0.4268011450767517, 0.6089890003204346],\n",
       " [4.135098457336426, -1.2520384788513184],\n",
       " [-0.5631285905838013, 0.8965380787849426],\n",
       " [2.005786657333374, -0.2729184031486511],\n",
       " [0.21059060096740723, 0.5490313172340393],\n",
       " [-1.0892744064331055, 1.1855556964874268],\n",
       " [2.8693132400512695, -0.5306868553161621],\n",
       " [-1.0366450548171997, 1.0091191530227661],\n",
       " [-1.3957475423812866, 1.48554527759552],\n",
       " [-0.39603161811828613, 1.0310373306274414],\n",
       " [3.48769211769104, -0.9123141765594482],\n",
       " [3.8332202434539795, -1.0623600482940674],\n",
       " [-0.6780423521995544, 1.1284382343292236],\n",
       " [2.175712823867798, -0.45277780294418335],\n",
       " [1.0102925300598145, -0.07070469856262207],\n",
       " [-1.3261445760726929, 1.3093854188919067],\n",
       " [-0.8154171109199524, 0.7606050372123718],\n",
       " [0.17619848251342773, 0.39653462171554565],\n",
       " [-1.3160682916641235, 1.1852571964263916],\n",
       " [0.2931070029735565, 0.15557876229286194],\n",
       " [0.3624359965324402, -0.14009296894073486],\n",
       " [-1.0062004327774048, 1.2790416479110718],\n",
       " [-1.2208480834960938, 1.227178692817688],\n",
       " [0.03157471865415573, 0.4747607111930847],\n",
       " [3.211318254470825, -0.7030986547470093],\n",
       " [1.0696107149124146, 0.0361441969871521],\n",
       " [-0.1017281636595726, 0.37480998039245605],\n",
       " [-0.5398217439651489, 0.5404590964317322],\n",
       " [0.41082146763801575, -0.0787796825170517],\n",
       " [2.5754241943359375, -0.5708260536193848],\n",
       " [3.8307790756225586, -1.1259381771087646],\n",
       " [-0.6368441581726074, 1.058196783065796],\n",
       " [0.7873054146766663, 0.03507615625858307],\n",
       " [-0.9685752391815186, 1.2604598999023438],\n",
       " [-0.0849669799208641, 0.28969520330429077],\n",
       " [3.9152722358703613, -1.124375581741333],\n",
       " [4.003794193267822, -1.1564536094665527],\n",
       " [-0.7774960398674011, 1.1904871463775635],\n",
       " [-1.5401941537857056, 1.3124217987060547],\n",
       " [3.8970141410827637, -0.9865697622299194],\n",
       " [3.8556032180786133, -1.0222275257110596],\n",
       " [-0.5240117311477661, 0.5489850044250488],\n",
       " [-0.8849647045135498, 1.2219643592834473],\n",
       " [3.5709266662597656, -0.9065232276916504],\n",
       " [0.6297805309295654, 0.4431237578392029],\n",
       " [-1.045871376991272, 1.3117111921310425],\n",
       " [3.14327335357666, -0.45810961723327637],\n",
       " [3.5967676639556885, -0.6897129416465759],\n",
       " [-0.8477396368980408, 0.7831453084945679],\n",
       " [0.753808319568634, -0.06602714955806732],\n",
       " [-1.1312724351882935, 1.438539981842041],\n",
       " [-1.028948187828064, 0.8603112697601318],\n",
       " [0.34969910979270935, 0.4858364462852478],\n",
       " [3.9047884941101074, -0.9909462928771973],\n",
       " [3.8328120708465576, -1.021073341369629],\n",
       " [2.4147918224334717, -0.519920825958252],\n",
       " [-0.35631632804870605, 0.6349437236785889],\n",
       " [1.454344630241394, -0.4142951965332031],\n",
       " [3.446457624435425, -1.1099492311477661],\n",
       " [-1.017303466796875, 0.9185173511505127],\n",
       " [-1.091568112373352, 1.4389662742614746],\n",
       " [-0.49959415197372437, 0.5421218276023865],\n",
       " [-1.1873753070831299, 1.1312540769577026],\n",
       " [2.473759174346924, -0.6406852006912231],\n",
       " [0.8145873546600342, -0.1423935741186142],\n",
       " [-0.8287496566772461, 0.8855501413345337],\n",
       " [0.47689682245254517, 0.07357372343540192],\n",
       " [-0.9372081160545349, 0.5538054704666138],\n",
       " [-0.9686619639396667, 0.9655420780181885],\n",
       " ...]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T02:15:56.301598Z",
     "start_time": "2021-08-31T02:15:56.223824Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y should be a 1d array, got an array of shape (67298, 2) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-fdd851311b68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrf_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rf auc : {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrf_auc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# plot the roc curve for the model81\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrf_fpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrf_tpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.6/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    543\u001b[0m                                              max_fpr=max_fpr),\n\u001b[1;32m    544\u001b[0m                                      \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m                                      sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    546\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# multilabel-indicator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m         return _average_binary_score(partial(_binary_roc_auc_score,\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.6/site-packages/sklearn/metrics/_base.py\u001b[0m in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbinary_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.6/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36m_binary_roc_auc_score\u001b[0;34m(y_true, y_score, sample_weight, max_fpr)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     fpr, tpr, _ = roc_curve(y_true, y_score,\n\u001b[0;32m--> 331\u001b[0;31m                             sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    332\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmax_fpr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmax_fpr\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.6/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[1;32m    912\u001b[0m     \"\"\"\n\u001b[1;32m    913\u001b[0m     fps, tps, thresholds = _binary_clf_curve(\n\u001b[0;32m--> 914\u001b[0;31m         y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m     \u001b[0;31m# Attempt to drop thresholds corresponding to points in between and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.6/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    693\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m     \u001b[0my_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    696\u001b[0m     \u001b[0massert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m     \u001b[0massert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcolumn_or_1d\u001b[0;34m(y, warn)\u001b[0m\n\u001b[1;32m    921\u001b[0m     raise ValueError(\n\u001b[1;32m    922\u001b[0m         \u001b[0;34m\"y should be a 1d array, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 923\u001b[0;31m         \"got an array of shape {} instead.\".format(shape))\n\u001b[0m\u001b[1;32m    924\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: y should be a 1d array, got an array of shape (67298, 2) instead."
     ]
    }
   ],
   "source": [
    "rf_auc = roc_auc_score(test_label, probs)\n",
    "print('rf auc : {}'.format(rf_auc))\n",
    "# plot the roc curve for the model81\n",
    "rf_fpr, rf_tpr, _ = roc_curve(test_label, probs)\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.plot(rf_fpr, rf_tpr, marker='.', label='RF AUC = {:.4f}'.format(rf_auc), color='orange')\n",
    "plt.title('ROC curve')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()\n",
    "\n",
    "cf_matrix = confusion_matrix(test_label, y_pred)\n",
    "plt.figure(figsize=(3, 3))\n",
    "sns.heatmap(cf_matrix, annot=True, fmt=',.0f')\n",
    "plt.show()\n",
    "plt.figure(figsize=(3, 3))\n",
    "sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, \n",
    "            fmt='.2%', cmap='Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
