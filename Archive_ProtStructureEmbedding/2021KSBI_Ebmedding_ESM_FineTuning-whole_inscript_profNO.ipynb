{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T22:38:18.137255Z",
     "start_time": "2021-12-08T22:38:17.275979Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        param = parameter.numel()\n",
    "        table.add_row([name, param])\n",
    "        total_params+=param\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T22:38:18.149102Z",
     "start_time": "2021-12-08T22:38:18.138813Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19, 21)\n"
     ]
    }
   ],
   "source": [
    "df_aaindex = pd.read_csv('../data/aaindex/df_aaindex19.csv')\n",
    "print(df_aaindex.shape)\n",
    "df_aaindex.head(1)\n",
    "tmp = df_aaindex.drop('Unnamed: 0',axis=1).T\n",
    "aa2val = dict()\n",
    "for aa, val in zip(tmp.index, tmp.values):\n",
    "    aa2val[aa]=val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T22:38:20.944262Z",
     "start_time": "2021-12-08T22:38:18.150339Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>peptide</th>\n",
       "      <th>En</th>\n",
       "      <th>Ec</th>\n",
       "      <th>E1</th>\n",
       "      <th>E2</th>\n",
       "      <th>protein</th>\n",
       "      <th>PEP</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>595411</th>\n",
       "      <td>K.QELNEPPKQSTSFLVLQEILESEEKGDPNK.P</td>\n",
       "      <td>VYKMLQEKQELNEPP</td>\n",
       "      <td>EEKGDPNKPSGFRSV</td>\n",
       "      <td>QELNEPPKQSTSFLV</td>\n",
       "      <td>EILESEEKGDPNKPS</td>\n",
       "      <td>sp|O00151|PDLI1_HUMAN</td>\n",
       "      <td>QELNEPPKQSTSFLVLQEILESEEKGDPNK</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   peptide               En               Ec  \\\n",
       "595411  K.QELNEPPKQSTSFLVLQEILESEEKGDPNK.P  VYKMLQEKQELNEPP  EEKGDPNKPSGFRSV   \n",
       "\n",
       "                     E1               E2                protein  \\\n",
       "595411  QELNEPPKQSTSFLV  EILESEEKGDPNKPS  sp|O00151|PDLI1_HUMAN   \n",
       "\n",
       "                                   PEP  ID  \n",
       "595411  QELNEPPKQSTSFLVLQEILESEEKGDPNK   0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_detect_peptide_train = pd.read_csv('../data/df_detect_peptide_train.csv')\n",
    "test = pd.read_csv('../data/df_detect_peptide_test.csv')\n",
    "train, val = train_test_split(df_detect_peptide_train, test_size=0.2, random_state=7)\n",
    "\n",
    "df = pd.concat([train, val, test], axis=0).reset_index(drop=True)\n",
    "\n",
    "train_idx = df.iloc[:len(train), :].index\n",
    "val_idx = df.iloc[len(train):len(train)+len(val), :].index\n",
    "test_idx = df.iloc[len(train)+len(val):, :].index\n",
    "\n",
    "train.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T22:38:21.663289Z",
     "start_time": "2021-12-08T22:38:20.945464Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "PATH_TO_REPO = \"/home/bis/2021_AIhub/esm/\"\n",
    "sys.path.append(PATH_TO_REPO)\n",
    "\n",
    "import torch\n",
    "import esm\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import time\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AdamW\n",
    "# from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "from transformers import WarmupLinearSchedule as get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T22:38:21.666890Z",
     "start_time": "2021-12-08T22:38:21.664691Z"
    }
   },
   "outputs": [],
   "source": [
    "##GPU 사용 시\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T22:38:34.535922Z",
     "start_time": "2021-12-08T22:38:21.667927Z"
    }
   },
   "outputs": [],
   "source": [
    "esm_model, alphabet = esm.pretrained.esm1b_t33_650M_UR50S()\n",
    "# esm_model, alphabet = esm.pretrained.esm_msa1b_t12_100M_UR50S()\n",
    "batch_converter = alphabet.get_batch_converter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T22:38:34.544366Z",
     "start_time": "2021-12-08T22:38:34.537816Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProteinBertModel(\n",
       "  (embed_tokens): Embedding(33, 1280, padding_idx=1)\n",
       "  (layers): ModuleList(\n",
       "    (0): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (6): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (7): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (8): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (9): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (10): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (11): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (12): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (13): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (14): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (15): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (16): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (17): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (18): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (19): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (20): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (21): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (22): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (23): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (24): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (25): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (26): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (27): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (28): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (29): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (30): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (31): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (32): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (contact_head): ContactPredictionHead(\n",
       "    (regression): Linear(in_features=660, out_features=1, bias=True)\n",
       "    (activation): Sigmoid()\n",
       "  )\n",
       "  (embed_positions): LearnedPositionalEmbedding(1026, 1280, padding_idx=1)\n",
       "  (emb_layer_norm_before): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  (emb_layer_norm_after): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "esm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T22:38:34.589011Z",
     "start_time": "2021-12-08T22:38:34.545465Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+------------+\n",
      "|                Modules                | Parameters |\n",
      "+---------------------------------------+------------+\n",
      "|          embed_tokens.weight          |   42240    |\n",
      "|    layers.0.self_attn.k_proj.weight   |  1638400   |\n",
      "|     layers.0.self_attn.k_proj.bias    |    1280    |\n",
      "|    layers.0.self_attn.v_proj.weight   |  1638400   |\n",
      "|     layers.0.self_attn.v_proj.bias    |    1280    |\n",
      "|    layers.0.self_attn.q_proj.weight   |  1638400   |\n",
      "|     layers.0.self_attn.q_proj.bias    |    1280    |\n",
      "|   layers.0.self_attn.out_proj.weight  |  1638400   |\n",
      "|    layers.0.self_attn.out_proj.bias   |    1280    |\n",
      "|  layers.0.self_attn_layer_norm.weight |    1280    |\n",
      "|   layers.0.self_attn_layer_norm.bias  |    1280    |\n",
      "|          layers.0.fc1.weight          |  6553600   |\n",
      "|           layers.0.fc1.bias           |    5120    |\n",
      "|          layers.0.fc2.weight          |  6553600   |\n",
      "|           layers.0.fc2.bias           |    1280    |\n",
      "|    layers.0.final_layer_norm.weight   |    1280    |\n",
      "|     layers.0.final_layer_norm.bias    |    1280    |\n",
      "|    layers.1.self_attn.k_proj.weight   |  1638400   |\n",
      "|     layers.1.self_attn.k_proj.bias    |    1280    |\n",
      "|    layers.1.self_attn.v_proj.weight   |  1638400   |\n",
      "|     layers.1.self_attn.v_proj.bias    |    1280    |\n",
      "|    layers.1.self_attn.q_proj.weight   |  1638400   |\n",
      "|     layers.1.self_attn.q_proj.bias    |    1280    |\n",
      "|   layers.1.self_attn.out_proj.weight  |  1638400   |\n",
      "|    layers.1.self_attn.out_proj.bias   |    1280    |\n",
      "|  layers.1.self_attn_layer_norm.weight |    1280    |\n",
      "|   layers.1.self_attn_layer_norm.bias  |    1280    |\n",
      "|          layers.1.fc1.weight          |  6553600   |\n",
      "|           layers.1.fc1.bias           |    5120    |\n",
      "|          layers.1.fc2.weight          |  6553600   |\n",
      "|           layers.1.fc2.bias           |    1280    |\n",
      "|    layers.1.final_layer_norm.weight   |    1280    |\n",
      "|     layers.1.final_layer_norm.bias    |    1280    |\n",
      "|    layers.2.self_attn.k_proj.weight   |  1638400   |\n",
      "|     layers.2.self_attn.k_proj.bias    |    1280    |\n",
      "|    layers.2.self_attn.v_proj.weight   |  1638400   |\n",
      "|     layers.2.self_attn.v_proj.bias    |    1280    |\n",
      "|    layers.2.self_attn.q_proj.weight   |  1638400   |\n",
      "|     layers.2.self_attn.q_proj.bias    |    1280    |\n",
      "|   layers.2.self_attn.out_proj.weight  |  1638400   |\n",
      "|    layers.2.self_attn.out_proj.bias   |    1280    |\n",
      "|  layers.2.self_attn_layer_norm.weight |    1280    |\n",
      "|   layers.2.self_attn_layer_norm.bias  |    1280    |\n",
      "|          layers.2.fc1.weight          |  6553600   |\n",
      "|           layers.2.fc1.bias           |    5120    |\n",
      "|          layers.2.fc2.weight          |  6553600   |\n",
      "|           layers.2.fc2.bias           |    1280    |\n",
      "|    layers.2.final_layer_norm.weight   |    1280    |\n",
      "|     layers.2.final_layer_norm.bias    |    1280    |\n",
      "|    layers.3.self_attn.k_proj.weight   |  1638400   |\n",
      "|     layers.3.self_attn.k_proj.bias    |    1280    |\n",
      "|    layers.3.self_attn.v_proj.weight   |  1638400   |\n",
      "|     layers.3.self_attn.v_proj.bias    |    1280    |\n",
      "|    layers.3.self_attn.q_proj.weight   |  1638400   |\n",
      "|     layers.3.self_attn.q_proj.bias    |    1280    |\n",
      "|   layers.3.self_attn.out_proj.weight  |  1638400   |\n",
      "|    layers.3.self_attn.out_proj.bias   |    1280    |\n",
      "|  layers.3.self_attn_layer_norm.weight |    1280    |\n",
      "|   layers.3.self_attn_layer_norm.bias  |    1280    |\n",
      "|          layers.3.fc1.weight          |  6553600   |\n",
      "|           layers.3.fc1.bias           |    5120    |\n",
      "|          layers.3.fc2.weight          |  6553600   |\n",
      "|           layers.3.fc2.bias           |    1280    |\n",
      "|    layers.3.final_layer_norm.weight   |    1280    |\n",
      "|     layers.3.final_layer_norm.bias    |    1280    |\n",
      "|    layers.4.self_attn.k_proj.weight   |  1638400   |\n",
      "|     layers.4.self_attn.k_proj.bias    |    1280    |\n",
      "|    layers.4.self_attn.v_proj.weight   |  1638400   |\n",
      "|     layers.4.self_attn.v_proj.bias    |    1280    |\n",
      "|    layers.4.self_attn.q_proj.weight   |  1638400   |\n",
      "|     layers.4.self_attn.q_proj.bias    |    1280    |\n",
      "|   layers.4.self_attn.out_proj.weight  |  1638400   |\n",
      "|    layers.4.self_attn.out_proj.bias   |    1280    |\n",
      "|  layers.4.self_attn_layer_norm.weight |    1280    |\n",
      "|   layers.4.self_attn_layer_norm.bias  |    1280    |\n",
      "|          layers.4.fc1.weight          |  6553600   |\n",
      "|           layers.4.fc1.bias           |    5120    |\n",
      "|          layers.4.fc2.weight          |  6553600   |\n",
      "|           layers.4.fc2.bias           |    1280    |\n",
      "|    layers.4.final_layer_norm.weight   |    1280    |\n",
      "|     layers.4.final_layer_norm.bias    |    1280    |\n",
      "|    layers.5.self_attn.k_proj.weight   |  1638400   |\n",
      "|     layers.5.self_attn.k_proj.bias    |    1280    |\n",
      "|    layers.5.self_attn.v_proj.weight   |  1638400   |\n",
      "|     layers.5.self_attn.v_proj.bias    |    1280    |\n",
      "|    layers.5.self_attn.q_proj.weight   |  1638400   |\n",
      "|     layers.5.self_attn.q_proj.bias    |    1280    |\n",
      "|   layers.5.self_attn.out_proj.weight  |  1638400   |\n",
      "|    layers.5.self_attn.out_proj.bias   |    1280    |\n",
      "|  layers.5.self_attn_layer_norm.weight |    1280    |\n",
      "|   layers.5.self_attn_layer_norm.bias  |    1280    |\n",
      "|          layers.5.fc1.weight          |  6553600   |\n",
      "|           layers.5.fc1.bias           |    5120    |\n",
      "|          layers.5.fc2.weight          |  6553600   |\n",
      "|           layers.5.fc2.bias           |    1280    |\n",
      "|    layers.5.final_layer_norm.weight   |    1280    |\n",
      "|     layers.5.final_layer_norm.bias    |    1280    |\n",
      "|    layers.6.self_attn.k_proj.weight   |  1638400   |\n",
      "|     layers.6.self_attn.k_proj.bias    |    1280    |\n",
      "|    layers.6.self_attn.v_proj.weight   |  1638400   |\n",
      "|     layers.6.self_attn.v_proj.bias    |    1280    |\n",
      "|    layers.6.self_attn.q_proj.weight   |  1638400   |\n",
      "|     layers.6.self_attn.q_proj.bias    |    1280    |\n",
      "|   layers.6.self_attn.out_proj.weight  |  1638400   |\n",
      "|    layers.6.self_attn.out_proj.bias   |    1280    |\n",
      "|  layers.6.self_attn_layer_norm.weight |    1280    |\n",
      "|   layers.6.self_attn_layer_norm.bias  |    1280    |\n",
      "|          layers.6.fc1.weight          |  6553600   |\n",
      "|           layers.6.fc1.bias           |    5120    |\n",
      "|          layers.6.fc2.weight          |  6553600   |\n",
      "|           layers.6.fc2.bias           |    1280    |\n",
      "|    layers.6.final_layer_norm.weight   |    1280    |\n",
      "|     layers.6.final_layer_norm.bias    |    1280    |\n",
      "|    layers.7.self_attn.k_proj.weight   |  1638400   |\n",
      "|     layers.7.self_attn.k_proj.bias    |    1280    |\n",
      "|    layers.7.self_attn.v_proj.weight   |  1638400   |\n",
      "|     layers.7.self_attn.v_proj.bias    |    1280    |\n",
      "|    layers.7.self_attn.q_proj.weight   |  1638400   |\n",
      "|     layers.7.self_attn.q_proj.bias    |    1280    |\n",
      "|   layers.7.self_attn.out_proj.weight  |  1638400   |\n",
      "|    layers.7.self_attn.out_proj.bias   |    1280    |\n",
      "|  layers.7.self_attn_layer_norm.weight |    1280    |\n",
      "|   layers.7.self_attn_layer_norm.bias  |    1280    |\n",
      "|          layers.7.fc1.weight          |  6553600   |\n",
      "|           layers.7.fc1.bias           |    5120    |\n",
      "|          layers.7.fc2.weight          |  6553600   |\n",
      "|           layers.7.fc2.bias           |    1280    |\n",
      "|    layers.7.final_layer_norm.weight   |    1280    |\n",
      "|     layers.7.final_layer_norm.bias    |    1280    |\n",
      "|    layers.8.self_attn.k_proj.weight   |  1638400   |\n",
      "|     layers.8.self_attn.k_proj.bias    |    1280    |\n",
      "|    layers.8.self_attn.v_proj.weight   |  1638400   |\n",
      "|     layers.8.self_attn.v_proj.bias    |    1280    |\n",
      "|    layers.8.self_attn.q_proj.weight   |  1638400   |\n",
      "|     layers.8.self_attn.q_proj.bias    |    1280    |\n",
      "|   layers.8.self_attn.out_proj.weight  |  1638400   |\n",
      "|    layers.8.self_attn.out_proj.bias   |    1280    |\n",
      "|  layers.8.self_attn_layer_norm.weight |    1280    |\n",
      "|   layers.8.self_attn_layer_norm.bias  |    1280    |\n",
      "|          layers.8.fc1.weight          |  6553600   |\n",
      "|           layers.8.fc1.bias           |    5120    |\n",
      "|          layers.8.fc2.weight          |  6553600   |\n",
      "|           layers.8.fc2.bias           |    1280    |\n",
      "|    layers.8.final_layer_norm.weight   |    1280    |\n",
      "|     layers.8.final_layer_norm.bias    |    1280    |\n",
      "|    layers.9.self_attn.k_proj.weight   |  1638400   |\n",
      "|     layers.9.self_attn.k_proj.bias    |    1280    |\n",
      "|    layers.9.self_attn.v_proj.weight   |  1638400   |\n",
      "|     layers.9.self_attn.v_proj.bias    |    1280    |\n",
      "|    layers.9.self_attn.q_proj.weight   |  1638400   |\n",
      "|     layers.9.self_attn.q_proj.bias    |    1280    |\n",
      "|   layers.9.self_attn.out_proj.weight  |  1638400   |\n",
      "|    layers.9.self_attn.out_proj.bias   |    1280    |\n",
      "|  layers.9.self_attn_layer_norm.weight |    1280    |\n",
      "|   layers.9.self_attn_layer_norm.bias  |    1280    |\n",
      "|          layers.9.fc1.weight          |  6553600   |\n",
      "|           layers.9.fc1.bias           |    5120    |\n",
      "|          layers.9.fc2.weight          |  6553600   |\n",
      "|           layers.9.fc2.bias           |    1280    |\n",
      "|    layers.9.final_layer_norm.weight   |    1280    |\n",
      "|     layers.9.final_layer_norm.bias    |    1280    |\n",
      "|   layers.10.self_attn.k_proj.weight   |  1638400   |\n",
      "|    layers.10.self_attn.k_proj.bias    |    1280    |\n",
      "|   layers.10.self_attn.v_proj.weight   |  1638400   |\n",
      "|    layers.10.self_attn.v_proj.bias    |    1280    |\n",
      "|   layers.10.self_attn.q_proj.weight   |  1638400   |\n",
      "|    layers.10.self_attn.q_proj.bias    |    1280    |\n",
      "|  layers.10.self_attn.out_proj.weight  |  1638400   |\n",
      "|   layers.10.self_attn.out_proj.bias   |    1280    |\n",
      "| layers.10.self_attn_layer_norm.weight |    1280    |\n",
      "|  layers.10.self_attn_layer_norm.bias  |    1280    |\n",
      "|          layers.10.fc1.weight         |  6553600   |\n",
      "|           layers.10.fc1.bias          |    5120    |\n",
      "|          layers.10.fc2.weight         |  6553600   |\n",
      "|           layers.10.fc2.bias          |    1280    |\n",
      "|   layers.10.final_layer_norm.weight   |    1280    |\n",
      "|    layers.10.final_layer_norm.bias    |    1280    |\n",
      "|   layers.11.self_attn.k_proj.weight   |  1638400   |\n",
      "|    layers.11.self_attn.k_proj.bias    |    1280    |\n",
      "|   layers.11.self_attn.v_proj.weight   |  1638400   |\n",
      "|    layers.11.self_attn.v_proj.bias    |    1280    |\n",
      "|   layers.11.self_attn.q_proj.weight   |  1638400   |\n",
      "|    layers.11.self_attn.q_proj.bias    |    1280    |\n",
      "|  layers.11.self_attn.out_proj.weight  |  1638400   |\n",
      "|   layers.11.self_attn.out_proj.bias   |    1280    |\n",
      "| layers.11.self_attn_layer_norm.weight |    1280    |\n",
      "|  layers.11.self_attn_layer_norm.bias  |    1280    |\n",
      "|          layers.11.fc1.weight         |  6553600   |\n",
      "|           layers.11.fc1.bias          |    5120    |\n",
      "|          layers.11.fc2.weight         |  6553600   |\n",
      "|           layers.11.fc2.bias          |    1280    |\n",
      "|   layers.11.final_layer_norm.weight   |    1280    |\n",
      "|    layers.11.final_layer_norm.bias    |    1280    |\n",
      "|   layers.12.self_attn.k_proj.weight   |  1638400   |\n",
      "|    layers.12.self_attn.k_proj.bias    |    1280    |\n",
      "|   layers.12.self_attn.v_proj.weight   |  1638400   |\n",
      "|    layers.12.self_attn.v_proj.bias    |    1280    |\n",
      "|   layers.12.self_attn.q_proj.weight   |  1638400   |\n",
      "|    layers.12.self_attn.q_proj.bias    |    1280    |\n",
      "|  layers.12.self_attn.out_proj.weight  |  1638400   |\n",
      "|   layers.12.self_attn.out_proj.bias   |    1280    |\n",
      "| layers.12.self_attn_layer_norm.weight |    1280    |\n",
      "|  layers.12.self_attn_layer_norm.bias  |    1280    |\n",
      "|          layers.12.fc1.weight         |  6553600   |\n",
      "|           layers.12.fc1.bias          |    5120    |\n",
      "|          layers.12.fc2.weight         |  6553600   |\n",
      "|           layers.12.fc2.bias          |    1280    |\n",
      "|   layers.12.final_layer_norm.weight   |    1280    |\n",
      "|    layers.12.final_layer_norm.bias    |    1280    |\n",
      "|   layers.13.self_attn.k_proj.weight   |  1638400   |\n",
      "|    layers.13.self_attn.k_proj.bias    |    1280    |\n",
      "|   layers.13.self_attn.v_proj.weight   |  1638400   |\n",
      "|    layers.13.self_attn.v_proj.bias    |    1280    |\n",
      "|   layers.13.self_attn.q_proj.weight   |  1638400   |\n",
      "|    layers.13.self_attn.q_proj.bias    |    1280    |\n",
      "|  layers.13.self_attn.out_proj.weight  |  1638400   |\n",
      "|   layers.13.self_attn.out_proj.bias   |    1280    |\n",
      "| layers.13.self_attn_layer_norm.weight |    1280    |\n",
      "|  layers.13.self_attn_layer_norm.bias  |    1280    |\n",
      "|          layers.13.fc1.weight         |  6553600   |\n",
      "|           layers.13.fc1.bias          |    5120    |\n",
      "|          layers.13.fc2.weight         |  6553600   |\n",
      "|           layers.13.fc2.bias          |    1280    |\n",
      "|   layers.13.final_layer_norm.weight   |    1280    |\n",
      "|    layers.13.final_layer_norm.bias    |    1280    |\n",
      "|   layers.14.self_attn.k_proj.weight   |  1638400   |\n",
      "|    layers.14.self_attn.k_proj.bias    |    1280    |\n",
      "|   layers.14.self_attn.v_proj.weight   |  1638400   |\n",
      "|    layers.14.self_attn.v_proj.bias    |    1280    |\n",
      "|   layers.14.self_attn.q_proj.weight   |  1638400   |\n",
      "|    layers.14.self_attn.q_proj.bias    |    1280    |\n",
      "|  layers.14.self_attn.out_proj.weight  |  1638400   |\n",
      "|   layers.14.self_attn.out_proj.bias   |    1280    |\n",
      "| layers.14.self_attn_layer_norm.weight |    1280    |\n",
      "|  layers.14.self_attn_layer_norm.bias  |    1280    |\n",
      "|          layers.14.fc1.weight         |  6553600   |\n",
      "|           layers.14.fc1.bias          |    5120    |\n",
      "|          layers.14.fc2.weight         |  6553600   |\n",
      "|           layers.14.fc2.bias          |    1280    |\n",
      "|   layers.14.final_layer_norm.weight   |    1280    |\n",
      "|    layers.14.final_layer_norm.bias    |    1280    |\n",
      "|   layers.15.self_attn.k_proj.weight   |  1638400   |\n",
      "|    layers.15.self_attn.k_proj.bias    |    1280    |\n",
      "|   layers.15.self_attn.v_proj.weight   |  1638400   |\n",
      "|    layers.15.self_attn.v_proj.bias    |    1280    |\n",
      "|   layers.15.self_attn.q_proj.weight   |  1638400   |\n",
      "|    layers.15.self_attn.q_proj.bias    |    1280    |\n",
      "|  layers.15.self_attn.out_proj.weight  |  1638400   |\n",
      "|   layers.15.self_attn.out_proj.bias   |    1280    |\n",
      "| layers.15.self_attn_layer_norm.weight |    1280    |\n",
      "|  layers.15.self_attn_layer_norm.bias  |    1280    |\n",
      "|          layers.15.fc1.weight         |  6553600   |\n",
      "|           layers.15.fc1.bias          |    5120    |\n",
      "|          layers.15.fc2.weight         |  6553600   |\n",
      "|           layers.15.fc2.bias          |    1280    |\n",
      "|   layers.15.final_layer_norm.weight   |    1280    |\n",
      "|    layers.15.final_layer_norm.bias    |    1280    |\n",
      "|   layers.16.self_attn.k_proj.weight   |  1638400   |\n",
      "|    layers.16.self_attn.k_proj.bias    |    1280    |\n",
      "|   layers.16.self_attn.v_proj.weight   |  1638400   |\n",
      "|    layers.16.self_attn.v_proj.bias    |    1280    |\n",
      "|   layers.16.self_attn.q_proj.weight   |  1638400   |\n",
      "|    layers.16.self_attn.q_proj.bias    |    1280    |\n",
      "|  layers.16.self_attn.out_proj.weight  |  1638400   |\n",
      "|   layers.16.self_attn.out_proj.bias   |    1280    |\n",
      "| layers.16.self_attn_layer_norm.weight |    1280    |\n",
      "|  layers.16.self_attn_layer_norm.bias  |    1280    |\n",
      "|          layers.16.fc1.weight         |  6553600   |\n",
      "|           layers.16.fc1.bias          |    5120    |\n",
      "|          layers.16.fc2.weight         |  6553600   |\n",
      "|           layers.16.fc2.bias          |    1280    |\n",
      "|   layers.16.final_layer_norm.weight   |    1280    |\n",
      "|    layers.16.final_layer_norm.bias    |    1280    |\n",
      "|   layers.17.self_attn.k_proj.weight   |  1638400   |\n",
      "|    layers.17.self_attn.k_proj.bias    |    1280    |\n",
      "|   layers.17.self_attn.v_proj.weight   |  1638400   |\n",
      "|    layers.17.self_attn.v_proj.bias    |    1280    |\n",
      "|   layers.17.self_attn.q_proj.weight   |  1638400   |\n",
      "|    layers.17.self_attn.q_proj.bias    |    1280    |\n",
      "|  layers.17.self_attn.out_proj.weight  |  1638400   |\n",
      "|   layers.17.self_attn.out_proj.bias   |    1280    |\n",
      "| layers.17.self_attn_layer_norm.weight |    1280    |\n",
      "|  layers.17.self_attn_layer_norm.bias  |    1280    |\n",
      "|          layers.17.fc1.weight         |  6553600   |\n",
      "|           layers.17.fc1.bias          |    5120    |\n",
      "|          layers.17.fc2.weight         |  6553600   |\n",
      "|           layers.17.fc2.bias          |    1280    |\n",
      "|   layers.17.final_layer_norm.weight   |    1280    |\n",
      "|    layers.17.final_layer_norm.bias    |    1280    |\n",
      "|   layers.18.self_attn.k_proj.weight   |  1638400   |\n",
      "|    layers.18.self_attn.k_proj.bias    |    1280    |\n",
      "|   layers.18.self_attn.v_proj.weight   |  1638400   |\n",
      "|    layers.18.self_attn.v_proj.bias    |    1280    |\n",
      "|   layers.18.self_attn.q_proj.weight   |  1638400   |\n",
      "|    layers.18.self_attn.q_proj.bias    |    1280    |\n",
      "|  layers.18.self_attn.out_proj.weight  |  1638400   |\n",
      "|   layers.18.self_attn.out_proj.bias   |    1280    |\n",
      "| layers.18.self_attn_layer_norm.weight |    1280    |\n",
      "|  layers.18.self_attn_layer_norm.bias  |    1280    |\n",
      "|          layers.18.fc1.weight         |  6553600   |\n",
      "|           layers.18.fc1.bias          |    5120    |\n",
      "|          layers.18.fc2.weight         |  6553600   |\n",
      "|           layers.18.fc2.bias          |    1280    |\n",
      "|   layers.18.final_layer_norm.weight   |    1280    |\n",
      "|    layers.18.final_layer_norm.bias    |    1280    |\n",
      "|   layers.19.self_attn.k_proj.weight   |  1638400   |\n",
      "|    layers.19.self_attn.k_proj.bias    |    1280    |\n",
      "|   layers.19.self_attn.v_proj.weight   |  1638400   |\n",
      "|    layers.19.self_attn.v_proj.bias    |    1280    |\n",
      "|   layers.19.self_attn.q_proj.weight   |  1638400   |\n",
      "|    layers.19.self_attn.q_proj.bias    |    1280    |\n",
      "|  layers.19.self_attn.out_proj.weight  |  1638400   |\n",
      "|   layers.19.self_attn.out_proj.bias   |    1280    |\n",
      "| layers.19.self_attn_layer_norm.weight |    1280    |\n",
      "|  layers.19.self_attn_layer_norm.bias  |    1280    |\n",
      "|          layers.19.fc1.weight         |  6553600   |\n",
      "|           layers.19.fc1.bias          |    5120    |\n",
      "|          layers.19.fc2.weight         |  6553600   |\n",
      "|           layers.19.fc2.bias          |    1280    |\n",
      "|   layers.19.final_layer_norm.weight   |    1280    |\n",
      "|    layers.19.final_layer_norm.bias    |    1280    |\n",
      "|   layers.20.self_attn.k_proj.weight   |  1638400   |\n",
      "|    layers.20.self_attn.k_proj.bias    |    1280    |\n",
      "|   layers.20.self_attn.v_proj.weight   |  1638400   |\n",
      "|    layers.20.self_attn.v_proj.bias    |    1280    |\n",
      "|   layers.20.self_attn.q_proj.weight   |  1638400   |\n",
      "|    layers.20.self_attn.q_proj.bias    |    1280    |\n",
      "|  layers.20.self_attn.out_proj.weight  |  1638400   |\n",
      "|   layers.20.self_attn.out_proj.bias   |    1280    |\n",
      "| layers.20.self_attn_layer_norm.weight |    1280    |\n",
      "|  layers.20.self_attn_layer_norm.bias  |    1280    |\n",
      "|          layers.20.fc1.weight         |  6553600   |\n",
      "|           layers.20.fc1.bias          |    5120    |\n",
      "|          layers.20.fc2.weight         |  6553600   |\n",
      "|           layers.20.fc2.bias          |    1280    |\n",
      "|   layers.20.final_layer_norm.weight   |    1280    |\n",
      "|    layers.20.final_layer_norm.bias    |    1280    |\n",
      "|   layers.21.self_attn.k_proj.weight   |  1638400   |\n",
      "|    layers.21.self_attn.k_proj.bias    |    1280    |\n",
      "|   layers.21.self_attn.v_proj.weight   |  1638400   |\n",
      "|    layers.21.self_attn.v_proj.bias    |    1280    |\n",
      "|   layers.21.self_attn.q_proj.weight   |  1638400   |\n",
      "|    layers.21.self_attn.q_proj.bias    |    1280    |\n",
      "|  layers.21.self_attn.out_proj.weight  |  1638400   |\n",
      "|   layers.21.self_attn.out_proj.bias   |    1280    |\n",
      "| layers.21.self_attn_layer_norm.weight |    1280    |\n",
      "|  layers.21.self_attn_layer_norm.bias  |    1280    |\n",
      "|          layers.21.fc1.weight         |  6553600   |\n",
      "|           layers.21.fc1.bias          |    5120    |\n",
      "|          layers.21.fc2.weight         |  6553600   |\n",
      "|           layers.21.fc2.bias          |    1280    |\n",
      "|   layers.21.final_layer_norm.weight   |    1280    |\n",
      "|    layers.21.final_layer_norm.bias    |    1280    |\n",
      "|   layers.22.self_attn.k_proj.weight   |  1638400   |\n",
      "|    layers.22.self_attn.k_proj.bias    |    1280    |\n",
      "|   layers.22.self_attn.v_proj.weight   |  1638400   |\n",
      "|    layers.22.self_attn.v_proj.bias    |    1280    |\n",
      "|   layers.22.self_attn.q_proj.weight   |  1638400   |\n",
      "|    layers.22.self_attn.q_proj.bias    |    1280    |\n",
      "|  layers.22.self_attn.out_proj.weight  |  1638400   |\n",
      "|   layers.22.self_attn.out_proj.bias   |    1280    |\n",
      "| layers.22.self_attn_layer_norm.weight |    1280    |\n",
      "|  layers.22.self_attn_layer_norm.bias  |    1280    |\n",
      "|          layers.22.fc1.weight         |  6553600   |\n",
      "|           layers.22.fc1.bias          |    5120    |\n",
      "|          layers.22.fc2.weight         |  6553600   |\n",
      "|           layers.22.fc2.bias          |    1280    |\n",
      "|   layers.22.final_layer_norm.weight   |    1280    |\n",
      "|    layers.22.final_layer_norm.bias    |    1280    |\n",
      "|   layers.23.self_attn.k_proj.weight   |  1638400   |\n",
      "|    layers.23.self_attn.k_proj.bias    |    1280    |\n",
      "|   layers.23.self_attn.v_proj.weight   |  1638400   |\n",
      "|    layers.23.self_attn.v_proj.bias    |    1280    |\n",
      "|   layers.23.self_attn.q_proj.weight   |  1638400   |\n",
      "|    layers.23.self_attn.q_proj.bias    |    1280    |\n",
      "|  layers.23.self_attn.out_proj.weight  |  1638400   |\n",
      "|   layers.23.self_attn.out_proj.bias   |    1280    |\n",
      "| layers.23.self_attn_layer_norm.weight |    1280    |\n",
      "|  layers.23.self_attn_layer_norm.bias  |    1280    |\n",
      "|          layers.23.fc1.weight         |  6553600   |\n",
      "|           layers.23.fc1.bias          |    5120    |\n",
      "|          layers.23.fc2.weight         |  6553600   |\n",
      "|           layers.23.fc2.bias          |    1280    |\n",
      "|   layers.23.final_layer_norm.weight   |    1280    |\n",
      "|    layers.23.final_layer_norm.bias    |    1280    |\n",
      "|   layers.24.self_attn.k_proj.weight   |  1638400   |\n",
      "|    layers.24.self_attn.k_proj.bias    |    1280    |\n",
      "|   layers.24.self_attn.v_proj.weight   |  1638400   |\n",
      "|    layers.24.self_attn.v_proj.bias    |    1280    |\n",
      "|   layers.24.self_attn.q_proj.weight   |  1638400   |\n",
      "|    layers.24.self_attn.q_proj.bias    |    1280    |\n",
      "|  layers.24.self_attn.out_proj.weight  |  1638400   |\n",
      "|   layers.24.self_attn.out_proj.bias   |    1280    |\n",
      "| layers.24.self_attn_layer_norm.weight |    1280    |\n",
      "|  layers.24.self_attn_layer_norm.bias  |    1280    |\n",
      "|          layers.24.fc1.weight         |  6553600   |\n",
      "|           layers.24.fc1.bias          |    5120    |\n",
      "|          layers.24.fc2.weight         |  6553600   |\n",
      "|           layers.24.fc2.bias          |    1280    |\n",
      "|   layers.24.final_layer_norm.weight   |    1280    |\n",
      "|    layers.24.final_layer_norm.bias    |    1280    |\n",
      "|   layers.25.self_attn.k_proj.weight   |  1638400   |\n",
      "|    layers.25.self_attn.k_proj.bias    |    1280    |\n",
      "|   layers.25.self_attn.v_proj.weight   |  1638400   |\n",
      "|    layers.25.self_attn.v_proj.bias    |    1280    |\n",
      "|   layers.25.self_attn.q_proj.weight   |  1638400   |\n",
      "|    layers.25.self_attn.q_proj.bias    |    1280    |\n",
      "|  layers.25.self_attn.out_proj.weight  |  1638400   |\n",
      "|   layers.25.self_attn.out_proj.bias   |    1280    |\n",
      "| layers.25.self_attn_layer_norm.weight |    1280    |\n",
      "|  layers.25.self_attn_layer_norm.bias  |    1280    |\n",
      "|          layers.25.fc1.weight         |  6553600   |\n",
      "|           layers.25.fc1.bias          |    5120    |\n",
      "|          layers.25.fc2.weight         |  6553600   |\n",
      "|           layers.25.fc2.bias          |    1280    |\n",
      "|   layers.25.final_layer_norm.weight   |    1280    |\n",
      "|    layers.25.final_layer_norm.bias    |    1280    |\n",
      "|   layers.26.self_attn.k_proj.weight   |  1638400   |\n",
      "|    layers.26.self_attn.k_proj.bias    |    1280    |\n",
      "|   layers.26.self_attn.v_proj.weight   |  1638400   |\n",
      "|    layers.26.self_attn.v_proj.bias    |    1280    |\n",
      "|   layers.26.self_attn.q_proj.weight   |  1638400   |\n",
      "|    layers.26.self_attn.q_proj.bias    |    1280    |\n",
      "|  layers.26.self_attn.out_proj.weight  |  1638400   |\n",
      "|   layers.26.self_attn.out_proj.bias   |    1280    |\n",
      "| layers.26.self_attn_layer_norm.weight |    1280    |\n",
      "|  layers.26.self_attn_layer_norm.bias  |    1280    |\n",
      "|          layers.26.fc1.weight         |  6553600   |\n",
      "|           layers.26.fc1.bias          |    5120    |\n",
      "|          layers.26.fc2.weight         |  6553600   |\n",
      "|           layers.26.fc2.bias          |    1280    |\n",
      "|   layers.26.final_layer_norm.weight   |    1280    |\n",
      "|    layers.26.final_layer_norm.bias    |    1280    |\n",
      "|   layers.27.self_attn.k_proj.weight   |  1638400   |\n",
      "|    layers.27.self_attn.k_proj.bias    |    1280    |\n",
      "|   layers.27.self_attn.v_proj.weight   |  1638400   |\n",
      "|    layers.27.self_attn.v_proj.bias    |    1280    |\n",
      "|   layers.27.self_attn.q_proj.weight   |  1638400   |\n",
      "|    layers.27.self_attn.q_proj.bias    |    1280    |\n",
      "|  layers.27.self_attn.out_proj.weight  |  1638400   |\n",
      "|   layers.27.self_attn.out_proj.bias   |    1280    |\n",
      "| layers.27.self_attn_layer_norm.weight |    1280    |\n",
      "|  layers.27.self_attn_layer_norm.bias  |    1280    |\n",
      "|          layers.27.fc1.weight         |  6553600   |\n",
      "|           layers.27.fc1.bias          |    5120    |\n",
      "|          layers.27.fc2.weight         |  6553600   |\n",
      "|           layers.27.fc2.bias          |    1280    |\n",
      "|   layers.27.final_layer_norm.weight   |    1280    |\n",
      "|    layers.27.final_layer_norm.bias    |    1280    |\n",
      "|   layers.28.self_attn.k_proj.weight   |  1638400   |\n",
      "|    layers.28.self_attn.k_proj.bias    |    1280    |\n",
      "|   layers.28.self_attn.v_proj.weight   |  1638400   |\n",
      "|    layers.28.self_attn.v_proj.bias    |    1280    |\n",
      "|   layers.28.self_attn.q_proj.weight   |  1638400   |\n",
      "|    layers.28.self_attn.q_proj.bias    |    1280    |\n",
      "|  layers.28.self_attn.out_proj.weight  |  1638400   |\n",
      "|   layers.28.self_attn.out_proj.bias   |    1280    |\n",
      "| layers.28.self_attn_layer_norm.weight |    1280    |\n",
      "|  layers.28.self_attn_layer_norm.bias  |    1280    |\n",
      "|          layers.28.fc1.weight         |  6553600   |\n",
      "|           layers.28.fc1.bias          |    5120    |\n",
      "|          layers.28.fc2.weight         |  6553600   |\n",
      "|           layers.28.fc2.bias          |    1280    |\n",
      "|   layers.28.final_layer_norm.weight   |    1280    |\n",
      "|    layers.28.final_layer_norm.bias    |    1280    |\n",
      "|   layers.29.self_attn.k_proj.weight   |  1638400   |\n",
      "|    layers.29.self_attn.k_proj.bias    |    1280    |\n",
      "|   layers.29.self_attn.v_proj.weight   |  1638400   |\n",
      "|    layers.29.self_attn.v_proj.bias    |    1280    |\n",
      "|   layers.29.self_attn.q_proj.weight   |  1638400   |\n",
      "|    layers.29.self_attn.q_proj.bias    |    1280    |\n",
      "|  layers.29.self_attn.out_proj.weight  |  1638400   |\n",
      "|   layers.29.self_attn.out_proj.bias   |    1280    |\n",
      "| layers.29.self_attn_layer_norm.weight |    1280    |\n",
      "|  layers.29.self_attn_layer_norm.bias  |    1280    |\n",
      "|          layers.29.fc1.weight         |  6553600   |\n",
      "|           layers.29.fc1.bias          |    5120    |\n",
      "|          layers.29.fc2.weight         |  6553600   |\n",
      "|           layers.29.fc2.bias          |    1280    |\n",
      "|   layers.29.final_layer_norm.weight   |    1280    |\n",
      "|    layers.29.final_layer_norm.bias    |    1280    |\n",
      "|   layers.30.self_attn.k_proj.weight   |  1638400   |\n",
      "|    layers.30.self_attn.k_proj.bias    |    1280    |\n",
      "|   layers.30.self_attn.v_proj.weight   |  1638400   |\n",
      "|    layers.30.self_attn.v_proj.bias    |    1280    |\n",
      "|   layers.30.self_attn.q_proj.weight   |  1638400   |\n",
      "|    layers.30.self_attn.q_proj.bias    |    1280    |\n",
      "|  layers.30.self_attn.out_proj.weight  |  1638400   |\n",
      "|   layers.30.self_attn.out_proj.bias   |    1280    |\n",
      "| layers.30.self_attn_layer_norm.weight |    1280    |\n",
      "|  layers.30.self_attn_layer_norm.bias  |    1280    |\n",
      "|          layers.30.fc1.weight         |  6553600   |\n",
      "|           layers.30.fc1.bias          |    5120    |\n",
      "|          layers.30.fc2.weight         |  6553600   |\n",
      "|           layers.30.fc2.bias          |    1280    |\n",
      "|   layers.30.final_layer_norm.weight   |    1280    |\n",
      "|    layers.30.final_layer_norm.bias    |    1280    |\n",
      "|   layers.31.self_attn.k_proj.weight   |  1638400   |\n",
      "|    layers.31.self_attn.k_proj.bias    |    1280    |\n",
      "|   layers.31.self_attn.v_proj.weight   |  1638400   |\n",
      "|    layers.31.self_attn.v_proj.bias    |    1280    |\n",
      "|   layers.31.self_attn.q_proj.weight   |  1638400   |\n",
      "|    layers.31.self_attn.q_proj.bias    |    1280    |\n",
      "|  layers.31.self_attn.out_proj.weight  |  1638400   |\n",
      "|   layers.31.self_attn.out_proj.bias   |    1280    |\n",
      "| layers.31.self_attn_layer_norm.weight |    1280    |\n",
      "|  layers.31.self_attn_layer_norm.bias  |    1280    |\n",
      "|          layers.31.fc1.weight         |  6553600   |\n",
      "|           layers.31.fc1.bias          |    5120    |\n",
      "|          layers.31.fc2.weight         |  6553600   |\n",
      "|           layers.31.fc2.bias          |    1280    |\n",
      "|   layers.31.final_layer_norm.weight   |    1280    |\n",
      "|    layers.31.final_layer_norm.bias    |    1280    |\n",
      "|   layers.32.self_attn.k_proj.weight   |  1638400   |\n",
      "|    layers.32.self_attn.k_proj.bias    |    1280    |\n",
      "|   layers.32.self_attn.v_proj.weight   |  1638400   |\n",
      "|    layers.32.self_attn.v_proj.bias    |    1280    |\n",
      "|   layers.32.self_attn.q_proj.weight   |  1638400   |\n",
      "|    layers.32.self_attn.q_proj.bias    |    1280    |\n",
      "|  layers.32.self_attn.out_proj.weight  |  1638400   |\n",
      "|   layers.32.self_attn.out_proj.bias   |    1280    |\n",
      "| layers.32.self_attn_layer_norm.weight |    1280    |\n",
      "|  layers.32.self_attn_layer_norm.bias  |    1280    |\n",
      "|          layers.32.fc1.weight         |  6553600   |\n",
      "|           layers.32.fc1.bias          |    5120    |\n",
      "|          layers.32.fc2.weight         |  6553600   |\n",
      "|           layers.32.fc2.bias          |    1280    |\n",
      "|   layers.32.final_layer_norm.weight   |    1280    |\n",
      "|    layers.32.final_layer_norm.bias    |    1280    |\n",
      "|     contact_head.regression.weight    |    660     |\n",
      "|      contact_head.regression.bias     |     1      |\n",
      "|         embed_positions.weight        |  1313280   |\n",
      "|      emb_layer_norm_before.weight     |    1280    |\n",
      "|       emb_layer_norm_before.bias      |    1280    |\n",
      "|      emb_layer_norm_after.weight      |    1280    |\n",
      "|       emb_layer_norm_after.bias       |    1280    |\n",
      "|              lm_head.bias             |     33     |\n",
      "|          lm_head.dense.weight         |  1638400   |\n",
      "|           lm_head.dense.bias          |    1280    |\n",
      "|       lm_head.layer_norm.weight       |    1280    |\n",
      "|        lm_head.layer_norm.bias        |    1280    |\n",
      "+---------------------------------------+------------+\n",
      "Total Trainable Params: 652359094\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "652359094"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(esm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "652,359,094"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T22:38:34.591475Z",
     "start_time": "2021-12-08T22:38:34.589982Z"
    }
   },
   "outputs": [],
   "source": [
    "# for child in esm_model.children():\n",
    "#     for param in child.parameters():\n",
    "#         param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T22:38:34.601165Z",
     "start_time": "2021-12-08T22:38:34.592718Z"
    }
   },
   "outputs": [],
   "source": [
    "class ESMDataset(Dataset):\n",
    "    def __init__(self, datasets, idxes):\n",
    "        pep_idx, nterm_idx, cterm_idx, m1term_idx, m2term_idx, label_idx = idxes\n",
    "        pep_data = [(label, seq) for label, seq in zip(datasets[:, label_idx], datasets[:, pep_idx])]\n",
    "        nterm_data = [(label, seq) for label, seq in zip(datasets[:, label_idx], datasets[:, nterm_idx])]\n",
    "        cterm_data = [(label, seq) for label, seq in zip(datasets[:, label_idx], datasets[:, cterm_idx])]\n",
    "        m1_data = [(label, seq) for label, seq in zip(datasets[:, label_idx], datasets[:, m1term_idx])]\n",
    "        m2_data = [(label, seq) for label, seq in zip(datasets[:, label_idx], datasets[:, m2term_idx])]\n",
    "        \n",
    "        labels, pep_strs, pep_tokens = batch_converter(pep_data)\n",
    "        _, n_strs, n_tokens = batch_converter(nterm_data)\n",
    "        _, c_strs, c_tokens = batch_converter(cterm_data)\n",
    "        _, m1_strs, m1_tokens = batch_converter(m1_data)\n",
    "        _, m2_strs, m2_tokens = batch_converter(m2_data)\n",
    "\n",
    "        self.sentences = [pep_tokens, n_tokens, c_tokens, m1_tokens, m2_tokens]\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (\n",
    "                (self.sentences[0][i], ) + (self.sentences[1][i],) \\\n",
    "                + (self.sentences[2][i], ) + (self.sentences[3][i], ) + (self.sentences[4][i], ) \\\n",
    "                + (self.labels[i], )\n",
    "               )\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))\n",
    "\n",
    "# [PAD] = 1, [MASK] = 21  [CLS] = 0 (special classification token), [SEP] = 2 (seperate segment), Z = 27, '-' = 30, .=29, ,=28\n",
    "# J 없음\n",
    "# A 2, B 25, C 23, D 13, E 9, F 18, G 6, H 21, I 12, K 15, L 4, M 20, N 17, \n",
    "# O 28, P 14, Q 16, R 10, S 8, T 11, U 26, V 7, W 22, X 24, Y 19, Z 27\n",
    "# 3 5 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T22:38:54.086209Z",
     "start_time": "2021-12-08T22:38:54.083728Z"
    }
   },
   "outputs": [],
   "source": [
    "## Setting parameters\n",
    "max_len = 30\n",
    "batch_size = 4\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 3\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate =  1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T22:46:47.866888Z",
     "start_time": "2021-12-08T22:38:55.643518Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472.22 sec\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "\n",
    "dataset_train = train[['PEP', 'En', 'Ec', 'E1', 'E2', 'ID']].values\n",
    "dataset_valid = val[['PEP', 'En', 'Ec', 'E1', 'E2', 'ID']].values\n",
    "dataset_test = test[['PEP', 'En', 'Ec', 'E1', 'E2', 'ID']].values\n",
    "\n",
    "data_train = ESMDataset(dataset_train, [0, 1, 2, 3, 4, 5])\n",
    "data_valid = ESMDataset(dataset_valid, [0, 1, 2, 3, 4, 5])\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=48)\n",
    "valid_dataloader = torch.utils.data.DataLoader(data_valid, batch_size=batch_size, num_workers=48)\n",
    "\n",
    "e = time.time()\n",
    "print(round(e-s, 2),'sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T22:46:47.876344Z",
     "start_time": "2021-12-08T22:46:47.868204Z"
    }
   },
   "outputs": [],
   "source": [
    "class ESMClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 esm,\n",
    "                 num_classes=1,\n",
    "                 params=None):\n",
    "        \n",
    "        super(ESMClassifier, self).__init__()\n",
    "        self.esm = esm\n",
    "        self.pep_lstm1 = nn.LSTM(input_size=1280, hidden_size=1, batch_first=True)        \n",
    "        self.ts_lstm1 = nn.LSTM(input_size=1280, hidden_size=1, batch_first=True)\n",
    "                \n",
    "        self.fc1 = nn.Linear(5, 1)\n",
    "\n",
    "\n",
    "#     def gen_attention_mask(self, token_ids, valid_length):\n",
    "#         attention_mask = torch.zeros_like(token_ids)\n",
    "#         for i, v in enumerate(valid_length):\n",
    "#             attention_mask[i][:v] = 1\n",
    "#         return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, en_token_ids, ec_token_ids, e1_token_ids, e2_token_ids):\n",
    "#         attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        pep_embed = self.esm(token_ids, repr_layers=[33])['representations'][33]\n",
    "        pep_lstm, (pep_hn, __) = self.pep_lstm1(pep_embed)\n",
    "        \n",
    "        en_embed = self.esm(en_token_ids, repr_layers=[33])['representations'][33]\n",
    "        ec_embed = self.esm(ec_token_ids, repr_layers=[33])['representations'][33]\n",
    "        e1_embed = self.esm(e1_token_ids, repr_layers=[33])['representations'][33]\n",
    "        e2_embed = self.esm(e2_token_ids, repr_layers=[33])['representations'][33]\n",
    "        \n",
    "        en_lstm, (en_hn, __) = self.ts_lstm1(en_embed)\n",
    "        \n",
    "        ec_lstm, (ec_hn, __) = self.ts_lstm1(ec_embed)\n",
    "        \n",
    "        e1_lstm, (e1_hn, __) = self.ts_lstm1(e1_embed)\n",
    "        \n",
    "        e2_lstm, (e2_hn, __) = self.ts_lstm1(e2_embed)\n",
    "        \n",
    "        merge = torch.cat([pep_hn[0], en_hn[0], ec_hn[0], e1_hn[0], e2_hn[0]], dim=1)\n",
    "\n",
    "        merge = self.fc1(merge)        \n",
    "        out = torch.sigmoid(merge)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T22:47:00.103636Z",
     "start_time": "2021-12-08T22:46:47.877607Z"
    }
   },
   "outputs": [],
   "source": [
    "model = ESMClassifier(esm_model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T22:47:00.111995Z",
     "start_time": "2021-12-08T22:47:00.105017Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ESMClassifier(\n",
       "  (esm): ProteinBertModel(\n",
       "    (embed_tokens): Embedding(33, 1280, padding_idx=1)\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (12): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (13): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (14): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (15): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (16): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (17): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (18): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (19): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (20): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (21): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (22): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (23): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (24): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (25): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (26): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (27): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (28): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (29): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (30): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (31): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (32): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (contact_head): ContactPredictionHead(\n",
       "      (regression): Linear(in_features=660, out_features=1, bias=True)\n",
       "      (activation): Sigmoid()\n",
       "    )\n",
       "    (embed_positions): LearnedPositionalEmbedding(1026, 1280, padding_idx=1)\n",
       "    (emb_layer_norm_before): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    (emb_layer_norm_after): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    (lm_head): RobertaLMHead(\n",
       "      (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (pep_lstm1): LSTM(1280, 1, batch_first=True)\n",
       "  (ts_lstm1): LSTM(1280, 1, batch_first=True)\n",
       "  (fc1): Linear(in_features=5, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T22:47:00.158599Z",
     "start_time": "2021-12-08T22:47:00.112948Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+------------+\n",
      "|                  Modules                  | Parameters |\n",
      "+-------------------------------------------+------------+\n",
      "|          esm.embed_tokens.weight          |   42240    |\n",
      "|    esm.layers.0.self_attn.k_proj.weight   |  1638400   |\n",
      "|     esm.layers.0.self_attn.k_proj.bias    |    1280    |\n",
      "|    esm.layers.0.self_attn.v_proj.weight   |  1638400   |\n",
      "|     esm.layers.0.self_attn.v_proj.bias    |    1280    |\n",
      "|    esm.layers.0.self_attn.q_proj.weight   |  1638400   |\n",
      "|     esm.layers.0.self_attn.q_proj.bias    |    1280    |\n",
      "|   esm.layers.0.self_attn.out_proj.weight  |  1638400   |\n",
      "|    esm.layers.0.self_attn.out_proj.bias   |    1280    |\n",
      "|  esm.layers.0.self_attn_layer_norm.weight |    1280    |\n",
      "|   esm.layers.0.self_attn_layer_norm.bias  |    1280    |\n",
      "|          esm.layers.0.fc1.weight          |  6553600   |\n",
      "|           esm.layers.0.fc1.bias           |    5120    |\n",
      "|          esm.layers.0.fc2.weight          |  6553600   |\n",
      "|           esm.layers.0.fc2.bias           |    1280    |\n",
      "|    esm.layers.0.final_layer_norm.weight   |    1280    |\n",
      "|     esm.layers.0.final_layer_norm.bias    |    1280    |\n",
      "|    esm.layers.1.self_attn.k_proj.weight   |  1638400   |\n",
      "|     esm.layers.1.self_attn.k_proj.bias    |    1280    |\n",
      "|    esm.layers.1.self_attn.v_proj.weight   |  1638400   |\n",
      "|     esm.layers.1.self_attn.v_proj.bias    |    1280    |\n",
      "|    esm.layers.1.self_attn.q_proj.weight   |  1638400   |\n",
      "|     esm.layers.1.self_attn.q_proj.bias    |    1280    |\n",
      "|   esm.layers.1.self_attn.out_proj.weight  |  1638400   |\n",
      "|    esm.layers.1.self_attn.out_proj.bias   |    1280    |\n",
      "|  esm.layers.1.self_attn_layer_norm.weight |    1280    |\n",
      "|   esm.layers.1.self_attn_layer_norm.bias  |    1280    |\n",
      "|          esm.layers.1.fc1.weight          |  6553600   |\n",
      "|           esm.layers.1.fc1.bias           |    5120    |\n",
      "|          esm.layers.1.fc2.weight          |  6553600   |\n",
      "|           esm.layers.1.fc2.bias           |    1280    |\n",
      "|    esm.layers.1.final_layer_norm.weight   |    1280    |\n",
      "|     esm.layers.1.final_layer_norm.bias    |    1280    |\n",
      "|    esm.layers.2.self_attn.k_proj.weight   |  1638400   |\n",
      "|     esm.layers.2.self_attn.k_proj.bias    |    1280    |\n",
      "|    esm.layers.2.self_attn.v_proj.weight   |  1638400   |\n",
      "|     esm.layers.2.self_attn.v_proj.bias    |    1280    |\n",
      "|    esm.layers.2.self_attn.q_proj.weight   |  1638400   |\n",
      "|     esm.layers.2.self_attn.q_proj.bias    |    1280    |\n",
      "|   esm.layers.2.self_attn.out_proj.weight  |  1638400   |\n",
      "|    esm.layers.2.self_attn.out_proj.bias   |    1280    |\n",
      "|  esm.layers.2.self_attn_layer_norm.weight |    1280    |\n",
      "|   esm.layers.2.self_attn_layer_norm.bias  |    1280    |\n",
      "|          esm.layers.2.fc1.weight          |  6553600   |\n",
      "|           esm.layers.2.fc1.bias           |    5120    |\n",
      "|          esm.layers.2.fc2.weight          |  6553600   |\n",
      "|           esm.layers.2.fc2.bias           |    1280    |\n",
      "|    esm.layers.2.final_layer_norm.weight   |    1280    |\n",
      "|     esm.layers.2.final_layer_norm.bias    |    1280    |\n",
      "|    esm.layers.3.self_attn.k_proj.weight   |  1638400   |\n",
      "|     esm.layers.3.self_attn.k_proj.bias    |    1280    |\n",
      "|    esm.layers.3.self_attn.v_proj.weight   |  1638400   |\n",
      "|     esm.layers.3.self_attn.v_proj.bias    |    1280    |\n",
      "|    esm.layers.3.self_attn.q_proj.weight   |  1638400   |\n",
      "|     esm.layers.3.self_attn.q_proj.bias    |    1280    |\n",
      "|   esm.layers.3.self_attn.out_proj.weight  |  1638400   |\n",
      "|    esm.layers.3.self_attn.out_proj.bias   |    1280    |\n",
      "|  esm.layers.3.self_attn_layer_norm.weight |    1280    |\n",
      "|   esm.layers.3.self_attn_layer_norm.bias  |    1280    |\n",
      "|          esm.layers.3.fc1.weight          |  6553600   |\n",
      "|           esm.layers.3.fc1.bias           |    5120    |\n",
      "|          esm.layers.3.fc2.weight          |  6553600   |\n",
      "|           esm.layers.3.fc2.bias           |    1280    |\n",
      "|    esm.layers.3.final_layer_norm.weight   |    1280    |\n",
      "|     esm.layers.3.final_layer_norm.bias    |    1280    |\n",
      "|    esm.layers.4.self_attn.k_proj.weight   |  1638400   |\n",
      "|     esm.layers.4.self_attn.k_proj.bias    |    1280    |\n",
      "|    esm.layers.4.self_attn.v_proj.weight   |  1638400   |\n",
      "|     esm.layers.4.self_attn.v_proj.bias    |    1280    |\n",
      "|    esm.layers.4.self_attn.q_proj.weight   |  1638400   |\n",
      "|     esm.layers.4.self_attn.q_proj.bias    |    1280    |\n",
      "|   esm.layers.4.self_attn.out_proj.weight  |  1638400   |\n",
      "|    esm.layers.4.self_attn.out_proj.bias   |    1280    |\n",
      "|  esm.layers.4.self_attn_layer_norm.weight |    1280    |\n",
      "|   esm.layers.4.self_attn_layer_norm.bias  |    1280    |\n",
      "|          esm.layers.4.fc1.weight          |  6553600   |\n",
      "|           esm.layers.4.fc1.bias           |    5120    |\n",
      "|          esm.layers.4.fc2.weight          |  6553600   |\n",
      "|           esm.layers.4.fc2.bias           |    1280    |\n",
      "|    esm.layers.4.final_layer_norm.weight   |    1280    |\n",
      "|     esm.layers.4.final_layer_norm.bias    |    1280    |\n",
      "|    esm.layers.5.self_attn.k_proj.weight   |  1638400   |\n",
      "|     esm.layers.5.self_attn.k_proj.bias    |    1280    |\n",
      "|    esm.layers.5.self_attn.v_proj.weight   |  1638400   |\n",
      "|     esm.layers.5.self_attn.v_proj.bias    |    1280    |\n",
      "|    esm.layers.5.self_attn.q_proj.weight   |  1638400   |\n",
      "|     esm.layers.5.self_attn.q_proj.bias    |    1280    |\n",
      "|   esm.layers.5.self_attn.out_proj.weight  |  1638400   |\n",
      "|    esm.layers.5.self_attn.out_proj.bias   |    1280    |\n",
      "|  esm.layers.5.self_attn_layer_norm.weight |    1280    |\n",
      "|   esm.layers.5.self_attn_layer_norm.bias  |    1280    |\n",
      "|          esm.layers.5.fc1.weight          |  6553600   |\n",
      "|           esm.layers.5.fc1.bias           |    5120    |\n",
      "|          esm.layers.5.fc2.weight          |  6553600   |\n",
      "|           esm.layers.5.fc2.bias           |    1280    |\n",
      "|    esm.layers.5.final_layer_norm.weight   |    1280    |\n",
      "|     esm.layers.5.final_layer_norm.bias    |    1280    |\n",
      "|    esm.layers.6.self_attn.k_proj.weight   |  1638400   |\n",
      "|     esm.layers.6.self_attn.k_proj.bias    |    1280    |\n",
      "|    esm.layers.6.self_attn.v_proj.weight   |  1638400   |\n",
      "|     esm.layers.6.self_attn.v_proj.bias    |    1280    |\n",
      "|    esm.layers.6.self_attn.q_proj.weight   |  1638400   |\n",
      "|     esm.layers.6.self_attn.q_proj.bias    |    1280    |\n",
      "|   esm.layers.6.self_attn.out_proj.weight  |  1638400   |\n",
      "|    esm.layers.6.self_attn.out_proj.bias   |    1280    |\n",
      "|  esm.layers.6.self_attn_layer_norm.weight |    1280    |\n",
      "|   esm.layers.6.self_attn_layer_norm.bias  |    1280    |\n",
      "|          esm.layers.6.fc1.weight          |  6553600   |\n",
      "|           esm.layers.6.fc1.bias           |    5120    |\n",
      "|          esm.layers.6.fc2.weight          |  6553600   |\n",
      "|           esm.layers.6.fc2.bias           |    1280    |\n",
      "|    esm.layers.6.final_layer_norm.weight   |    1280    |\n",
      "|     esm.layers.6.final_layer_norm.bias    |    1280    |\n",
      "|    esm.layers.7.self_attn.k_proj.weight   |  1638400   |\n",
      "|     esm.layers.7.self_attn.k_proj.bias    |    1280    |\n",
      "|    esm.layers.7.self_attn.v_proj.weight   |  1638400   |\n",
      "|     esm.layers.7.self_attn.v_proj.bias    |    1280    |\n",
      "|    esm.layers.7.self_attn.q_proj.weight   |  1638400   |\n",
      "|     esm.layers.7.self_attn.q_proj.bias    |    1280    |\n",
      "|   esm.layers.7.self_attn.out_proj.weight  |  1638400   |\n",
      "|    esm.layers.7.self_attn.out_proj.bias   |    1280    |\n",
      "|  esm.layers.7.self_attn_layer_norm.weight |    1280    |\n",
      "|   esm.layers.7.self_attn_layer_norm.bias  |    1280    |\n",
      "|          esm.layers.7.fc1.weight          |  6553600   |\n",
      "|           esm.layers.7.fc1.bias           |    5120    |\n",
      "|          esm.layers.7.fc2.weight          |  6553600   |\n",
      "|           esm.layers.7.fc2.bias           |    1280    |\n",
      "|    esm.layers.7.final_layer_norm.weight   |    1280    |\n",
      "|     esm.layers.7.final_layer_norm.bias    |    1280    |\n",
      "|    esm.layers.8.self_attn.k_proj.weight   |  1638400   |\n",
      "|     esm.layers.8.self_attn.k_proj.bias    |    1280    |\n",
      "|    esm.layers.8.self_attn.v_proj.weight   |  1638400   |\n",
      "|     esm.layers.8.self_attn.v_proj.bias    |    1280    |\n",
      "|    esm.layers.8.self_attn.q_proj.weight   |  1638400   |\n",
      "|     esm.layers.8.self_attn.q_proj.bias    |    1280    |\n",
      "|   esm.layers.8.self_attn.out_proj.weight  |  1638400   |\n",
      "|    esm.layers.8.self_attn.out_proj.bias   |    1280    |\n",
      "|  esm.layers.8.self_attn_layer_norm.weight |    1280    |\n",
      "|   esm.layers.8.self_attn_layer_norm.bias  |    1280    |\n",
      "|          esm.layers.8.fc1.weight          |  6553600   |\n",
      "|           esm.layers.8.fc1.bias           |    5120    |\n",
      "|          esm.layers.8.fc2.weight          |  6553600   |\n",
      "|           esm.layers.8.fc2.bias           |    1280    |\n",
      "|    esm.layers.8.final_layer_norm.weight   |    1280    |\n",
      "|     esm.layers.8.final_layer_norm.bias    |    1280    |\n",
      "|    esm.layers.9.self_attn.k_proj.weight   |  1638400   |\n",
      "|     esm.layers.9.self_attn.k_proj.bias    |    1280    |\n",
      "|    esm.layers.9.self_attn.v_proj.weight   |  1638400   |\n",
      "|     esm.layers.9.self_attn.v_proj.bias    |    1280    |\n",
      "|    esm.layers.9.self_attn.q_proj.weight   |  1638400   |\n",
      "|     esm.layers.9.self_attn.q_proj.bias    |    1280    |\n",
      "|   esm.layers.9.self_attn.out_proj.weight  |  1638400   |\n",
      "|    esm.layers.9.self_attn.out_proj.bias   |    1280    |\n",
      "|  esm.layers.9.self_attn_layer_norm.weight |    1280    |\n",
      "|   esm.layers.9.self_attn_layer_norm.bias  |    1280    |\n",
      "|          esm.layers.9.fc1.weight          |  6553600   |\n",
      "|           esm.layers.9.fc1.bias           |    5120    |\n",
      "|          esm.layers.9.fc2.weight          |  6553600   |\n",
      "|           esm.layers.9.fc2.bias           |    1280    |\n",
      "|    esm.layers.9.final_layer_norm.weight   |    1280    |\n",
      "|     esm.layers.9.final_layer_norm.bias    |    1280    |\n",
      "|   esm.layers.10.self_attn.k_proj.weight   |  1638400   |\n",
      "|    esm.layers.10.self_attn.k_proj.bias    |    1280    |\n",
      "|   esm.layers.10.self_attn.v_proj.weight   |  1638400   |\n",
      "|    esm.layers.10.self_attn.v_proj.bias    |    1280    |\n",
      "|   esm.layers.10.self_attn.q_proj.weight   |  1638400   |\n",
      "|    esm.layers.10.self_attn.q_proj.bias    |    1280    |\n",
      "|  esm.layers.10.self_attn.out_proj.weight  |  1638400   |\n",
      "|   esm.layers.10.self_attn.out_proj.bias   |    1280    |\n",
      "| esm.layers.10.self_attn_layer_norm.weight |    1280    |\n",
      "|  esm.layers.10.self_attn_layer_norm.bias  |    1280    |\n",
      "|          esm.layers.10.fc1.weight         |  6553600   |\n",
      "|           esm.layers.10.fc1.bias          |    5120    |\n",
      "|          esm.layers.10.fc2.weight         |  6553600   |\n",
      "|           esm.layers.10.fc2.bias          |    1280    |\n",
      "|   esm.layers.10.final_layer_norm.weight   |    1280    |\n",
      "|    esm.layers.10.final_layer_norm.bias    |    1280    |\n",
      "|   esm.layers.11.self_attn.k_proj.weight   |  1638400   |\n",
      "|    esm.layers.11.self_attn.k_proj.bias    |    1280    |\n",
      "|   esm.layers.11.self_attn.v_proj.weight   |  1638400   |\n",
      "|    esm.layers.11.self_attn.v_proj.bias    |    1280    |\n",
      "|   esm.layers.11.self_attn.q_proj.weight   |  1638400   |\n",
      "|    esm.layers.11.self_attn.q_proj.bias    |    1280    |\n",
      "|  esm.layers.11.self_attn.out_proj.weight  |  1638400   |\n",
      "|   esm.layers.11.self_attn.out_proj.bias   |    1280    |\n",
      "| esm.layers.11.self_attn_layer_norm.weight |    1280    |\n",
      "|  esm.layers.11.self_attn_layer_norm.bias  |    1280    |\n",
      "|          esm.layers.11.fc1.weight         |  6553600   |\n",
      "|           esm.layers.11.fc1.bias          |    5120    |\n",
      "|          esm.layers.11.fc2.weight         |  6553600   |\n",
      "|           esm.layers.11.fc2.bias          |    1280    |\n",
      "|   esm.layers.11.final_layer_norm.weight   |    1280    |\n",
      "|    esm.layers.11.final_layer_norm.bias    |    1280    |\n",
      "|   esm.layers.12.self_attn.k_proj.weight   |  1638400   |\n",
      "|    esm.layers.12.self_attn.k_proj.bias    |    1280    |\n",
      "|   esm.layers.12.self_attn.v_proj.weight   |  1638400   |\n",
      "|    esm.layers.12.self_attn.v_proj.bias    |    1280    |\n",
      "|   esm.layers.12.self_attn.q_proj.weight   |  1638400   |\n",
      "|    esm.layers.12.self_attn.q_proj.bias    |    1280    |\n",
      "|  esm.layers.12.self_attn.out_proj.weight  |  1638400   |\n",
      "|   esm.layers.12.self_attn.out_proj.bias   |    1280    |\n",
      "| esm.layers.12.self_attn_layer_norm.weight |    1280    |\n",
      "|  esm.layers.12.self_attn_layer_norm.bias  |    1280    |\n",
      "|          esm.layers.12.fc1.weight         |  6553600   |\n",
      "|           esm.layers.12.fc1.bias          |    5120    |\n",
      "|          esm.layers.12.fc2.weight         |  6553600   |\n",
      "|           esm.layers.12.fc2.bias          |    1280    |\n",
      "|   esm.layers.12.final_layer_norm.weight   |    1280    |\n",
      "|    esm.layers.12.final_layer_norm.bias    |    1280    |\n",
      "|   esm.layers.13.self_attn.k_proj.weight   |  1638400   |\n",
      "|    esm.layers.13.self_attn.k_proj.bias    |    1280    |\n",
      "|   esm.layers.13.self_attn.v_proj.weight   |  1638400   |\n",
      "|    esm.layers.13.self_attn.v_proj.bias    |    1280    |\n",
      "|   esm.layers.13.self_attn.q_proj.weight   |  1638400   |\n",
      "|    esm.layers.13.self_attn.q_proj.bias    |    1280    |\n",
      "|  esm.layers.13.self_attn.out_proj.weight  |  1638400   |\n",
      "|   esm.layers.13.self_attn.out_proj.bias   |    1280    |\n",
      "| esm.layers.13.self_attn_layer_norm.weight |    1280    |\n",
      "|  esm.layers.13.self_attn_layer_norm.bias  |    1280    |\n",
      "|          esm.layers.13.fc1.weight         |  6553600   |\n",
      "|           esm.layers.13.fc1.bias          |    5120    |\n",
      "|          esm.layers.13.fc2.weight         |  6553600   |\n",
      "|           esm.layers.13.fc2.bias          |    1280    |\n",
      "|   esm.layers.13.final_layer_norm.weight   |    1280    |\n",
      "|    esm.layers.13.final_layer_norm.bias    |    1280    |\n",
      "|   esm.layers.14.self_attn.k_proj.weight   |  1638400   |\n",
      "|    esm.layers.14.self_attn.k_proj.bias    |    1280    |\n",
      "|   esm.layers.14.self_attn.v_proj.weight   |  1638400   |\n",
      "|    esm.layers.14.self_attn.v_proj.bias    |    1280    |\n",
      "|   esm.layers.14.self_attn.q_proj.weight   |  1638400   |\n",
      "|    esm.layers.14.self_attn.q_proj.bias    |    1280    |\n",
      "|  esm.layers.14.self_attn.out_proj.weight  |  1638400   |\n",
      "|   esm.layers.14.self_attn.out_proj.bias   |    1280    |\n",
      "| esm.layers.14.self_attn_layer_norm.weight |    1280    |\n",
      "|  esm.layers.14.self_attn_layer_norm.bias  |    1280    |\n",
      "|          esm.layers.14.fc1.weight         |  6553600   |\n",
      "|           esm.layers.14.fc1.bias          |    5120    |\n",
      "|          esm.layers.14.fc2.weight         |  6553600   |\n",
      "|           esm.layers.14.fc2.bias          |    1280    |\n",
      "|   esm.layers.14.final_layer_norm.weight   |    1280    |\n",
      "|    esm.layers.14.final_layer_norm.bias    |    1280    |\n",
      "|   esm.layers.15.self_attn.k_proj.weight   |  1638400   |\n",
      "|    esm.layers.15.self_attn.k_proj.bias    |    1280    |\n",
      "|   esm.layers.15.self_attn.v_proj.weight   |  1638400   |\n",
      "|    esm.layers.15.self_attn.v_proj.bias    |    1280    |\n",
      "|   esm.layers.15.self_attn.q_proj.weight   |  1638400   |\n",
      "|    esm.layers.15.self_attn.q_proj.bias    |    1280    |\n",
      "|  esm.layers.15.self_attn.out_proj.weight  |  1638400   |\n",
      "|   esm.layers.15.self_attn.out_proj.bias   |    1280    |\n",
      "| esm.layers.15.self_attn_layer_norm.weight |    1280    |\n",
      "|  esm.layers.15.self_attn_layer_norm.bias  |    1280    |\n",
      "|          esm.layers.15.fc1.weight         |  6553600   |\n",
      "|           esm.layers.15.fc1.bias          |    5120    |\n",
      "|          esm.layers.15.fc2.weight         |  6553600   |\n",
      "|           esm.layers.15.fc2.bias          |    1280    |\n",
      "|   esm.layers.15.final_layer_norm.weight   |    1280    |\n",
      "|    esm.layers.15.final_layer_norm.bias    |    1280    |\n",
      "|   esm.layers.16.self_attn.k_proj.weight   |  1638400   |\n",
      "|    esm.layers.16.self_attn.k_proj.bias    |    1280    |\n",
      "|   esm.layers.16.self_attn.v_proj.weight   |  1638400   |\n",
      "|    esm.layers.16.self_attn.v_proj.bias    |    1280    |\n",
      "|   esm.layers.16.self_attn.q_proj.weight   |  1638400   |\n",
      "|    esm.layers.16.self_attn.q_proj.bias    |    1280    |\n",
      "|  esm.layers.16.self_attn.out_proj.weight  |  1638400   |\n",
      "|   esm.layers.16.self_attn.out_proj.bias   |    1280    |\n",
      "| esm.layers.16.self_attn_layer_norm.weight |    1280    |\n",
      "|  esm.layers.16.self_attn_layer_norm.bias  |    1280    |\n",
      "|          esm.layers.16.fc1.weight         |  6553600   |\n",
      "|           esm.layers.16.fc1.bias          |    5120    |\n",
      "|          esm.layers.16.fc2.weight         |  6553600   |\n",
      "|           esm.layers.16.fc2.bias          |    1280    |\n",
      "|   esm.layers.16.final_layer_norm.weight   |    1280    |\n",
      "|    esm.layers.16.final_layer_norm.bias    |    1280    |\n",
      "|   esm.layers.17.self_attn.k_proj.weight   |  1638400   |\n",
      "|    esm.layers.17.self_attn.k_proj.bias    |    1280    |\n",
      "|   esm.layers.17.self_attn.v_proj.weight   |  1638400   |\n",
      "|    esm.layers.17.self_attn.v_proj.bias    |    1280    |\n",
      "|   esm.layers.17.self_attn.q_proj.weight   |  1638400   |\n",
      "|    esm.layers.17.self_attn.q_proj.bias    |    1280    |\n",
      "|  esm.layers.17.self_attn.out_proj.weight  |  1638400   |\n",
      "|   esm.layers.17.self_attn.out_proj.bias   |    1280    |\n",
      "| esm.layers.17.self_attn_layer_norm.weight |    1280    |\n",
      "|  esm.layers.17.self_attn_layer_norm.bias  |    1280    |\n",
      "|          esm.layers.17.fc1.weight         |  6553600   |\n",
      "|           esm.layers.17.fc1.bias          |    5120    |\n",
      "|          esm.layers.17.fc2.weight         |  6553600   |\n",
      "|           esm.layers.17.fc2.bias          |    1280    |\n",
      "|   esm.layers.17.final_layer_norm.weight   |    1280    |\n",
      "|    esm.layers.17.final_layer_norm.bias    |    1280    |\n",
      "|   esm.layers.18.self_attn.k_proj.weight   |  1638400   |\n",
      "|    esm.layers.18.self_attn.k_proj.bias    |    1280    |\n",
      "|   esm.layers.18.self_attn.v_proj.weight   |  1638400   |\n",
      "|    esm.layers.18.self_attn.v_proj.bias    |    1280    |\n",
      "|   esm.layers.18.self_attn.q_proj.weight   |  1638400   |\n",
      "|    esm.layers.18.self_attn.q_proj.bias    |    1280    |\n",
      "|  esm.layers.18.self_attn.out_proj.weight  |  1638400   |\n",
      "|   esm.layers.18.self_attn.out_proj.bias   |    1280    |\n",
      "| esm.layers.18.self_attn_layer_norm.weight |    1280    |\n",
      "|  esm.layers.18.self_attn_layer_norm.bias  |    1280    |\n",
      "|          esm.layers.18.fc1.weight         |  6553600   |\n",
      "|           esm.layers.18.fc1.bias          |    5120    |\n",
      "|          esm.layers.18.fc2.weight         |  6553600   |\n",
      "|           esm.layers.18.fc2.bias          |    1280    |\n",
      "|   esm.layers.18.final_layer_norm.weight   |    1280    |\n",
      "|    esm.layers.18.final_layer_norm.bias    |    1280    |\n",
      "|   esm.layers.19.self_attn.k_proj.weight   |  1638400   |\n",
      "|    esm.layers.19.self_attn.k_proj.bias    |    1280    |\n",
      "|   esm.layers.19.self_attn.v_proj.weight   |  1638400   |\n",
      "|    esm.layers.19.self_attn.v_proj.bias    |    1280    |\n",
      "|   esm.layers.19.self_attn.q_proj.weight   |  1638400   |\n",
      "|    esm.layers.19.self_attn.q_proj.bias    |    1280    |\n",
      "|  esm.layers.19.self_attn.out_proj.weight  |  1638400   |\n",
      "|   esm.layers.19.self_attn.out_proj.bias   |    1280    |\n",
      "| esm.layers.19.self_attn_layer_norm.weight |    1280    |\n",
      "|  esm.layers.19.self_attn_layer_norm.bias  |    1280    |\n",
      "|          esm.layers.19.fc1.weight         |  6553600   |\n",
      "|           esm.layers.19.fc1.bias          |    5120    |\n",
      "|          esm.layers.19.fc2.weight         |  6553600   |\n",
      "|           esm.layers.19.fc2.bias          |    1280    |\n",
      "|   esm.layers.19.final_layer_norm.weight   |    1280    |\n",
      "|    esm.layers.19.final_layer_norm.bias    |    1280    |\n",
      "|   esm.layers.20.self_attn.k_proj.weight   |  1638400   |\n",
      "|    esm.layers.20.self_attn.k_proj.bias    |    1280    |\n",
      "|   esm.layers.20.self_attn.v_proj.weight   |  1638400   |\n",
      "|    esm.layers.20.self_attn.v_proj.bias    |    1280    |\n",
      "|   esm.layers.20.self_attn.q_proj.weight   |  1638400   |\n",
      "|    esm.layers.20.self_attn.q_proj.bias    |    1280    |\n",
      "|  esm.layers.20.self_attn.out_proj.weight  |  1638400   |\n",
      "|   esm.layers.20.self_attn.out_proj.bias   |    1280    |\n",
      "| esm.layers.20.self_attn_layer_norm.weight |    1280    |\n",
      "|  esm.layers.20.self_attn_layer_norm.bias  |    1280    |\n",
      "|          esm.layers.20.fc1.weight         |  6553600   |\n",
      "|           esm.layers.20.fc1.bias          |    5120    |\n",
      "|          esm.layers.20.fc2.weight         |  6553600   |\n",
      "|           esm.layers.20.fc2.bias          |    1280    |\n",
      "|   esm.layers.20.final_layer_norm.weight   |    1280    |\n",
      "|    esm.layers.20.final_layer_norm.bias    |    1280    |\n",
      "|   esm.layers.21.self_attn.k_proj.weight   |  1638400   |\n",
      "|    esm.layers.21.self_attn.k_proj.bias    |    1280    |\n",
      "|   esm.layers.21.self_attn.v_proj.weight   |  1638400   |\n",
      "|    esm.layers.21.self_attn.v_proj.bias    |    1280    |\n",
      "|   esm.layers.21.self_attn.q_proj.weight   |  1638400   |\n",
      "|    esm.layers.21.self_attn.q_proj.bias    |    1280    |\n",
      "|  esm.layers.21.self_attn.out_proj.weight  |  1638400   |\n",
      "|   esm.layers.21.self_attn.out_proj.bias   |    1280    |\n",
      "| esm.layers.21.self_attn_layer_norm.weight |    1280    |\n",
      "|  esm.layers.21.self_attn_layer_norm.bias  |    1280    |\n",
      "|          esm.layers.21.fc1.weight         |  6553600   |\n",
      "|           esm.layers.21.fc1.bias          |    5120    |\n",
      "|          esm.layers.21.fc2.weight         |  6553600   |\n",
      "|           esm.layers.21.fc2.bias          |    1280    |\n",
      "|   esm.layers.21.final_layer_norm.weight   |    1280    |\n",
      "|    esm.layers.21.final_layer_norm.bias    |    1280    |\n",
      "|   esm.layers.22.self_attn.k_proj.weight   |  1638400   |\n",
      "|    esm.layers.22.self_attn.k_proj.bias    |    1280    |\n",
      "|   esm.layers.22.self_attn.v_proj.weight   |  1638400   |\n",
      "|    esm.layers.22.self_attn.v_proj.bias    |    1280    |\n",
      "|   esm.layers.22.self_attn.q_proj.weight   |  1638400   |\n",
      "|    esm.layers.22.self_attn.q_proj.bias    |    1280    |\n",
      "|  esm.layers.22.self_attn.out_proj.weight  |  1638400   |\n",
      "|   esm.layers.22.self_attn.out_proj.bias   |    1280    |\n",
      "| esm.layers.22.self_attn_layer_norm.weight |    1280    |\n",
      "|  esm.layers.22.self_attn_layer_norm.bias  |    1280    |\n",
      "|          esm.layers.22.fc1.weight         |  6553600   |\n",
      "|           esm.layers.22.fc1.bias          |    5120    |\n",
      "|          esm.layers.22.fc2.weight         |  6553600   |\n",
      "|           esm.layers.22.fc2.bias          |    1280    |\n",
      "|   esm.layers.22.final_layer_norm.weight   |    1280    |\n",
      "|    esm.layers.22.final_layer_norm.bias    |    1280    |\n",
      "|   esm.layers.23.self_attn.k_proj.weight   |  1638400   |\n",
      "|    esm.layers.23.self_attn.k_proj.bias    |    1280    |\n",
      "|   esm.layers.23.self_attn.v_proj.weight   |  1638400   |\n",
      "|    esm.layers.23.self_attn.v_proj.bias    |    1280    |\n",
      "|   esm.layers.23.self_attn.q_proj.weight   |  1638400   |\n",
      "|    esm.layers.23.self_attn.q_proj.bias    |    1280    |\n",
      "|  esm.layers.23.self_attn.out_proj.weight  |  1638400   |\n",
      "|   esm.layers.23.self_attn.out_proj.bias   |    1280    |\n",
      "| esm.layers.23.self_attn_layer_norm.weight |    1280    |\n",
      "|  esm.layers.23.self_attn_layer_norm.bias  |    1280    |\n",
      "|          esm.layers.23.fc1.weight         |  6553600   |\n",
      "|           esm.layers.23.fc1.bias          |    5120    |\n",
      "|          esm.layers.23.fc2.weight         |  6553600   |\n",
      "|           esm.layers.23.fc2.bias          |    1280    |\n",
      "|   esm.layers.23.final_layer_norm.weight   |    1280    |\n",
      "|    esm.layers.23.final_layer_norm.bias    |    1280    |\n",
      "|   esm.layers.24.self_attn.k_proj.weight   |  1638400   |\n",
      "|    esm.layers.24.self_attn.k_proj.bias    |    1280    |\n",
      "|   esm.layers.24.self_attn.v_proj.weight   |  1638400   |\n",
      "|    esm.layers.24.self_attn.v_proj.bias    |    1280    |\n",
      "|   esm.layers.24.self_attn.q_proj.weight   |  1638400   |\n",
      "|    esm.layers.24.self_attn.q_proj.bias    |    1280    |\n",
      "|  esm.layers.24.self_attn.out_proj.weight  |  1638400   |\n",
      "|   esm.layers.24.self_attn.out_proj.bias   |    1280    |\n",
      "| esm.layers.24.self_attn_layer_norm.weight |    1280    |\n",
      "|  esm.layers.24.self_attn_layer_norm.bias  |    1280    |\n",
      "|          esm.layers.24.fc1.weight         |  6553600   |\n",
      "|           esm.layers.24.fc1.bias          |    5120    |\n",
      "|          esm.layers.24.fc2.weight         |  6553600   |\n",
      "|           esm.layers.24.fc2.bias          |    1280    |\n",
      "|   esm.layers.24.final_layer_norm.weight   |    1280    |\n",
      "|    esm.layers.24.final_layer_norm.bias    |    1280    |\n",
      "|   esm.layers.25.self_attn.k_proj.weight   |  1638400   |\n",
      "|    esm.layers.25.self_attn.k_proj.bias    |    1280    |\n",
      "|   esm.layers.25.self_attn.v_proj.weight   |  1638400   |\n",
      "|    esm.layers.25.self_attn.v_proj.bias    |    1280    |\n",
      "|   esm.layers.25.self_attn.q_proj.weight   |  1638400   |\n",
      "|    esm.layers.25.self_attn.q_proj.bias    |    1280    |\n",
      "|  esm.layers.25.self_attn.out_proj.weight  |  1638400   |\n",
      "|   esm.layers.25.self_attn.out_proj.bias   |    1280    |\n",
      "| esm.layers.25.self_attn_layer_norm.weight |    1280    |\n",
      "|  esm.layers.25.self_attn_layer_norm.bias  |    1280    |\n",
      "|          esm.layers.25.fc1.weight         |  6553600   |\n",
      "|           esm.layers.25.fc1.bias          |    5120    |\n",
      "|          esm.layers.25.fc2.weight         |  6553600   |\n",
      "|           esm.layers.25.fc2.bias          |    1280    |\n",
      "|   esm.layers.25.final_layer_norm.weight   |    1280    |\n",
      "|    esm.layers.25.final_layer_norm.bias    |    1280    |\n",
      "|   esm.layers.26.self_attn.k_proj.weight   |  1638400   |\n",
      "|    esm.layers.26.self_attn.k_proj.bias    |    1280    |\n",
      "|   esm.layers.26.self_attn.v_proj.weight   |  1638400   |\n",
      "|    esm.layers.26.self_attn.v_proj.bias    |    1280    |\n",
      "|   esm.layers.26.self_attn.q_proj.weight   |  1638400   |\n",
      "|    esm.layers.26.self_attn.q_proj.bias    |    1280    |\n",
      "|  esm.layers.26.self_attn.out_proj.weight  |  1638400   |\n",
      "|   esm.layers.26.self_attn.out_proj.bias   |    1280    |\n",
      "| esm.layers.26.self_attn_layer_norm.weight |    1280    |\n",
      "|  esm.layers.26.self_attn_layer_norm.bias  |    1280    |\n",
      "|          esm.layers.26.fc1.weight         |  6553600   |\n",
      "|           esm.layers.26.fc1.bias          |    5120    |\n",
      "|          esm.layers.26.fc2.weight         |  6553600   |\n",
      "|           esm.layers.26.fc2.bias          |    1280    |\n",
      "|   esm.layers.26.final_layer_norm.weight   |    1280    |\n",
      "|    esm.layers.26.final_layer_norm.bias    |    1280    |\n",
      "|   esm.layers.27.self_attn.k_proj.weight   |  1638400   |\n",
      "|    esm.layers.27.self_attn.k_proj.bias    |    1280    |\n",
      "|   esm.layers.27.self_attn.v_proj.weight   |  1638400   |\n",
      "|    esm.layers.27.self_attn.v_proj.bias    |    1280    |\n",
      "|   esm.layers.27.self_attn.q_proj.weight   |  1638400   |\n",
      "|    esm.layers.27.self_attn.q_proj.bias    |    1280    |\n",
      "|  esm.layers.27.self_attn.out_proj.weight  |  1638400   |\n",
      "|   esm.layers.27.self_attn.out_proj.bias   |    1280    |\n",
      "| esm.layers.27.self_attn_layer_norm.weight |    1280    |\n",
      "|  esm.layers.27.self_attn_layer_norm.bias  |    1280    |\n",
      "|          esm.layers.27.fc1.weight         |  6553600   |\n",
      "|           esm.layers.27.fc1.bias          |    5120    |\n",
      "|          esm.layers.27.fc2.weight         |  6553600   |\n",
      "|           esm.layers.27.fc2.bias          |    1280    |\n",
      "|   esm.layers.27.final_layer_norm.weight   |    1280    |\n",
      "|    esm.layers.27.final_layer_norm.bias    |    1280    |\n",
      "|   esm.layers.28.self_attn.k_proj.weight   |  1638400   |\n",
      "|    esm.layers.28.self_attn.k_proj.bias    |    1280    |\n",
      "|   esm.layers.28.self_attn.v_proj.weight   |  1638400   |\n",
      "|    esm.layers.28.self_attn.v_proj.bias    |    1280    |\n",
      "|   esm.layers.28.self_attn.q_proj.weight   |  1638400   |\n",
      "|    esm.layers.28.self_attn.q_proj.bias    |    1280    |\n",
      "|  esm.layers.28.self_attn.out_proj.weight  |  1638400   |\n",
      "|   esm.layers.28.self_attn.out_proj.bias   |    1280    |\n",
      "| esm.layers.28.self_attn_layer_norm.weight |    1280    |\n",
      "|  esm.layers.28.self_attn_layer_norm.bias  |    1280    |\n",
      "|          esm.layers.28.fc1.weight         |  6553600   |\n",
      "|           esm.layers.28.fc1.bias          |    5120    |\n",
      "|          esm.layers.28.fc2.weight         |  6553600   |\n",
      "|           esm.layers.28.fc2.bias          |    1280    |\n",
      "|   esm.layers.28.final_layer_norm.weight   |    1280    |\n",
      "|    esm.layers.28.final_layer_norm.bias    |    1280    |\n",
      "|   esm.layers.29.self_attn.k_proj.weight   |  1638400   |\n",
      "|    esm.layers.29.self_attn.k_proj.bias    |    1280    |\n",
      "|   esm.layers.29.self_attn.v_proj.weight   |  1638400   |\n",
      "|    esm.layers.29.self_attn.v_proj.bias    |    1280    |\n",
      "|   esm.layers.29.self_attn.q_proj.weight   |  1638400   |\n",
      "|    esm.layers.29.self_attn.q_proj.bias    |    1280    |\n",
      "|  esm.layers.29.self_attn.out_proj.weight  |  1638400   |\n",
      "|   esm.layers.29.self_attn.out_proj.bias   |    1280    |\n",
      "| esm.layers.29.self_attn_layer_norm.weight |    1280    |\n",
      "|  esm.layers.29.self_attn_layer_norm.bias  |    1280    |\n",
      "|          esm.layers.29.fc1.weight         |  6553600   |\n",
      "|           esm.layers.29.fc1.bias          |    5120    |\n",
      "|          esm.layers.29.fc2.weight         |  6553600   |\n",
      "|           esm.layers.29.fc2.bias          |    1280    |\n",
      "|   esm.layers.29.final_layer_norm.weight   |    1280    |\n",
      "|    esm.layers.29.final_layer_norm.bias    |    1280    |\n",
      "|   esm.layers.30.self_attn.k_proj.weight   |  1638400   |\n",
      "|    esm.layers.30.self_attn.k_proj.bias    |    1280    |\n",
      "|   esm.layers.30.self_attn.v_proj.weight   |  1638400   |\n",
      "|    esm.layers.30.self_attn.v_proj.bias    |    1280    |\n",
      "|   esm.layers.30.self_attn.q_proj.weight   |  1638400   |\n",
      "|    esm.layers.30.self_attn.q_proj.bias    |    1280    |\n",
      "|  esm.layers.30.self_attn.out_proj.weight  |  1638400   |\n",
      "|   esm.layers.30.self_attn.out_proj.bias   |    1280    |\n",
      "| esm.layers.30.self_attn_layer_norm.weight |    1280    |\n",
      "|  esm.layers.30.self_attn_layer_norm.bias  |    1280    |\n",
      "|          esm.layers.30.fc1.weight         |  6553600   |\n",
      "|           esm.layers.30.fc1.bias          |    5120    |\n",
      "|          esm.layers.30.fc2.weight         |  6553600   |\n",
      "|           esm.layers.30.fc2.bias          |    1280    |\n",
      "|   esm.layers.30.final_layer_norm.weight   |    1280    |\n",
      "|    esm.layers.30.final_layer_norm.bias    |    1280    |\n",
      "|   esm.layers.31.self_attn.k_proj.weight   |  1638400   |\n",
      "|    esm.layers.31.self_attn.k_proj.bias    |    1280    |\n",
      "|   esm.layers.31.self_attn.v_proj.weight   |  1638400   |\n",
      "|    esm.layers.31.self_attn.v_proj.bias    |    1280    |\n",
      "|   esm.layers.31.self_attn.q_proj.weight   |  1638400   |\n",
      "|    esm.layers.31.self_attn.q_proj.bias    |    1280    |\n",
      "|  esm.layers.31.self_attn.out_proj.weight  |  1638400   |\n",
      "|   esm.layers.31.self_attn.out_proj.bias   |    1280    |\n",
      "| esm.layers.31.self_attn_layer_norm.weight |    1280    |\n",
      "|  esm.layers.31.self_attn_layer_norm.bias  |    1280    |\n",
      "|          esm.layers.31.fc1.weight         |  6553600   |\n",
      "|           esm.layers.31.fc1.bias          |    5120    |\n",
      "|          esm.layers.31.fc2.weight         |  6553600   |\n",
      "|           esm.layers.31.fc2.bias          |    1280    |\n",
      "|   esm.layers.31.final_layer_norm.weight   |    1280    |\n",
      "|    esm.layers.31.final_layer_norm.bias    |    1280    |\n",
      "|   esm.layers.32.self_attn.k_proj.weight   |  1638400   |\n",
      "|    esm.layers.32.self_attn.k_proj.bias    |    1280    |\n",
      "|   esm.layers.32.self_attn.v_proj.weight   |  1638400   |\n",
      "|    esm.layers.32.self_attn.v_proj.bias    |    1280    |\n",
      "|   esm.layers.32.self_attn.q_proj.weight   |  1638400   |\n",
      "|    esm.layers.32.self_attn.q_proj.bias    |    1280    |\n",
      "|  esm.layers.32.self_attn.out_proj.weight  |  1638400   |\n",
      "|   esm.layers.32.self_attn.out_proj.bias   |    1280    |\n",
      "| esm.layers.32.self_attn_layer_norm.weight |    1280    |\n",
      "|  esm.layers.32.self_attn_layer_norm.bias  |    1280    |\n",
      "|          esm.layers.32.fc1.weight         |  6553600   |\n",
      "|           esm.layers.32.fc1.bias          |    5120    |\n",
      "|          esm.layers.32.fc2.weight         |  6553600   |\n",
      "|           esm.layers.32.fc2.bias          |    1280    |\n",
      "|   esm.layers.32.final_layer_norm.weight   |    1280    |\n",
      "|    esm.layers.32.final_layer_norm.bias    |    1280    |\n",
      "|     esm.contact_head.regression.weight    |    660     |\n",
      "|      esm.contact_head.regression.bias     |     1      |\n",
      "|         esm.embed_positions.weight        |  1313280   |\n",
      "|      esm.emb_layer_norm_before.weight     |    1280    |\n",
      "|       esm.emb_layer_norm_before.bias      |    1280    |\n",
      "|      esm.emb_layer_norm_after.weight      |    1280    |\n",
      "|       esm.emb_layer_norm_after.bias       |    1280    |\n",
      "|              esm.lm_head.bias             |     33     |\n",
      "|          esm.lm_head.dense.weight         |  1638400   |\n",
      "|           esm.lm_head.dense.bias          |    1280    |\n",
      "|       esm.lm_head.layer_norm.weight       |    1280    |\n",
      "|        esm.lm_head.layer_norm.bias        |    1280    |\n",
      "|           pep_lstm1.weight_ih_l0          |    5120    |\n",
      "|           pep_lstm1.weight_hh_l0          |     4      |\n",
      "|            pep_lstm1.bias_ih_l0           |     4      |\n",
      "|            pep_lstm1.bias_hh_l0           |     4      |\n",
      "|           ts_lstm1.weight_ih_l0           |    5120    |\n",
      "|           ts_lstm1.weight_hh_l0           |     4      |\n",
      "|            ts_lstm1.bias_ih_l0            |     4      |\n",
      "|            ts_lstm1.bias_hh_l0            |     4      |\n",
      "|                 fc1.weight                |     5      |\n",
      "|                  fc1.bias                 |     1      |\n",
      "+-------------------------------------------+------------+\n",
      "Total Trainable Params: 652369364\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "652369364"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T22:47:00.175856Z",
     "start_time": "2021-12-08T22:47:00.159585Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "# optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "loss_fn = nn.BCELoss()\n",
    "# loss_fn = F.binary_cross_entropy()\n",
    "\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "# warmup_step = int(t_total * warmup_ratio)\n",
    "\n",
    "# scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps=warmup_step, t_total=t_total)\n",
    "\n",
    "def calc_accuracy(X,Y):\n",
    "    train_acc = ((X>0.5)==Y).sum().data.cpu().numpy() / len(Y)\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-09T13:06:40.100275Z",
     "start_time": "2021-12-08T22:47:00.176866Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 0.6405693292617798 train acc 0.75 time 4.78\n",
      "epoch 1 batch id 201 loss 0.7181351184844971 train acc 0.5223880597014925 time 462.63\n",
      "epoch 1 batch id 401 loss 0.5228497982025146 train acc 0.6022443890274314 time 920.62\n",
      "epoch 1 batch id 601 loss 0.46038901805877686 train acc 0.632279534109817 time 1383.4\n",
      "epoch 1 batch id 801 loss 0.6447947025299072 train acc 0.6385767790262172 time 1852.77\n",
      "epoch 1 batch id 1001 loss 0.5278580784797668 train acc 0.6478521478521478 time 2324.18\n",
      "epoch 1 batch id 1201 loss 0.6635206937789917 train acc 0.6546627810158201 time 2782.02\n",
      "epoch 1 batch id 1401 loss 0.6624365448951721 train acc 0.6577444682369736 time 3240.01\n",
      "epoch 1 batch id 1601 loss 0.5692307949066162 train acc 0.6597439100562149 time 3697.91\n",
      "epoch 1 batch id 1801 loss 0.4833444356918335 train acc 0.6640755136035535 time 4155.86\n",
      "epoch 1 batch id 2001 loss 0.6179523468017578 train acc 0.6682908545727136 time 4613.82\n",
      "epoch 1 batch id 2201 loss 0.721555233001709 train acc 0.6701499318491595 time 5071.78\n",
      "epoch 1 batch id 2401 loss 0.653188943862915 train acc 0.6712827988338192 time 5529.73\n",
      "epoch 1 batch id 2601 loss 0.6185049414634705 train acc 0.672722029988466 time 5987.53\n",
      "epoch 1 batch id 2801 loss 0.6561743021011353 train acc 0.6738664762584791 time 6445.29\n",
      "epoch 1 batch id 3001 loss 0.573311448097229 train acc 0.6750249916694435 time 6903.0\n",
      "epoch 1 batch id 3201 loss 0.6739221811294556 train acc 0.6742424242424242 time 7360.81\n",
      "epoch 1 batch id 3401 loss 0.6583537459373474 train acc 0.672596295207292 time 7818.54\n",
      "epoch 1 batch id 3601 loss 0.757366418838501 train acc 0.6734934740349903 time 8276.19\n",
      "epoch 1 batch id 3801 loss 0.5423081517219543 train acc 0.6748881873191266 time 8733.9\n",
      "epoch 1 batch id 4001 loss 0.7057210206985474 train acc 0.6760809797550612 time 9191.72\n",
      "epoch 1 batch id 4201 loss 0.7808379530906677 train acc 0.675672458938348 time 9684.91\n",
      "epoch 1 batch id 4401 loss 0.6048949956893921 train acc 0.6763235628266303 time 10142.48\n",
      "epoch 1 batch id 4601 loss 0.48800915479660034 train acc 0.6756683329710932 time 10600.2\n",
      "epoch 1 batch id 4801 loss 0.6354656219482422 train acc 0.6751718392001667 time 11057.87\n",
      "epoch 1 batch id 5001 loss 0.686551034450531 train acc 0.6749150169966007 time 11515.54\n",
      "epoch 1 batch id 5201 loss 0.5595107078552246 train acc 0.676552586041146 time 11973.31\n",
      "epoch 1 batch id 5401 loss 0.6377036571502686 train acc 0.6769116830216626 time 12431.04\n",
      "epoch 1 batch id 5601 loss 0.604987382888794 train acc 0.6775575790037494 time 12888.6\n",
      "epoch 1 batch id 5801 loss 0.7857099771499634 train acc 0.6774694018272711 time 13346.36\n",
      "epoch 1 batch id 6001 loss 0.7193372845649719 train acc 0.6730961506415597 time 13804.82\n",
      "epoch 1 batch id 6201 loss 0.6943725347518921 train acc 0.6677552007740687 time 14263.55\n",
      "epoch 1 batch id 6401 loss 0.67017662525177 train acc 0.6614982034057179 time 14721.08\n",
      "epoch 1 batch id 6601 loss 0.6942840814590454 train acc 0.6565671867898804 time 15179.89\n",
      "epoch 1 batch id 6801 loss 0.6942472457885742 train acc 0.6513380385237465 time 15637.6\n",
      "epoch 1 batch id 7001 loss 0.7173478603363037 train acc 0.6468361662619626 time 16096.39\n",
      "epoch 1 batch id 7201 loss 0.6709837317466736 train acc 0.6437994722955145 time 16555.01\n",
      "epoch 1 batch id 7401 loss 0.6942180395126343 train acc 0.6400486420753952 time 17013.74\n",
      "epoch 1 batch id 7601 loss 0.7397689819335938 train acc 0.6360347322720694 time 17472.48\n",
      "epoch 1 batch id 7801 loss 0.7170249223709106 train acc 0.6334444302012563 time 17931.16\n",
      "epoch 1 batch id 8001 loss 0.6941606998443604 train acc 0.6298587676540433 time 18389.85\n",
      "epoch 1 batch id 8201 loss 0.6717585325241089 train acc 0.6269662236312645 time 18848.47\n",
      "epoch 1 batch id 8401 loss 0.694115161895752 train acc 0.6236162361623616 time 19307.26\n",
      "epoch 1 batch id 8601 loss 0.6941006183624268 train acc 0.6207127078246716 time 19764.87\n",
      "epoch 1 batch id 8801 loss 0.6725225448608398 train acc 0.6179127371889558 time 20223.59\n",
      "epoch 1 batch id 9001 loss 0.6940666437149048 train acc 0.6154593934007333 time 20682.26\n",
      "epoch 1 batch id 9201 loss 0.7368261814117432 train acc 0.6132213889794588 time 21141.0\n",
      "epoch 1 batch id 9401 loss 0.715248703956604 train acc 0.6109190511647697 time 21599.76\n",
      "epoch 1 batch id 9601 loss 0.7150523662567139 train acc 0.6086605561920633 time 22055.14\n",
      "epoch 1 batch id 9801 loss 0.7357781529426575 train acc 0.6064687276808489 time 22514.13\n",
      "epoch 1 batch id 10001 loss 0.6735444664955139 train acc 0.603964603539646 time 22972.02\n",
      "epoch 1 batch id 10201 loss 0.6939523816108704 train acc 0.6015341633173218 time 23430.69\n",
      "epoch 1 batch id 10401 loss 0.6540334820747375 train acc 0.5998221324872608 time 23889.42\n",
      "epoch 1 batch id 10601 loss 0.6939510107040405 train acc 0.5985048580322611 time 24348.12\n",
      "epoch 1 batch id 10801 loss 0.674126923084259 train acc 0.5965882788630682 time 24806.89\n",
      "epoch 1 batch id 11001 loss 0.6939454078674316 train acc 0.5955140441778021 time 25265.61\n",
      "epoch 1 batch id 11201 loss 0.6742414236068726 train acc 0.5936077135970003 time 25724.49\n",
      "epoch 1 batch id 11401 loss 0.6544172167778015 train acc 0.5926234540829752 time 26183.33\n",
      "epoch 1 batch id 11601 loss 0.6939162015914917 train acc 0.5909619860356866 time 26642.09\n",
      "epoch 1 batch id 11801 loss 0.6939215660095215 train acc 0.5898440810100839 time 27099.73\n",
      "epoch 1 batch id 12001 loss 0.6939157843589783 train acc 0.588534288809266 time 27558.73\n",
      "epoch 1 batch id 12201 loss 0.7135980725288391 train acc 0.587554298827965 time 28018.03\n",
      "epoch 1 batch id 12401 loss 0.7269567847251892 train acc 0.5880977340537054 time 28476.49\n",
      "epoch 1 batch id 12601 loss 0.7446995973587036 train acc 0.5895365447186731 time 28934.37\n",
      "epoch 1 batch id 12801 loss 0.5520864129066467 train acc 0.5915358175142567 time 29392.42\n",
      "epoch 1 batch id 13001 loss 0.6161393523216248 train acc 0.592877470963772 time 29850.66\n",
      "epoch 1 batch id 13201 loss 0.6796363592147827 train acc 0.5947844860237861 time 30309.7\n",
      "epoch 1 batch id 13401 loss 0.7048095464706421 train acc 0.5960189538094172 time 30768.79\n",
      "epoch 1 batch id 13601 loss 0.757416844367981 train acc 0.5977317844276157 time 31227.85\n",
      "epoch 1 batch id 13801 loss 0.5763623118400574 train acc 0.5992319397145135 time 31686.84\n",
      "epoch 1 batch id 14001 loss 0.7061231136322021 train acc 0.600082136990215 time 32145.55\n",
      "epoch 1 batch id 14201 loss 0.8633127212524414 train acc 0.5989719033870854 time 32602.7\n",
      "epoch 1 batch id 14401 loss 0.7046996355056763 train acc 0.5972501909589611 time 33059.94\n",
      "epoch 1 batch id 14601 loss 0.6300373077392578 train acc 0.5957811108828163 time 33517.31\n",
      "epoch 1 batch id 14801 loss 0.8481632471084595 train acc 0.5943517329910141 time 33974.79\n",
      "epoch 1 batch id 15001 loss 0.6323766112327576 train acc 0.5931104593027131 time 34432.21\n",
      "epoch 1 batch id 15201 loss 0.6335625052452087 train acc 0.5918689559897375 time 34889.62\n",
      "epoch 1 batch id 15401 loss 0.5660874843597412 train acc 0.59091942081683 time 35346.95\n",
      "epoch 1 batch id 15601 loss 0.768841028213501 train acc 0.5898019357733478 time 35804.1\n",
      "epoch 1 batch id 15801 loss 0.7017245888710022 train acc 0.5887602050503132 time 36261.39\n",
      "epoch 1 batch id 16001 loss 0.7654106616973877 train acc 0.5874007874507843 time 36718.63\n",
      "epoch 1 batch id 16201 loss 0.6380807161331177 train acc 0.5865070057403864 time 37175.94\n",
      "epoch 1 batch id 16401 loss 0.700635552406311 train acc 0.5850253033351624 time 37633.24\n",
      "epoch 1 batch id 16601 loss 0.6399964690208435 train acc 0.584302150472863 time 38090.57\n",
      "epoch 1 batch id 16801 loss 0.5811769962310791 train acc 0.5834325337777513 time 38547.67\n",
      "epoch 1 batch id 17001 loss 0.5842016935348511 train acc 0.5821128168931239 time 39004.9\n",
      "epoch 1 batch id 17201 loss 0.6428613066673279 train acc 0.5811580722051044 time 39462.17\n",
      "epoch 1 batch id 17401 loss 0.7542778253555298 train acc 0.5798660996494455 time 39919.46\n",
      "epoch 1 batch id 17601 loss 0.6990888118743896 train acc 0.5792710641440827 time 40376.73\n",
      "epoch 1 batch id 17801 loss 0.7523558139801025 train acc 0.5784225605303073 time 40834.01\n",
      "epoch 1 batch id 18001 loss 0.750883936882019 train acc 0.577398477862341 time 41291.26\n",
      "epoch 1 batch id 18201 loss 0.6473081707954407 train acc 0.576424372287237 time 41748.45\n",
      "epoch 1 batch id 18401 loss 0.7489950060844421 train acc 0.5759605456225205 time 42205.59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 18601 loss 0.6980684995651245 train acc 0.575116929197355 time 42662.77\n",
      "epoch 1 batch id 18801 loss 0.7467469573020935 train acc 0.5744109355885325 time 43119.91\n",
      "epoch 1 batch id 19001 loss 0.7456659078598022 train acc 0.5735750749960529 time 43576.97\n",
      "epoch 1 batch id 19201 loss 0.6504814624786377 train acc 0.5729389094318005 time 44033.94\n",
      "epoch 1 batch id 19401 loss 0.6974238157272339 train acc 0.5722385444049276 time 44490.96\n",
      "epoch 1 batch id 19601 loss 0.6973432302474976 train acc 0.5717310341309116 time 44948.17\n",
      "epoch 1 batch id 19801 loss 0.6523370742797852 train acc 0.5709433866976416 time 45405.39\n",
      "epoch 1 batch id 20001 loss 0.7851259112358093 train acc 0.5702464876756163 time 45862.52\n",
      "epoch 1 batch id 20201 loss 0.6967732906341553 train acc 0.5692911242017722 time 46319.62\n",
      "epoch 1 batch id 20401 loss 0.7386776208877563 train acc 0.5687588843684133 time 46776.7\n",
      "epoch 1 batch id 20601 loss 0.6965295076370239 train acc 0.5680792194553662 time 47233.87\n",
      "epoch 1 batch id 20801 loss 0.655990481376648 train acc 0.5674126243930581 time 47690.89\n",
      "epoch 1 batch id 21001 loss 0.6961873173713684 train acc 0.5664849292890815 time 48148.03\n",
      "epoch 1 batch id 21201 loss 0.772664487361908 train acc 0.5658459506627046 time 48605.13\n",
      "epoch 1 batch id 21401 loss 0.6959894895553589 train acc 0.5653006868837904 time 49062.21\n",
      "epoch 1 batch id 21601 loss 0.6587932109832764 train acc 0.5647770936530716 time 49519.28\n",
      "epoch 1 batch id 21801 loss 0.7327352166175842 train acc 0.5644121829273887 time 49976.18\n",
      "epoch 1 batch id 22001 loss 0.7676907777786255 train acc 0.5637243761647198 time 50433.2\n",
      "epoch 1 batch id 22201 loss 0.6955474615097046 train acc 0.5628575289401379 time 50890.2\n",
      "epoch 1 batch id 22401 loss 0.6955089569091797 train acc 0.5624860497299228 time 51346.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-37:\n",
      "Process Process-39:\n",
      "Process Process-38:\n",
      "Process Process-40:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/multiprocessing/process.py\", line 261, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/multiprocessing/process.py\", line 261, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/multiprocessing/process.py\", line 261, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/multiprocessing/process.py\", line 261, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/multiprocessing/util.py\", line 322, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/multiprocessing/util.py\", line 322, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/multiprocessing/util.py\", line 322, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/multiprocessing/util.py\", line 322, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/multiprocessing/util.py\", line 262, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/multiprocessing/util.py\", line 262, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/multiprocessing/util.py\", line 262, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/multiprocessing/util.py\", line 262, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/multiprocessing/util.py\", line 186, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/multiprocessing/util.py\", line 186, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/multiprocessing/util.py\", line 186, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/multiprocessing/util.py\", line 186, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/multiprocessing/queues.py\", line 191, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/multiprocessing/queues.py\", line 191, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/multiprocessing/queues.py\", line 191, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/multiprocessing/queues.py\", line 191, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/threading.py\", line 1056, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/threading.py\", line 1056, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/threading.py\", line 1056, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/threading.py\", line 1056, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/threading.py\", line 1072, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/threading.py\", line 1072, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/threading.py\", line 1072, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/threading.py\", line 1072, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f5458149898>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/multiprocessing/process.py\", line 124, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/multiprocessing/popen_fork.py\", line 47, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-ac63adb9f466>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpep_token_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_token_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_token_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm1_token_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm2_token_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ProtEmbedding/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-0ed2c3ed11ed>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, token_ids, en_token_ids, ec_token_ids, e1_token_ids, e2_token_ids)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0men_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mesm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men_token_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepr_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m33\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'representations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m33\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mec_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mesm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mec_token_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepr_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m33\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'representations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m33\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0me1_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mesm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1_token_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepr_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m33\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'representations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m33\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0me2_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mesm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me2_token_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepr_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m33\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'representations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m33\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ProtEmbedding/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/2021_AIhub/esm/esm/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tokens, repr_layers, need_head_weights, return_contacts)\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpadding_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m             \u001b[0mpadding_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "for e in range(num_epochs):\n",
    "    t0 = time.time()\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    for batch_id, (pep_token_ids, n_token_ids, c_token_ids, m1_token_ids, m2_token_ids, label) in enumerate(train_dataloader):\n",
    "#         print(batch_id, round(time.time()-t0, 2))  # batch8->70000 loop, each 3 sec -> per 7 epoch, 3h\n",
    "        \n",
    "        pep_token_ids = pep_token_ids.long().to(device)\n",
    "        n_token_ids = n_token_ids.long().to(device)\n",
    "        c_token_ids = c_token_ids.long().to(device)\n",
    "        m1_token_ids = m1_token_ids.long().to(device)\n",
    "        m2_token_ids = m2_token_ids.long().to(device)\n",
    "        label = torch.reshape(label.float(), (-1, 1)).to(device)\n",
    "        \n",
    "        pred = model(pep_token_ids, n_token_ids, c_token_ids, m1_token_ids, m2_token_ids)\n",
    "        loss = F.binary_cross_entropy(pred, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_acc += calc_accuracy(pred, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {} time {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1), round(time.time()-t0, 2)))\n",
    "        \n",
    "    print(\"epoch {} train acc {} time {}\".format(e+1, train_acc / (batch_id+1), round(time.time()-t0,2)))\n",
    "    \n",
    "    model.eval()\n",
    "    for batch_id, (pep_token_ids, n_token_ids, c_token_ids, m1_token_ids, m2_token_ids, label) in enumerate(valid_dataloader):\n",
    "        pep_token_ids = pep_token_ids.long().to(device)\n",
    "        n_token_ids = n_token_ids.long().to(device)\n",
    "        c_token_ids = c_token_ids.long().to(device)\n",
    "        m1_token_ids = m1_token_ids.long().to(device)\n",
    "        m2_token_ids = m2_token_ids.long().to(device)\n",
    "        label = label.long().to(device)\n",
    "        label = torch.reshape(label, (-1, 1))\n",
    "        pred = model(pep_token_ids, n_token_ids, c_token_ids, m1_token_ids, m2_token_ids)\n",
    "        \n",
    "        test_acc += calc_accuracy(pred, label)\n",
    "    if test_acc > best_acc:\n",
    "        best_acc=test_acc\n",
    "        torch.save({\"best_acc\":best_acc / (batch_id+1) ,\"model\":model.state_dict()},f'./finetuning_whole.pl')\n",
    "        print(f\"best_acc: {best_acc}\")\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T22:38:10.474629Z",
     "start_time": "2021-12-06T03:00:53.668157Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 0.7029515504837036 train acc 0.25 time 3.23\n",
      "epoch 1 batch id 201 loss 0.6523497700691223 train acc 0.6355721393034826 time 139.91\n",
      "epoch 1 batch id 401 loss 0.6511130332946777 train acc 0.6536783042394015 time 509.62\n",
      "epoch 1 batch id 601 loss 0.672355592250824 train acc 0.6649334442595674 time 1084.9\n",
      "epoch 1 batch id 801 loss 0.6703794002532959 train acc 0.6707240948813983 time 1660.45\n",
      "epoch 1 batch id 1001 loss 0.6852197647094727 train acc 0.6750749250749251 time 2236.06\n",
      "epoch 1 batch id 1201 loss 0.6913191080093384 train acc 0.6824521232306411 time 2811.94\n",
      "epoch 1 batch id 1401 loss 0.6857815980911255 train acc 0.6847787294789436 time 3387.61\n",
      "epoch 1 batch id 1601 loss 0.6768355369567871 train acc 0.6862898188632105 time 3963.47\n",
      "epoch 1 batch id 1801 loss 0.6946740746498108 train acc 0.6868406440866185 time 4539.09\n",
      "epoch 1 batch id 2001 loss 0.6692252159118652 train acc 0.6881559220389805 time 5115.35\n",
      "epoch 1 batch id 2201 loss 0.6292107105255127 train acc 0.6886642435256701 time 5692.34\n",
      "epoch 1 batch id 2401 loss 0.6614770889282227 train acc 0.68856726364015 time 6269.58\n",
      "epoch 1 batch id 2601 loss 0.670512318611145 train acc 0.6891099577085736 time 6846.52\n",
      "epoch 1 batch id 2801 loss 0.6574257612228394 train acc 0.6913156015708676 time 7423.29\n",
      "epoch 1 batch id 3001 loss 0.6816809177398682 train acc 0.6919360213262246 time 8000.32\n",
      "epoch 1 batch id 3201 loss 0.6767693758010864 train acc 0.6930256169946891 time 8577.49\n",
      "epoch 1 batch id 3401 loss 0.6885885000228882 train acc 0.6938400470449868 time 9154.8\n",
      "epoch 1 batch id 3601 loss 0.6893326044082642 train acc 0.6924465426270481 time 9732.53\n",
      "epoch 1 batch id 3801 loss 0.61659836769104 train acc 0.6930084188371481 time 10310.01\n",
      "epoch 1 batch id 4001 loss 0.5990813970565796 train acc 0.693732816795801 time 10887.11\n",
      "epoch 1 batch id 4201 loss 0.681769609451294 train acc 0.6867412520828374 time 11464.12\n",
      "epoch 1 batch id 4401 loss 0.682075023651123 train acc 0.6781413315155647 time 12038.23\n",
      "epoch 1 batch id 4601 loss 0.706433892250061 train acc 0.6708595957400565 time 12612.34\n",
      "epoch 1 batch id 4801 loss 0.7178919315338135 train acc 0.6638460737346387 time 13186.63\n",
      "epoch 1 batch id 5001 loss 0.6828223466873169 train acc 0.6569686062787442 time 13761.0\n",
      "epoch 1 batch id 5201 loss 0.6941104531288147 train acc 0.6507402422611036 time 14335.81\n",
      "epoch 1 batch id 5401 loss 0.6940873265266418 train acc 0.6456443251249768 time 14910.12\n",
      "epoch 1 batch id 5601 loss 0.6619249582290649 train acc 0.6409792894126048 time 15484.92\n",
      "epoch 1 batch id 5801 loss 0.6940525770187378 train acc 0.6366574728495087 time 16060.07\n",
      "epoch 1 batch id 6001 loss 0.704662561416626 train acc 0.6326862189635061 time 16635.26\n",
      "epoch 1 batch id 6201 loss 0.6940235495567322 train acc 0.6287090791807773 time 17204.87\n",
      "epoch 1 batch id 6401 loss 0.6939521431922913 train acc 0.6243946258397125 time 17804.47\n",
      "epoch 1 batch id 6601 loss 0.6744154691696167 train acc 0.620625662778367 time 18405.79\n",
      "epoch 1 batch id 6801 loss 0.6938936710357666 train acc 0.6175194824290545 time 19007.17\n",
      "epoch 1 batch id 7001 loss 0.6843774914741516 train acc 0.6143943722325382 time 19608.74\n",
      "epoch 1 batch id 7201 loss 0.6845952272415161 train acc 0.6111651159561172 time 20209.79\n",
      "epoch 1 batch id 7401 loss 0.7203373312950134 train acc 0.6078570463450885 time 20810.68\n",
      "epoch 1 batch id 7601 loss 0.685173749923706 train acc 0.6050355216418892 time 21411.89\n",
      "epoch 1 batch id 7801 loss 0.7106409668922424 train acc 0.6026150493526471 time 22013.75\n",
      "epoch 1 batch id 8001 loss 0.7018961310386658 train acc 0.5999093863267092 time 22615.89\n",
      "epoch 1 batch id 8201 loss 0.7014947533607483 train acc 0.5972137544201926 time 23217.84\n",
      "epoch 1 batch id 8401 loss 0.6700797080993652 train acc 0.595360671348649 time 23819.84\n",
      "epoch 1 batch id 8601 loss 0.6935887336730957 train acc 0.5927799093128706 time 24421.69\n",
      "epoch 1 batch id 8801 loss 0.6935654878616333 train acc 0.5906431087376435 time 25023.24\n",
      "epoch 1 batch id 9001 loss 0.7004948258399963 train acc 0.5885179424508388 time 25625.05\n",
      "epoch 1 batch id 9201 loss 0.7073390483856201 train acc 0.5868248016519944 time 26226.94\n",
      "epoch 1 batch id 9401 loss 0.7002544403076172 train acc 0.5850574406977981 time 26828.72\n",
      "epoch 1 batch id 9601 loss 0.69349604845047 train acc 0.5833506926361838 time 27430.62\n",
      "epoch 1 batch id 9801 loss 0.7001097202301025 train acc 0.5819304152637486 time 28032.74\n",
      "epoch 1 batch id 10001 loss 0.6999356746673584 train acc 0.5802669733026697 time 28634.99\n",
      "epoch 1 batch id 10201 loss 0.6995707750320435 train acc 0.5785707283599647 time 29237.11\n",
      "epoch 1 batch id 10401 loss 0.687432050704956 train acc 0.5770238438611672 time 29839.36\n",
      "epoch 1 batch id 10601 loss 0.704649806022644 train acc 0.5752759173662862 time 30441.59\n",
      "epoch 1 batch id 10801 loss 0.6878324747085571 train acc 0.5740324969910193 time 31043.63\n",
      "epoch 1 batch id 11001 loss 0.7099200487136841 train acc 0.5728115625852195 time 31644.34\n",
      "epoch 1 batch id 11201 loss 0.6933709383010864 train acc 0.5714221944469244 time 32245.27\n",
      "epoch 1 batch id 11401 loss 0.7035709619522095 train acc 0.570399526357337 time 32847.21\n",
      "epoch 1 batch id 11601 loss 0.739017128944397 train acc 0.5692720455133178 time 33451.14\n",
      "epoch 1 batch id 11801 loss 0.7030224800109863 train acc 0.5680980425387679 time 34055.14\n",
      "epoch 1 batch id 12001 loss 0.7027684450149536 train acc 0.5669527539371719 time 34658.85\n",
      "epoch 1 batch id 12201 loss 0.6682164669036865 train acc 0.5660294238177198 time 35262.66\n",
      "epoch 1 batch id 12401 loss 0.7700622081756592 train acc 0.564964518990404 time 35866.5\n",
      "epoch 1 batch id 12601 loss 0.7691433429718018 train acc 0.5641318149353226 time 36470.53\n",
      "epoch 1 batch id 12801 loss 0.8012534379959106 train acc 0.5632470119521913 time 37074.63\n",
      "epoch 1 batch id 13001 loss 0.7343580722808838 train acc 0.5623028997769403 time 37678.29\n",
      "epoch 1 batch id 13201 loss 0.7657876014709473 train acc 0.56116013938338 time 38281.84\n",
      "epoch 1 batch id 13401 loss 0.6379023790359497 train acc 0.5599488844116111 time 38885.53\n",
      "epoch 1 batch id 13601 loss 0.7632206082344055 train acc 0.5589846334828321 time 39485.29\n",
      "epoch 1 batch id 13801 loss 0.6699211597442627 train acc 0.5582657053836678 time 40085.43\n",
      "epoch 1 batch id 14001 loss 0.7005360126495361 train acc 0.5574423255481751 time 40674.82\n",
      "epoch 1 batch id 14201 loss 0.70039302110672 train acc 0.5569062037884656 time 41278.82\n",
      "epoch 1 batch id 14401 loss 0.7001464366912842 train acc 0.5559162558155684 time 41882.96\n",
      "epoch 1 batch id 14601 loss 0.6999571919441223 train acc 0.5550989658242587 time 42487.28\n",
      "epoch 1 batch id 14801 loss 0.6709724068641663 train acc 0.5543290993851767 time 43091.52\n",
      "epoch 1 batch id 15001 loss 0.7281103134155273 train acc 0.5537464169055396 time 43695.71\n",
      "epoch 1 batch id 15201 loss 0.6432919502258301 train acc 0.5529570422998487 time 44299.44\n",
      "epoch 1 batch id 15401 loss 0.699242115020752 train acc 0.5521557041750536 time 44903.32\n",
      "epoch 1 batch id 15601 loss 0.6447097063064575 train acc 0.55129478879559 time 45507.59\n",
      "epoch 1 batch id 15801 loss 0.7523070573806763 train acc 0.5504398455793937 time 46111.53\n",
      "epoch 1 batch id 16001 loss 0.7518815398216248 train acc 0.550137491406787 time 46715.27\n",
      "epoch 1 batch id 16201 loss 0.6986032724380493 train acc 0.5494413925066354 time 47319.41\n",
      "epoch 1 batch id 16401 loss 0.7241401672363281 train acc 0.5486327053228461 time 47923.55\n",
      "epoch 1 batch id 16601 loss 0.698216438293457 train acc 0.5477757364014216 time 48527.5\n",
      "epoch 1 batch id 16801 loss 0.7228666543960571 train acc 0.5470433307541218 time 49131.65\n",
      "epoch 1 batch id 17001 loss 0.7224035263061523 train acc 0.5464972648667725 time 49735.57\n",
      "epoch 1 batch id 17201 loss 0.697779655456543 train acc 0.5458984942735887 time 50339.48\n",
      "epoch 1 batch id 17401 loss 0.6976053714752197 train acc 0.5451195333601517 time 50943.58\n",
      "epoch 1 batch id 17601 loss 0.6974912285804749 train acc 0.544606840520425 time 51547.79\n",
      "epoch 1 batch id 17801 loss 0.7202574014663696 train acc 0.5439371383630133 time 52151.99\n",
      "epoch 1 batch id 18001 loss 0.7427220940589905 train acc 0.5436572968168435 time 52755.71\n",
      "epoch 1 batch id 18201 loss 0.6971410512924194 train acc 0.5430882918520961 time 53359.57\n",
      "epoch 1 batch id 18401 loss 0.6970347166061401 train acc 0.5425927938698983 time 53963.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 18601 loss 0.7406469583511353 train acc 0.5422826729745712 time 54566.96\n",
      "epoch 1 batch id 18801 loss 0.6752915382385254 train acc 0.5418661241423328 time 55170.97\n",
      "epoch 1 batch id 19001 loss 0.6967687606811523 train acc 0.5414780801010474 time 55775.07\n",
      "epoch 1 batch id 19201 loss 0.6756535172462463 train acc 0.5410785896567887 time 56379.1\n",
      "epoch 1 batch id 19401 loss 0.655083417892456 train acc 0.5407453224060615 time 56982.81\n",
      "epoch 1 batch id 19601 loss 0.6554033756256104 train acc 0.5404188561808071 time 57586.55\n",
      "epoch 1 batch id 19801 loss 0.6964237689971924 train acc 0.5399853542750366 time 58189.85\n",
      "epoch 1 batch id 20001 loss 0.6361497044563293 train acc 0.5397355132243388 time 58793.38\n",
      "epoch 1 batch id 20201 loss 0.6962546706199646 train acc 0.539193604277016 time 59396.85\n",
      "epoch 1 batch id 20401 loss 0.6571980714797974 train acc 0.5389380422528307 time 60000.37\n",
      "epoch 1 batch id 20601 loss 0.715394914150238 train acc 0.5385782243580409 time 60603.79\n",
      "epoch 1 batch id 20801 loss 0.7340497970581055 train acc 0.538165232440748 time 61207.15\n",
      "epoch 1 batch id 21001 loss 0.6772779226303101 train acc 0.5377065377839151 time 61810.56\n",
      "epoch 1 batch id 21201 loss 0.6958491802215576 train acc 0.5373449365595963 time 62414.04\n",
      "epoch 1 batch id 21401 loss 0.7319890856742859 train acc 0.5369258445867016 time 63017.61\n",
      "epoch 1 batch id 21601 loss 0.6956678032875061 train acc 0.5364392852182769 time 63621.21\n",
      "epoch 1 batch id 21801 loss 0.7305268049240112 train acc 0.5360935278198248 time 64224.79\n",
      "epoch 1 batch id 22001 loss 0.6782236695289612 train acc 0.5358620062724422 time 64828.69\n",
      "epoch 1 batch id 22201 loss 0.7128229141235352 train acc 0.5358035674068735 time 65432.35\n",
      "epoch 1 batch id 22401 loss 0.6784440279006958 train acc 0.5354504263202535 time 66036.14\n",
      "epoch 1 batch id 22601 loss 0.6785631775856018 train acc 0.5352141498163798 time 66639.94\n",
      "epoch 1 batch id 22801 loss 0.7120504379272461 train acc 0.5349875005482215 time 67243.26\n",
      "epoch 1 batch id 23001 loss 0.678795337677002 train acc 0.5347974001130386 time 67846.47\n",
      "epoch 1 batch id 23201 loss 0.6788924932479858 train acc 0.5345890263350718 time 68449.73\n",
      "epoch 1 batch id 23401 loss 0.6952609419822693 train acc 0.5344216059142771 time 69053.09\n",
      "epoch 1 batch id 23601 loss 0.6790683269500732 train acc 0.5342676157789924 time 69656.74\n",
      "epoch 1 batch id 23801 loss 0.647408127784729 train acc 0.5339954203604891 time 70260.39\n",
      "epoch 1 batch id 24001 loss 0.7266767024993896 train acc 0.5337642181575768 time 70864.32\n",
      "epoch 1 batch id 24201 loss 0.6795057058334351 train acc 0.5335523325482419 time 71467.92\n",
      "epoch 1 batch id 24401 loss 0.6950392127037048 train acc 0.5332465882545797 time 72070.32\n",
      "epoch 1 batch id 24601 loss 0.7101556658744812 train acc 0.5329763017763506 time 72672.48\n",
      "epoch 1 batch id 24801 loss 0.6800079941749573 train acc 0.5326650135075198 time 73275.37\n",
      "epoch 1 batch id 25001 loss 0.6801605820655823 train acc 0.53244370225191 time 73877.45\n",
      "epoch 1 batch id 25201 loss 0.7236217260360718 train acc 0.5320175786675132 time 74480.98\n",
      "epoch 1 batch id 25401 loss 0.6664478778839111 train acc 0.531740876343451 time 75084.57\n",
      "epoch 1 batch id 25601 loss 0.6807575225830078 train acc 0.5314343189719152 time 75687.83\n",
      "epoch 1 batch id 25801 loss 0.6946531534194946 train acc 0.5311761172047595 time 76290.85\n",
      "epoch 1 batch id 26001 loss 0.708159863948822 train acc 0.5309507326641283 time 76894.06\n",
      "epoch 1 batch id 26201 loss 0.7212448716163635 train acc 0.5306810808747757 time 77497.38\n",
      "epoch 1 batch id 26401 loss 0.6681102514266968 train acc 0.5305433506306579 time 78100.6\n",
      "epoch 1 batch id 26601 loss 0.7076510787010193 train acc 0.530393594225781 time 78703.89\n",
      "epoch 1 batch id 26801 loss 0.7073021531105042 train acc 0.5300455206895265 time 79307.41\n",
      "epoch 1 batch id 27001 loss 0.7324000597000122 train acc 0.5297813043961335 time 79907.11\n",
      "epoch 1 batch id 27201 loss 0.6820446252822876 train acc 0.5294290651079004 time 80507.24\n",
      "epoch 1 batch id 27401 loss 0.6943375468254089 train acc 0.5292735666581512 time 81110.72\n",
      "epoch 1 batch id 27601 loss 0.730239987373352 train acc 0.5289889859063077 time 81710.57\n",
      "epoch 1 batch id 27801 loss 0.7181577086448669 train acc 0.528901838063379 time 82292.15\n",
      "epoch 1 batch id 28001 loss 0.6942431926727295 train acc 0.5286195135888004 time 82860.02\n",
      "epoch 1 batch id 28201 loss 0.6826333999633789 train acc 0.5284741675827098 time 83427.68\n",
      "epoch 1 batch id 28401 loss 0.7054903507232666 train acc 0.5281284109714447 time 84026.49\n",
      "epoch 1 batch id 28601 loss 0.6941555142402649 train acc 0.5280278661585259 time 84632.8\n",
      "epoch 1 batch id 28801 loss 0.7160966396331787 train acc 0.5277464324155411 time 85219.63\n",
      "epoch 1 batch id 29001 loss 0.6724185943603516 train acc 0.5275378435226371 time 85776.47\n",
      "epoch 1 batch id 29201 loss 0.6833016276359558 train acc 0.5274605321735557 time 86333.21\n",
      "epoch 1 batch id 29401 loss 0.7047635316848755 train acc 0.5273460086391619 time 86890.12\n",
      "epoch 1 batch id 29601 loss 0.6940571665763855 train acc 0.5272710381406034 time 87446.99\n",
      "epoch 1 batch id 29801 loss 0.7150594592094421 train acc 0.5270502667695715 time 88003.67\n",
      "epoch 1 batch id 30001 loss 0.7044384479522705 train acc 0.5269574347521749 time 88560.34\n",
      "epoch 1 batch id 30201 loss 0.6732954978942871 train acc 0.5268409986424291 time 89117.11\n",
      "epoch 1 batch id 30401 loss 0.6737223863601685 train acc 0.5265698496759975 time 89673.79\n",
      "epoch 1 batch id 30601 loss 0.6939475536346436 train acc 0.5264247900395412 time 90230.6\n",
      "epoch 1 batch id 30801 loss 0.6939290761947632 train acc 0.5262694393039187 time 90787.22\n",
      "epoch 1 batch id 31001 loss 0.693921685218811 train acc 0.5262330247411374 time 91343.8\n",
      "epoch 1 batch id 31201 loss 0.7036327719688416 train acc 0.5260448383064645 time 91900.39\n",
      "epoch 1 batch id 31401 loss 0.6938832998275757 train acc 0.525882933664533 time 92457.03\n",
      "epoch 1 batch id 31601 loss 0.6938615441322327 train acc 0.5257033005284643 time 93013.79\n",
      "epoch 1 batch id 31801 loss 0.6845223903656006 train acc 0.5255573724096727 time 93570.39\n",
      "epoch 1 batch id 32001 loss 0.721383810043335 train acc 0.5253898315677635 time 94127.09\n",
      "epoch 1 batch id 32201 loss 0.7029682993888855 train acc 0.5253175367224621 time 94683.51\n",
      "epoch 1 batch id 32401 loss 0.7027885913848877 train acc 0.5251226813987223 time 95240.28\n",
      "epoch 1 batch id 32601 loss 0.6937593221664429 train acc 0.5248535321002423 time 95797.19\n",
      "epoch 1 batch id 32801 loss 0.7023518085479736 train acc 0.5247134233712387 time 96353.86\n",
      "epoch 1 batch id 33001 loss 0.6852526664733887 train acc 0.5245295597103118 time 96910.39\n",
      "epoch 1 batch id 33201 loss 0.6768734455108643 train acc 0.5244570946658234 time 97466.94\n",
      "epoch 1 batch id 33401 loss 0.6937018632888794 train acc 0.5243181341875992 time 98023.53\n",
      "epoch 1 batch id 33601 loss 0.6936730742454529 train acc 0.5240803845123657 time 98580.03\n",
      "epoch 1 batch id 33801 loss 0.7173621654510498 train acc 0.5238602408212775 time 99136.72\n",
      "epoch 1 batch id 34001 loss 0.685848593711853 train acc 0.5237345960412929 time 99693.34\n",
      "epoch 1 batch id 34201 loss 0.693626880645752 train acc 0.5236286950673957 time 100249.9\n",
      "epoch 1 batch id 34401 loss 0.6936156749725342 train acc 0.5235276590796779 time 100806.42\n",
      "epoch 1 batch id 34601 loss 0.7162523865699768 train acc 0.5233736019190197 time 101362.95\n",
      "epoch 1 batch id 34801 loss 0.7086901664733887 train acc 0.523325479152898 time 101919.51\n",
      "epoch 1 batch id 35001 loss 0.7084305882453918 train acc 0.5231850518556612 time 102475.99\n",
      "epoch 1 batch id 35201 loss 0.6862520575523376 train acc 0.523049771313315 time 103032.28\n",
      "epoch 1 batch id 35401 loss 0.6790966391563416 train acc 0.5229407361373972 time 103588.63\n",
      "epoch 1 batch id 35601 loss 0.6935547590255737 train acc 0.5228048369427825 time 104145.16\n",
      "epoch 1 batch id 35801 loss 0.6794695854187012 train acc 0.5226879137454261 time 104701.58\n",
      "epoch 1 batch id 36001 loss 0.7075960636138916 train acc 0.522613955167912 time 105257.85\n",
      "epoch 1 batch id 36201 loss 0.6866261959075928 train acc 0.5224855666970526 time 105814.31\n",
      "epoch 1 batch id 36401 loss 0.7002010345458984 train acc 0.5222796077030851 time 106370.91\n",
      "epoch 1 batch id 36601 loss 0.6801914572715759 train acc 0.5221988470260376 time 106927.38\n",
      "epoch 1 batch id 36801 loss 0.7002537846565247 train acc 0.5222072769761691 time 107483.51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 37001 loss 0.6868007183074951 train acc 0.5221514283397746 time 108039.73\n",
      "epoch 1 batch id 37201 loss 0.6934949159622192 train acc 0.5220121770920136 time 108596.11\n",
      "epoch 1 batch id 37401 loss 0.6870068311691284 train acc 0.5218643886527098 time 109152.39\n",
      "epoch 1 batch id 37601 loss 0.6934705972671509 train acc 0.5217281455280445 time 109708.56\n",
      "epoch 1 batch id 37801 loss 0.6810169816017151 train acc 0.521566889764821 time 110263.52\n",
      "epoch 1 batch id 38001 loss 0.699654221534729 train acc 0.5215027236125365 time 110818.71\n",
      "epoch 1 batch id 38201 loss 0.6934456825256348 train acc 0.521406507683045 time 111374.94\n",
      "epoch 1 batch id 38401 loss 0.6934536695480347 train acc 0.5214024374365251 time 111931.05\n",
      "epoch 1 batch id 38601 loss 0.699426531791687 train acc 0.5212073521411362 time 112486.17\n",
      "epoch 1 batch id 38801 loss 0.699333906173706 train acc 0.5210948171438881 time 113042.2\n",
      "epoch 1 batch id 39001 loss 0.6875702142715454 train acc 0.521034717058537 time 113598.57\n",
      "epoch 1 batch id 39201 loss 0.6877544522285461 train acc 0.5208444937629142 time 114154.77\n",
      "epoch 1 batch id 39401 loss 0.6878324747085571 train acc 0.5207386868353595 time 114710.95\n",
      "epoch 1 batch id 39601 loss 0.6933857202529907 train acc 0.5205960708062928 time 115267.08\n",
      "epoch 1 batch id 39801 loss 0.6824421882629395 train acc 0.5205899349262582 time 115823.47\n",
      "epoch 1 batch id 40001 loss 0.7100675106048584 train acc 0.5205526111847204 time 116380.01\n",
      "epoch 1 batch id 40201 loss 0.699065625667572 train acc 0.5206120494515062 time 116936.53\n",
      "epoch 1 batch id 40401 loss 0.6879615783691406 train acc 0.5203862528155244 time 117489.61\n",
      "epoch 1 batch id 40601 loss 0.7095094323158264 train acc 0.5202981453658777 time 118042.9\n",
      "epoch 1 batch id 40801 loss 0.6985867023468018 train acc 0.5201496286855715 time 118599.61\n",
      "epoch 1 batch id 41001 loss 0.6933580636978149 train acc 0.5200360966805687 time 119156.35\n",
      "epoch 1 batch id 41201 loss 0.6933491230010986 train acc 0.5199054634596247 time 119713.14\n",
      "epoch 1 batch id 41401 loss 0.673308253288269 train acc 0.5198606313857154 time 120266.16\n",
      "epoch 1 batch id 41601 loss 0.6981885433197021 train acc 0.5196930362250908 time 120822.84\n",
      "epoch 1 batch id 41801 loss 0.6885151267051697 train acc 0.5196406784526686 time 121379.5\n",
      "epoch 1 batch id 42001 loss 0.6981201171875 train acc 0.5195650103568963 time 121936.03\n",
      "epoch 1 batch id 42201 loss 0.7075636386871338 train acc 0.5194782114167911 time 122492.64\n",
      "epoch 1 batch id 42401 loss 0.7116620540618896 train acc 0.5193185302233438 time 123049.08\n",
      "epoch 1 batch id 42601 loss 0.6890799403190613 train acc 0.5190312433980423 time 123605.55\n",
      "epoch 1 batch id 42801 loss 0.6851001977920532 train acc 0.5189101890142753 time 124161.74\n",
      "epoch 1 batch id 43001 loss 0.6807628273963928 train acc 0.5189239785121276 time 124718.08\n",
      "epoch 1 batch id 43201 loss 0.7018305063247681 train acc 0.5189000254623736 time 125274.28\n",
      "epoch 1 batch id 43401 loss 0.6932848691940308 train acc 0.5187927697518491 time 125830.69\n",
      "epoch 1 batch id 43601 loss 0.6972468495368958 train acc 0.5186148253480425 time 126386.93\n",
      "epoch 1 batch id 43801 loss 0.6971377730369568 train acc 0.5184898746603959 time 126943.18\n",
      "epoch 1 batch id 44001 loss 0.6932541131973267 train acc 0.5183092429717506 time 127499.46\n",
      "epoch 1 batch id 44201 loss 0.6932486295700073 train acc 0.5181981176896451 time 128055.77\n",
      "epoch 1 batch id 44401 loss 0.6966904401779175 train acc 0.5180654714983897 time 128612.07\n",
      "epoch 1 batch id 44601 loss 0.6932355761528015 train acc 0.5179564359543508 time 129168.33\n",
      "epoch 1 batch id 44801 loss 0.696519136428833 train acc 0.5178874355483136 time 129724.61\n",
      "epoch 1 batch id 45001 loss 0.6899691224098206 train acc 0.5178190484655897 time 130280.73\n",
      "epoch 1 batch id 45201 loss 0.6900660991668701 train acc 0.5177263777350058 time 130836.97\n",
      "epoch 1 batch id 45401 loss 0.6963858604431152 train acc 0.5176400299552874 time 131393.28\n",
      "epoch 1 batch id 45601 loss 0.6932286024093628 train acc 0.517639415802285 time 131949.49\n",
      "epoch 1 batch id 45801 loss 0.6964439153671265 train acc 0.5175760354577411 time 132505.76\n",
      "epoch 1 batch id 46001 loss 0.6899880170822144 train acc 0.5175594008825896 time 133062.12\n",
      "epoch 1 batch id 46201 loss 0.6932287812232971 train acc 0.5174752710980282 time 133618.51\n",
      "epoch 1 batch id 46401 loss 0.6964378952980042 train acc 0.517443050796319 time 134174.78\n",
      "epoch 1 batch id 46601 loss 0.6995021104812622 train acc 0.5173601424862128 time 134731.09\n",
      "epoch 1 batch id 46801 loss 0.6932299137115479 train acc 0.5173447148565201 time 135287.22\n",
      "epoch 1 batch id 47001 loss 0.6932292580604553 train acc 0.5173028233441842 time 135843.59\n",
      "epoch 1 batch id 47201 loss 0.7020525932312012 train acc 0.5170785576576767 time 136399.82\n",
      "epoch 1 batch id 47401 loss 0.6961480975151062 train acc 0.5170355055800511 time 136956.15\n",
      "epoch 1 batch id 47601 loss 0.6875896453857422 train acc 0.5169324173861893 time 137512.47\n",
      "epoch 1 batch id 47801 loss 0.6932010650634766 train acc 0.5167465115792557 time 138068.84\n",
      "epoch 1 batch id 48001 loss 0.6932004690170288 train acc 0.5166845482385783 time 138625.11\n",
      "epoch 1 batch id 48201 loss 0.6932017803192139 train acc 0.516656812099334 time 139181.53\n",
      "epoch 1 batch id 48401 loss 0.7008036971092224 train acc 0.5165647403979257 time 139737.89\n",
      "epoch 1 batch id 48601 loss 0.6982237100601196 train acc 0.5165017180716446 time 140294.39\n",
      "epoch 1 batch id 48801 loss 0.695676863193512 train acc 0.5164315280424582 time 140850.64\n",
      "epoch 1 batch id 49001 loss 0.6908230185508728 train acc 0.5163236464561948 time 141407.08\n",
      "epoch 1 batch id 49201 loss 0.6976820230484009 train acc 0.5162064795430987 time 141963.67\n",
      "epoch 1 batch id 49401 loss 0.690909206867218 train acc 0.5161838829173498 time 142520.02\n",
      "epoch 1 batch id 49601 loss 0.6885974407196045 train acc 0.5161513880768532 time 143076.65\n",
      "epoch 1 batch id 49801 loss 0.6884530782699585 train acc 0.5161367241621654 time 143633.14\n",
      "epoch 1 batch id 50001 loss 0.6908049583435059 train acc 0.5161146777064459 time 144189.72\n",
      "epoch 1 batch id 50201 loss 0.6931912899017334 train acc 0.5160380271309336 time 144746.3\n",
      "epoch 1 batch id 50401 loss 0.69764244556427 train acc 0.515909902581298 time 145302.87\n",
      "epoch 1 batch id 50601 loss 0.6908360719680786 train acc 0.5159433608031462 time 145859.59\n",
      "epoch 1 batch id 50801 loss 0.6979485750198364 train acc 0.5159199622054684 time 146416.16\n",
      "epoch 1 batch id 51001 loss 0.6884015798568726 train acc 0.5158918452579361 time 146972.51\n",
      "epoch 1 batch id 51201 loss 0.6957134008407593 train acc 0.5159127751411106 time 147527.93\n",
      "epoch 1 batch id 51401 loss 0.6956518888473511 train acc 0.5158362677768915 time 148083.64\n",
      "epoch 1 batch id 51601 loss 0.6977187991142273 train acc 0.5156876804713086 time 148640.1\n",
      "epoch 1 batch id 51801 loss 0.6973800659179688 train acc 0.5155474797783827 time 149196.76\n",
      "epoch 1 batch id 52001 loss 0.6951276063919067 train acc 0.5154275879310013 time 149753.32\n",
      "epoch 1 batch id 52201 loss 0.6877580881118774 train acc 0.5153277714986303 time 150308.76\n",
      "epoch 1 batch id 52401 loss 0.6931685209274292 train acc 0.5151690807427339 time 150865.26\n",
      "epoch 1 batch id 52601 loss 0.6948717832565308 train acc 0.5151589323396893 time 151422.05\n",
      "epoch 1 batch id 52801 loss 0.6949036121368408 train acc 0.5151464934376243 time 151978.6\n",
      "epoch 1 batch id 53001 loss 0.691323459148407 train acc 0.5151624497650987 time 152535.41\n",
      "epoch 1 batch id 53201 loss 0.6913944482803345 train acc 0.5150725550271611 time 153092.14\n",
      "epoch 1 batch id 53401 loss 0.6894801259040833 train acc 0.5150863279713863 time 153648.93\n",
      "epoch 1 batch id 53601 loss 0.6931804418563843 train acc 0.515132646778978 time 154205.75\n",
      "epoch 1 batch id 53801 loss 0.6911293268203735 train acc 0.5151019497778851 time 154759.01\n",
      "epoch 1 batch id 54001 loss 0.6931799650192261 train acc 0.5150344438065961 time 155312.11\n",
      "epoch 1 batch id 54201 loss 0.6931801438331604 train acc 0.5150089481743879 time 155868.51\n",
      "epoch 1 batch id 54401 loss 0.6931778192520142 train acc 0.5149216007058693 time 156425.01\n",
      "epoch 1 batch id 54601 loss 0.6911401748657227 train acc 0.5149356238896724 time 156981.47\n",
      "epoch 1 batch id 54801 loss 0.6931803822517395 train acc 0.514885677268663 time 157537.84\n",
      "epoch 1 batch id 55001 loss 0.6911273002624512 train acc 0.5148451846330067 time 158090.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 55201 loss 0.691095769405365 train acc 0.5148344232894332 time 158647.06\n",
      "epoch 1 batch id 55401 loss 0.6992965936660767 train acc 0.514758307611776 time 159203.5\n",
      "epoch 1 batch id 55601 loss 0.6911715865135193 train acc 0.5147097174511249 time 159759.96\n",
      "epoch 1 batch id 55801 loss 0.6872472763061523 train acc 0.5146659558072436 time 160316.33\n",
      "epoch 1 batch id 56001 loss 0.6987712383270264 train acc 0.5145466152390136 time 160872.78\n",
      "epoch 1 batch id 56201 loss 0.6931712031364441 train acc 0.5144659347698439 time 161429.28\n",
      "epoch 1 batch id 56401 loss 0.6931722164154053 train acc 0.5144456658569884 time 161985.72\n",
      "epoch 1 batch id 56601 loss 0.6931712627410889 train acc 0.514379162912316 time 162542.12\n",
      "epoch 1 batch id 56801 loss 0.6931695342063904 train acc 0.5143065262935512 time 163098.61\n",
      "epoch 1 batch id 57001 loss 0.6916094422340393 train acc 0.5142146629006509 time 163654.97\n",
      "epoch 1 batch id 57201 loss 0.696082592010498 train acc 0.5141147007919442 time 164211.32\n",
      "epoch 1 batch id 57401 loss 0.6916593909263611 train acc 0.5141177854044354 time 164767.62\n",
      "epoch 1 batch id 57601 loss 0.690054178237915 train acc 0.5140991475842434 time 165323.85\n",
      "epoch 1 batch id 57801 loss 0.6931597590446472 train acc 0.5139011435788308 time 165880.28\n",
      "epoch 1 batch id 58001 loss 0.6906951665878296 train acc 0.5138596748331925 time 166436.85\n",
      "epoch 1 batch id 58201 loss 0.690926194190979 train acc 0.5137626501262865 time 166993.22\n",
      "epoch 1 batch id 58401 loss 0.6953212022781372 train acc 0.5136962552011096 time 167549.53\n",
      "epoch 1 batch id 58601 loss 0.6901888847351074 train acc 0.5136239142676746 time 168106.02\n",
      "epoch 1 batch id 58801 loss 0.6922019124031067 train acc 0.5135669461403718 time 168662.58\n",
      "epoch 1 batch id 59001 loss 0.6931524872779846 train acc 0.5134573990271352 time 169219.06\n",
      "epoch 1 batch id 59201 loss 0.6931535005569458 train acc 0.5134436073714971 time 169775.74\n",
      "epoch 1 batch id 59401 loss 0.6915178298950195 train acc 0.5133857174121648 time 170332.15\n",
      "epoch 1 batch id 59601 loss 0.6957744359970093 train acc 0.5133596751732353 time 170888.52\n",
      "epoch 1 batch id 59801 loss 0.6949659585952759 train acc 0.5133338071269711 time 171444.86\n",
      "epoch 1 batch id 60001 loss 0.6937028169631958 train acc 0.5131185313578107 time 172001.23\n",
      "epoch 1 batch id 60201 loss 0.6937086582183838 train acc 0.5130915599408647 time 172557.57\n",
      "epoch 1 batch id 60401 loss 0.6941920518875122 train acc 0.5130337246072085 time 173113.93\n",
      "epoch 1 batch id 60601 loss 0.6936365365982056 train acc 0.5129824590353295 time 173670.26\n",
      "epoch 1 batch id 60801 loss 0.693149209022522 train acc 0.5129500337165507 time 174226.57\n",
      "epoch 1 batch id 61001 loss 0.692715048789978 train acc 0.5128809363780922 time 174782.97\n",
      "epoch 1 batch id 61201 loss 0.6923241019248962 train acc 0.5128388425025735 time 175339.11\n",
      "epoch 1 batch id 61401 loss 0.6931487917900085 train acc 0.5127990586472533 time 175895.31\n",
      "epoch 1 batch id 61601 loss 0.6931480765342712 train acc 0.5127088034285158 time 176451.12\n",
      "epoch 1 batch id 61801 loss 0.6931478977203369 train acc 0.5126616074173557 time 177007.09\n",
      "epoch 1 batch id 62001 loss 0.6931479573249817 train acc 0.5126328607603103 time 177563.04\n",
      "epoch 1 batch id 62201 loss 0.6925573348999023 train acc 0.5125922412822945 time 178119.1\n",
      "epoch 1 batch id 62401 loss 0.6935715079307556 train acc 0.5126019615070271 time 178675.29\n",
      "epoch 1 batch id 62601 loss 0.6931484341621399 train acc 0.5125477228798262 time 179231.22\n",
      "epoch 1 batch id 62801 loss 0.6931487917900085 train acc 0.5125455804843871 time 179787.27\n",
      "epoch 1 batch id 63001 loss 0.693524956703186 train acc 0.5124680560626022 time 180343.32\n",
      "epoch 1 batch id 63201 loss 0.692824125289917 train acc 0.5123989335611778 time 180899.37\n",
      "epoch 1 batch id 63401 loss 0.69362473487854 train acc 0.5124524849765777 time 181455.4\n",
      "epoch 1 batch id 63601 loss 0.6940386891365051 train acc 0.5123936730554551 time 182011.38\n",
      "epoch 1 batch id 63801 loss 0.6937005519866943 train acc 0.5124018432312973 time 182567.33\n",
      "epoch 1 batch id 64001 loss 0.692583441734314 train acc 0.5123845721160607 time 183123.45\n",
      "epoch 1 batch id 64201 loss 0.693149745464325 train acc 0.5123382034547749 time 183679.59\n",
      "epoch 1 batch id 64401 loss 0.6926143169403076 train acc 0.5122824179748762 time 184235.87\n",
      "epoch 1 batch id 64601 loss 0.6947964429855347 train acc 0.5122618070927695 time 184791.11\n",
      "epoch 1 batch id 64801 loss 0.6926614046096802 train acc 0.5122027437848181 time 185346.31\n",
      "epoch 1 batch id 65001 loss 0.6926071047782898 train acc 0.5121863509792157 time 185902.42\n",
      "epoch 1 batch id 65201 loss 0.6927539706230164 train acc 0.5120933728010306 time 186458.55\n",
      "epoch 1 batch id 65401 loss 0.6931478381156921 train acc 0.5120105197168239 time 187014.57\n",
      "epoch 1 batch id 65601 loss 0.6929358243942261 train acc 0.5119453209554733 time 187570.71\n",
      "epoch 1 batch id 65801 loss 0.693335771560669 train acc 0.5118919165362229 time 188125.68\n",
      "epoch 1 batch id 66001 loss 0.6923789978027344 train acc 0.5118994409175619 time 188681.73\n",
      "epoch 1 batch id 66201 loss 0.6928192377090454 train acc 0.5118842615670458 time 189237.95\n",
      "epoch 1 batch id 66401 loss 0.693148136138916 train acc 0.5118729386605624 time 189794.09\n",
      "epoch 1 batch id 66601 loss 0.6931488513946533 train acc 0.5118804522454617 time 190350.31\n",
      "epoch 1 batch id 66801 loss 0.6935973167419434 train acc 0.5118280414963848 time 190906.74\n",
      "epoch 1 batch id 67001 loss 0.6935633420944214 train acc 0.5117908687930031 time 191463.17\n",
      "epoch 1 batch id 67201 loss 0.6937012672424316 train acc 0.5118115801848186 time 192015.82\n",
      "epoch 1 batch id 67401 loss 0.6917275190353394 train acc 0.5118711146718892 time 192568.67\n",
      "epoch 1 batch id 67601 loss 0.6931526064872742 train acc 0.5118748243369181 time 193124.79\n",
      "epoch 1 batch id 67801 loss 0.6910811066627502 train acc 0.5119319774044631 time 193680.99\n",
      "epoch 1 train acc 0.5118578893141007 time 194067.14\n",
      "best_acc: 8492.25\n",
      "epoch 1 test acc 0.4999852811304092\n",
      "epoch 2 batch id 1 loss 0.694074273109436 train acc 0.375 time 5.27\n",
      "epoch 2 batch id 201 loss 0.6940509080886841 train acc 0.4987562189054726 time 561.59\n",
      "epoch 2 batch id 401 loss 0.6910598278045654 train acc 0.5130922693266833 time 1117.94\n",
      "epoch 2 batch id 601 loss 0.6920976042747498 train acc 0.5103993344425957 time 1674.08\n",
      "epoch 2 batch id 801 loss 0.6943870186805725 train acc 0.5137328339575531 time 2230.3\n",
      "epoch 2 batch id 1001 loss 0.6943032145500183 train acc 0.5091158841158842 time 2786.56\n",
      "epoch 2 batch id 1201 loss 0.6956084370613098 train acc 0.5096794338051623 time 3342.58\n",
      "epoch 2 batch id 1401 loss 0.6919447183609009 train acc 0.5084760885082085 time 3898.65\n",
      "epoch 2 batch id 1601 loss 0.6931588649749756 train acc 0.5074953154278576 time 4454.92\n",
      "epoch 2 batch id 1801 loss 0.6954845190048218 train acc 0.5065241532481954 time 5011.4\n",
      "epoch 2 batch id 2001 loss 0.6920663118362427 train acc 0.5046851574212894 time 5568.01\n",
      "epoch 2 batch id 2201 loss 0.6912963390350342 train acc 0.5024988641526579 time 6124.57\n",
      "epoch 2 batch id 2401 loss 0.6911219954490662 train acc 0.5035401915868388 time 6680.96\n",
      "epoch 2 batch id 2601 loss 0.6941404342651367 train acc 0.5029796232218378 time 7237.51\n",
      "epoch 2 batch id 2801 loss 0.6941124200820923 train acc 0.5026776151374509 time 7794.21\n",
      "epoch 2 batch id 3001 loss 0.6922400593757629 train acc 0.5023325558147285 time 8350.87\n",
      "epoch 2 batch id 3201 loss 0.6921517848968506 train acc 0.503045923149016 time 8907.54\n",
      "epoch 2 batch id 3401 loss 0.6941961050033569 train acc 0.503418112319906 time 9464.43\n",
      "epoch 2 batch id 3601 loss 0.6941085457801819 train acc 0.5023951680088864 time 10021.14\n",
      "epoch 2 batch id 3801 loss 0.690386176109314 train acc 0.5024993422783478 time 10577.89\n",
      "epoch 2 batch id 4001 loss 0.692305326461792 train acc 0.5017495626093477 time 11134.65\n",
      "epoch 2 batch id 4201 loss 0.6940121054649353 train acc 0.5016662699357296 time 11691.26\n",
      "epoch 2 batch id 4401 loss 0.6940078735351562 train acc 0.501704158145876 time 12248.09\n",
      "epoch 2 batch id 4601 loss 0.6924022436141968 train acc 0.5011682242990654 time 12804.75\n",
      "epoch 2 batch id 4801 loss 0.6917175650596619 train acc 0.5010154134555301 time 13360.36\n",
      "epoch 2 batch id 5001 loss 0.693924069404602 train acc 0.5012997400519896 time 13916.33\n",
      "epoch 2 batch id 5201 loss 0.6931522488594055 train acc 0.5014420303787733 time 14473.21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 5401 loss 0.69315105676651 train acc 0.5009026106276615 time 15030.23\n",
      "epoch 2 batch id 5601 loss 0.6949414014816284 train acc 0.5003347616497054 time 15587.27\n",
      "epoch 2 batch id 5801 loss 0.6931489706039429 train acc 0.49978451991036027 time 16144.14\n",
      "epoch 2 batch id 6001 loss 0.6928141117095947 train acc 0.4992084652557907 time 16701.18\n",
      "epoch 2 batch id 6201 loss 0.6931477189064026 train acc 0.49893162393162394 time 17258.28\n",
      "epoch 2 batch id 6401 loss 0.6931480169296265 train acc 0.4992579284486799 time 17814.29\n",
      "epoch 2 batch id 6601 loss 0.6937879920005798 train acc 0.499280412058779 time 18371.4\n",
      "epoch 2 batch id 6801 loss 0.693147599697113 train acc 0.4988604616968093 time 18928.39\n",
      "epoch 2 batch id 7001 loss 0.6933163404464722 train acc 0.49866090558491644 time 19485.37\n",
      "epoch 2 batch id 7201 loss 0.6933102607727051 train acc 0.4987501735870018 time 20042.32\n",
      "epoch 2 batch id 7401 loss 0.692475438117981 train acc 0.4990879610863397 time 20595.54\n",
      "epoch 2 batch id 7601 loss 0.6933937072753906 train acc 0.499095513748191 time 21152.01\n",
      "epoch 2 batch id 7801 loss 0.6928024291992188 train acc 0.4988463017561851 time 21707.71\n",
      "epoch 2 batch id 8001 loss 0.6929683685302734 train acc 0.49901574803149606 time 22264.23\n",
      "epoch 2 batch id 8201 loss 0.6929050087928772 train acc 0.4992988659919522 time 22820.69\n",
      "epoch 2 batch id 8401 loss 0.6935420036315918 train acc 0.4988543030591596 time 23377.1\n",
      "epoch 2 batch id 8601 loss 0.693147599697113 train acc 0.499244273921637 time 23933.4\n",
      "epoch 2 batch id 8801 loss 0.693147599697113 train acc 0.49928985342574705 time 24489.96\n",
      "epoch 2 batch id 9001 loss 0.6928837299346924 train acc 0.4994167314742806 time 25045.31\n",
      "epoch 2 batch id 9201 loss 0.692763090133667 train acc 0.49919845668949026 time 25601.76\n",
      "epoch 2 batch id 9401 loss 0.6929718255996704 train acc 0.4991357302414637 time 26158.1\n",
      "epoch 2 batch id 9601 loss 0.6931473612785339 train acc 0.49908863660035413 time 26714.39\n",
      "epoch 2 batch id 9801 loss 0.6930962800979614 train acc 0.49882665034180185 time 27270.81\n",
      "epoch 2 batch id 10001 loss 0.6931107640266418 train acc 0.4988751124887511 time 27827.11\n",
      "epoch 2 batch id 10201 loss 0.6930347681045532 train acc 0.4990197039505931 time 28383.6\n",
      "epoch 2 batch id 10401 loss 0.6932332515716553 train acc 0.49906259013556387 time 28940.14\n",
      "epoch 2 batch id 10601 loss 0.6927626729011536 train acc 0.4993750589567022 time 29496.64\n",
      "epoch 2 batch id 10801 loss 0.6932897567749023 train acc 0.4992361818350153 time 30053.07\n",
      "epoch 2 batch id 11001 loss 0.6928786635398865 train acc 0.4991364421416235 time 30609.4\n",
      "epoch 2 batch id 11201 loss 0.6931473016738892 train acc 0.4992411391840014 time 31165.64\n",
      "epoch 2 batch id 11401 loss 0.6931479573249817 train acc 0.4997368651872643 time 31721.95\n",
      "epoch 2 batch id 11601 loss 0.693510890007019 train acc 0.49983837600206876 time 32278.0\n",
      "epoch 2 batch id 11801 loss 0.6931480765342712 train acc 0.49984111515973223 time 32834.13\n",
      "epoch 2 batch id 12001 loss 0.6931481957435608 train acc 0.4998333472210649 time 33390.26\n",
      "epoch 2 batch id 12201 loss 0.692732572555542 train acc 0.5000102450618802 time 33946.56\n",
      "epoch 2 batch id 12401 loss 0.6939957141876221 train acc 0.5000100798322716 time 34502.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-110:\n",
      "Process Process-112:\n",
      "Process Process-111:\n",
      "Process Process-109:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process Process-113:\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/multiprocessing/process.py\", line 261, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/multiprocessing/process.py\", line 261, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/multiprocessing/process.py\", line 261, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/multiprocessing/process.py\", line 261, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/multiprocessing/util.py\", line 322, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/multiprocessing/util.py\", line 322, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/multiprocessing/util.py\", line 322, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/multiprocessing/util.py\", line 322, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/multiprocessing/util.py\", line 262, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/multiprocessing/util.py\", line 262, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/multiprocessing/util.py\", line 262, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/multiprocessing/util.py\", line 262, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/multiprocessing/util.py\", line 186, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/multiprocessing/util.py\", line 186, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/multiprocessing/util.py\", line 186, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/multiprocessing/util.py\", line 186, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/multiprocessing/queues.py\", line 191, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/multiprocessing/queues.py\", line 191, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/multiprocessing/process.py\", line 261, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/multiprocessing/queues.py\", line 191, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/multiprocessing/queues.py\", line 191, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/threading.py\", line 1056, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/threading.py\", line 1056, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/multiprocessing/util.py\", line 322, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/threading.py\", line 1056, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/threading.py\", line 1056, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/threading.py\", line 1072, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/threading.py\", line 1072, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/multiprocessing/util.py\", line 262, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/threading.py\", line 1072, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/threading.py\", line 1072, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/multiprocessing/util.py\", line 186, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/multiprocessing/queues.py\", line 191, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/threading.py\", line 1056, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/threading.py\", line 1072, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fc9d76bee80>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/multiprocessing/process.py\", line 124, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/multiprocessing/popen_fork.py\", line 47, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/bis/miniconda3/envs/ProtEmbedding/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-ac63adb9f466>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mtrain_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcalc_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ProtEmbedding/lib/python3.6/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ProtEmbedding/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    117\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m                    )\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ProtEmbedding/lib/python3.6/site-packages/torch/optim/functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "for e in range(num_epochs):\n",
    "    t0 = time.time()\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    for batch_id, (pep_token_ids, n_token_ids, c_token_ids, m1_token_ids, m2_token_ids, label) in enumerate(train_dataloader):\n",
    "#         print(batch_id, round(time.time()-t0, 2))  # batch8->70000 loop, each 3 sec -> per 7 epoch, 3h\n",
    "        \n",
    "        pep_token_ids = pep_token_ids.long().to(device)\n",
    "        n_token_ids = n_token_ids.long().to(device)\n",
    "        c_token_ids = c_token_ids.long().to(device)\n",
    "        m1_token_ids = m1_token_ids.long().to(device)\n",
    "        m2_token_ids = m2_token_ids.long().to(device)\n",
    "        label = torch.reshape(label.float(), (-1, 1)).to(device)\n",
    "        \n",
    "        pred = model(pep_token_ids, n_token_ids, c_token_ids, m1_token_ids, m2_token_ids)\n",
    "        loss = F.binary_cross_entropy(pred, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_acc += calc_accuracy(pred, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {} time {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1), round(time.time()-t0, 2)))\n",
    "        \n",
    "    print(\"epoch {} train acc {} time {}\".format(e+1, train_acc / (batch_id+1), round(time.time()-t0,2)))\n",
    "    \n",
    "    model.eval()\n",
    "    for batch_id, (pep_token_ids, n_token_ids, c_token_ids, m1_token_ids, m2_token_ids, label) in enumerate(valid_dataloader):\n",
    "        pep_token_ids = pep_token_ids.long().to(device)\n",
    "        n_token_ids = n_token_ids.long().to(device)\n",
    "        c_token_ids = c_token_ids.long().to(device)\n",
    "        m1_token_ids = m1_token_ids.long().to(device)\n",
    "        m2_token_ids = m2_token_ids.long().to(device)\n",
    "        label = label.long().to(device)\n",
    "        label = torch.reshape(label, (-1, 1))\n",
    "        pred = model(pep_token_ids, n_token_ids, c_token_ids, m1_token_ids, m2_token_ids)\n",
    "        \n",
    "        test_acc += calc_accuracy(pred, label)\n",
    "    if test_acc > best_acc:\n",
    "        best_acc=test_acc\n",
    "        torch.save({\"best_acc\":best_acc / (batch_id+1) ,\"model\":model.state_dict()},f'./finetuning_whole.pl')\n",
    "        print(f\"best_acc: {best_acc}\")\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T02:51:48.423088Z",
     "start_time": "2021-12-06T02:51:48.419817Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.817590277777777"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(train_dataloader) / 8) * (8.18-5.29) / 3600  # 1210금요일 확인"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ProtEmbedding",
   "language": "python",
   "name": "protembedding"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
