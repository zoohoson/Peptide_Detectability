{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T17:36:58.182439Z",
     "start_time": "2021-12-27T17:36:57.105574Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T17:36:58.467674Z",
     "start_time": "2021-12-27T17:36:58.448629Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19, 21)\n"
     ]
    }
   ],
   "source": [
    "df_aaindex = pd.read_csv('../data/aaindex/df_aaindex19.csv')\n",
    "print(df_aaindex.shape)\n",
    "df_aaindex.head(1)\n",
    "tmp = df_aaindex.drop('Unnamed: 0',axis=1).T\n",
    "aa2val = dict()\n",
    "for aa, val in zip(tmp.index, tmp.values):\n",
    "    aa2val[aa]=val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T17:37:02.404314Z",
     "start_time": "2021-12-27T17:36:59.582321Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>peptide</th>\n",
       "      <th>En</th>\n",
       "      <th>Ec</th>\n",
       "      <th>E1</th>\n",
       "      <th>E2</th>\n",
       "      <th>protein</th>\n",
       "      <th>PEP</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>595411</th>\n",
       "      <td>K.QELNEPPKQSTSFLVLQEILESEEKGDPNK.P</td>\n",
       "      <td>VYKMLQEKQELNEPP</td>\n",
       "      <td>EEKGDPNKPSGFRSV</td>\n",
       "      <td>QELNEPPKQSTSFLV</td>\n",
       "      <td>EILESEEKGDPNKPS</td>\n",
       "      <td>sp|O00151|PDLI1_HUMAN</td>\n",
       "      <td>QELNEPPKQSTSFLVLQEILESEEKGDPNK</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   peptide               En               Ec  \\\n",
       "595411  K.QELNEPPKQSTSFLVLQEILESEEKGDPNK.P  VYKMLQEKQELNEPP  EEKGDPNKPSGFRSV   \n",
       "\n",
       "                     E1               E2                protein  \\\n",
       "595411  QELNEPPKQSTSFLV  EILESEEKGDPNKPS  sp|O00151|PDLI1_HUMAN   \n",
       "\n",
       "                                   PEP  ID  \n",
       "595411  QELNEPPKQSTSFLVLQEILESEEKGDPNK   0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_detect_peptide_train = pd.read_csv('../data/df_detect_peptide_train.csv')\n",
    "test = pd.read_csv('../data/df_detect_peptide_test.csv')\n",
    "train, val = train_test_split(df_detect_peptide_train, test_size=0.2, random_state=7)\n",
    "\n",
    "df = pd.concat([train, val, test], axis=0).reset_index(drop=True)\n",
    "\n",
    "train_idx = df.iloc[:len(train), :].index\n",
    "val_idx = df.iloc[len(train):len(train)+len(val), :].index\n",
    "test_idx = df.iloc[len(train)+len(val):, :].index\n",
    "\n",
    "train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T17:37:05.264516Z",
     "start_time": "2021-12-27T17:37:02.405528Z"
    }
   },
   "outputs": [],
   "source": [
    "data = [[n[:8]+p+c[8:], prot, lab] for _, n, c, __, ___, prot, p, lab in train.values]\n",
    "train_new = pd.DataFrame(data, columns=['peptide', 'protein', 'ID'])\n",
    "\n",
    "data = [[n[:8]+p+c[8:], prot, lab] for _, n, c, __, ___, prot, p, lab in val.values]\n",
    "val_new = pd.DataFrame(data, columns=['peptide', 'protein', 'ID'])\n",
    "\n",
    "data = [[n[:8]+p+c[8:], prot, lab] for _, n, c, __, ___, prot, p, lab in test.values]\n",
    "test_new = pd.DataFrame(data, columns=['peptide', 'protein', 'ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T17:37:05.271835Z",
     "start_time": "2021-12-27T17:37:05.265942Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>peptide</th>\n",
       "      <th>protein</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VYKMLQEKQELNEPPKQSTSFLVLQEILESEEKGDPNKPSGFRSV</td>\n",
       "      <td>sp|O00151|PDLI1_HUMAN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         peptide                protein  ID\n",
       "0  VYKMLQEKQELNEPPKQSTSFLVLQEILESEEKGDPNKPSGFRSV  sp|O00151|PDLI1_HUMAN   0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_new.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T17:37:05.983426Z",
     "start_time": "2021-12-27T17:37:05.272889Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "PATH_TO_REPO = \"/home/bis/2021_AIhub/esm/\"\n",
    "sys.path.append(PATH_TO_REPO)\n",
    "\n",
    "import torch\n",
    "import esm\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import time\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AdamW\n",
    "# from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "from transformers import WarmupLinearSchedule as get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T17:37:06.304391Z",
     "start_time": "2021-12-27T17:37:06.301772Z"
    }
   },
   "outputs": [],
   "source": [
    "##GPU 사용 시\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T17:37:17.881355Z",
     "start_time": "2021-12-27T17:37:06.929436Z"
    }
   },
   "outputs": [],
   "source": [
    "esm_model, alphabet = esm.pretrained.esm1b_t33_650M_UR50S()\n",
    "batch_converter = alphabet.get_batch_converter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T17:37:17.889534Z",
     "start_time": "2021-12-27T17:37:17.883270Z"
    }
   },
   "outputs": [],
   "source": [
    "ct = 0\n",
    "for child in esm_model.children():\n",
    "    ct += 1\n",
    "#     print(ct, child)\n",
    "#     if ct < 7:\n",
    "    for param in child.parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T17:37:17.901303Z",
     "start_time": "2021-12-27T17:37:17.890784Z"
    }
   },
   "outputs": [],
   "source": [
    "class ESMDataset(Dataset):\n",
    "    def __init__(self, datasets, idxes):\n",
    "        pep_idx, label_idx = idxes\n",
    "        pep_data = [(label, seq) for label, seq in zip(datasets[:, label_idx], datasets[:, pep_idx])]\n",
    "        labels, pep_strs, pep_tokens = batch_converter(pep_data)\n",
    "\n",
    "        self.sentences = pep_tokens\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return ((self.sentences[i], ) + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))\n",
    "\n",
    "# [PAD] = 1, [MASK] = 21  [CLS] = 0 (special classification token), [SEP] = 2 (seperate segment), Z = 27, '-' = 30, .=29, ,=28\n",
    "# J 없음\n",
    "# A 2, B 25, C 23, D 13, E 9, F 18, G 6, H 21, I 12, K 15, L 4, M 20, N 17, \n",
    "# O 28, P 14, Q 16, R 10, S 8, T 11, U 26, V 7, W 22, X 24, Y 19, Z 27\n",
    "# 3 5 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T17:37:17.909824Z",
     "start_time": "2021-12-27T17:37:17.902388Z"
    }
   },
   "outputs": [],
   "source": [
    "## Setting parameters\n",
    "max_len = 30\n",
    "batch_size = 256\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 10\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate =  1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T17:42:46.389429Z",
     "start_time": "2021-12-27T17:39:01.909641Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224.47 sec\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "\n",
    "dataset_train = train_new[['peptide', 'ID']].values\n",
    "dataset_valid = val_new[['peptide', 'ID']].values\n",
    "dataset_test = test_new[['peptide', 'ID']].values\n",
    "\n",
    "data_train = ESMDataset(dataset_train, [0, 1])\n",
    "data_valid = ESMDataset(dataset_valid, [0, 1])\n",
    "data_test = ESMDataset(dataset_test, [0, 1])\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=48)\n",
    "valid_dataloader = torch.utils.data.DataLoader(data_valid, batch_size=batch_size, num_workers=48)\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=48)\n",
    "\n",
    "e = time.time()\n",
    "print(round(e-s, 2),'sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T17:42:51.327000Z",
     "start_time": "2021-12-27T17:42:48.972937Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor([[ 0,  7, 19,  ...,  8,  7,  2],\n",
      "        [ 0, 10,  5,  ...,  1,  1,  1],\n",
      "        [ 0, 14, 10,  ...,  1,  1,  1],\n",
      "        ...,\n",
      "        [ 0,  5,  4,  ...,  1,  1,  1],\n",
      "        [ 0,  9,  4,  ...,  1,  1,  1],\n",
      "        [ 0,  9,  4,  ...,  1,  1,  1]])\n",
      "tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,\n",
      "        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,\n",
      "        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,\n",
      "        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
      "        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,\n",
      "        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,\n",
      "        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,\n",
      "        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,\n",
      "        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,\n",
      "        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "for batch_id, (pep_token_ids, label) in enumerate(train_dataloader):\n",
    "    print(batch_id)\n",
    "    print(pep_token_ids)\n",
    "    print(label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-21T00:22:20.310995Z",
     "start_time": "2021-12-21T00:22:20.294605Z"
    }
   },
   "outputs": [],
   "source": [
    "class ESMClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 esm,\n",
    "                 num_classes=1,\n",
    "                 params=None):\n",
    "        \n",
    "        super(ESMClassifier, self).__init__()\n",
    "        self.esm = esm\n",
    "        self.pep_lstm1 = nn.LSTM(input_size=1280, hidden_size=1, batch_first=True)        \n",
    "        self.ts_lstm1 = nn.LSTM(input_size=1280, hidden_size=1, batch_first=True)\n",
    "                \n",
    "        self.fc1 = nn.Linear(5, 1)\n",
    "\n",
    "\n",
    "#     def gen_attention_mask(self, token_ids, valid_length):\n",
    "#         attention_mask = torch.zeros_like(token_ids)\n",
    "#         for i, v in enumerate(valid_length):\n",
    "#             attention_mask[i][:v] = 1\n",
    "#         return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "#         attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        pep_dig_embed = self.esm(token_ids, repr_layers=[33])['representations'][33]\n",
    "        \n",
    "        pad_ids = torch.tensor([[0, 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2]]*len(token_ids)).long().to(device)\n",
    "        pad_vec = self.esm(pad_ids, repr_layers=[33])['representations'][33]  # 15mer\n",
    "        pad_vec = torch.stack([pad_vec[row_idx][1:-1] for row_idx in range(len(pad_vec))])\n",
    "        \n",
    "        pep_embed = torch.stack([pep_dig_embed[row_idx][8:-7] for row_idx in range(len(pep_dig_embed))])\n",
    "        en_embed = torch.stack([pep_dig_embed[row_idx][:15] for row_idx in range(len(pep_dig_embed))])\n",
    "        ec_embed = torch.stack([pep_dig_embed[row_idx][-15:] for row_idx in range(len(pep_dig_embed))])\n",
    "        \n",
    "        lpad = torch.tensor([0]*8).long().to(device)\n",
    "        rpad = torch.tensor([0]*8).long().to(device)\n",
    "        pep_ids = torch.stack([torch.cat([lpad, token_ids[row_idx][8:-8], rpad]) for row_idx in range(len(token_ids))])  # __KR에서 마지막 ts 제외한 ids.\n",
    "        e1_embed = []\n",
    "        e2_embed = []\n",
    "        for row_idx, pep_id in enumerate(pep_ids):\n",
    "            k_idx = (pep_id==15).nonzero(as_tuple=True)[0]\n",
    "            r_idx = (pep_id==10).nonzero(as_tuple=True)[0]\n",
    "            ts_idx = torch.sort(torch.cat([k_idx, r_idx],dim=0))[0]  # 0 : nterm, 1: m1, 2: m2, 3: cterm, if len(ts_idx)==4\n",
    "            \n",
    "            if len(ts_idx)==1:\n",
    "                e1 = pep_dig_embed[row_idx][ts_idx[0]-7:ts_idx[0]+8]  # 15mer (15*1280 shape)\n",
    "                e2 = pad_vec[row_idx]  # 15*1280\n",
    "                # e2_embed = torch.tensor([[0]*1280]*15).long().to(device)\n",
    "            elif len(ts_idx)==2:\n",
    "                e1 = pep_dig_embed[row_idx][ts_idx[0]-7:ts_idx[0]+8]\n",
    "                e2 = pep_dig_embed[row_idx][ts_idx[1]-7:ts_idx[1]+8]\n",
    "            else:\n",
    "                e1 = pad_vec[row_idx]\n",
    "                e2 = pad_vec[row_idx]\n",
    "            e1_embed.append(e1)\n",
    "            e2_embed.append(e2)\n",
    "            \n",
    "        e1_embed = torch.stack(e1_embed)\n",
    "        e2_embed = torch.stack(e2_embed)\n",
    "        \n",
    "        pep_lstm, (pep_hn, __) = self.pep_lstm1(pep_embed)\n",
    "        en_lstm, (en_hn, __) = self.ts_lstm1(en_embed)\n",
    "        ec_lstm, (ec_hn, __) = self.ts_lstm1(ec_embed)\n",
    "        e1_lstm, (e1_hn, __) = self.ts_lstm1(e1_embed)\n",
    "        e2_lstm, (e2_hn, __) = self.ts_lstm1(e2_embed)\n",
    "        \n",
    "        merge = torch.cat([pep_hn[0], en_hn[0], ec_hn[0], e1_hn[0], e2_hn[0]], dim=1)\n",
    "\n",
    "        merge = self.fc1(merge)        \n",
    "        out = torch.sigmoid(merge)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-21T00:22:23.758317Z",
     "start_time": "2021-12-21T00:22:20.312350Z"
    }
   },
   "outputs": [],
   "source": [
    "model = ESMClassifier(esm_model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-21T00:22:23.767221Z",
     "start_time": "2021-12-21T00:22:23.759780Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ESMClassifier(\n",
       "  (esm): ProteinBertModel(\n",
       "    (embed_tokens): Embedding(33, 1280, padding_idx=1)\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (12): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (13): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (14): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (15): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (16): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (17): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (18): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (19): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (20): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (21): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (22): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (23): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (24): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (25): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (26): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (27): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (28): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (29): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (30): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (31): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (32): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (contact_head): ContactPredictionHead(\n",
       "      (regression): Linear(in_features=660, out_features=1, bias=True)\n",
       "      (activation): Sigmoid()\n",
       "    )\n",
       "    (embed_positions): LearnedPositionalEmbedding(1026, 1280, padding_idx=1)\n",
       "    (emb_layer_norm_before): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    (emb_layer_norm_after): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    (lm_head): RobertaLMHead(\n",
       "      (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (pep_lstm1): LSTM(1280, 1, batch_first=True)\n",
       "  (ts_lstm1): LSTM(1280, 1, batch_first=True)\n",
       "  (fc1): Linear(in_features=5, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-21T00:22:23.792068Z",
     "start_time": "2021-12-21T00:22:23.768244Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+------------+\n",
      "|        Modules         | Parameters |\n",
      "+------------------------+------------+\n",
      "| pep_lstm1.weight_ih_l0 |    5120    |\n",
      "| pep_lstm1.weight_hh_l0 |     4      |\n",
      "|  pep_lstm1.bias_ih_l0  |     4      |\n",
      "|  pep_lstm1.bias_hh_l0  |     4      |\n",
      "| ts_lstm1.weight_ih_l0  |    5120    |\n",
      "| ts_lstm1.weight_hh_l0  |     4      |\n",
      "|  ts_lstm1.bias_ih_l0   |     4      |\n",
      "|  ts_lstm1.bias_hh_l0   |     4      |\n",
      "|       fc1.weight       |     5      |\n",
      "|        fc1.bias        |     1      |\n",
      "+------------------------+------------+\n",
      "Total Trainable Params: 10270\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10270"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        param = parameter.numel()\n",
    "        table.add_row([name, param])\n",
    "        total_params+=param\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-21T00:22:23.809164Z",
     "start_time": "2021-12-21T00:22:23.793112Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "# optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.BCELoss()\n",
    "# loss_fn = F.binary_cross_entropy()\n",
    "\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "# warmup_step = int(t_total * warmup_ratio)\n",
    "\n",
    "# scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps=warmup_step, t_total=t_total)\n",
    "\n",
    "def calc_accuracy(X,Y):\n",
    "    train_acc = ((X>0.5)==Y).sum().data.cpu().numpy() / len(Y)\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-23T02:51:19.513561Z",
     "start_time": "2021-12-21T00:43:15.735854Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 0.6745380163192749 train acc 0.65625 time 7.3\n",
      "epoch 1 batch id 201 loss 0.6721905469894409 train acc 0.5851990049751243 time 586.32\n",
      "epoch 1 batch id 401 loss 0.6573207974433899 train acc 0.5945293017456359 time 1180.67\n",
      "epoch 1 batch id 601 loss 0.6337117552757263 train acc 0.6046563019966722 time 1781.37\n",
      "epoch 1 batch id 801 loss 0.6262001991271973 train acc 0.611725577403246 time 2380.62\n",
      "epoch 1 batch id 1001 loss 0.6685328483581543 train acc 0.6163367882117882 time 2979.72\n",
      "epoch 1 batch id 1201 loss 0.6001158952713013 train acc 0.6222711542464613 time 3578.73\n",
      "epoch 1 batch id 1401 loss 0.6021649837493896 train acc 0.6285939730549608 time 4173.06\n",
      "epoch 1 batch id 1601 loss 0.6278054714202881 train acc 0.6342300710493441 time 4765.72\n",
      "epoch 1 batch id 1801 loss 0.6127115488052368 train acc 0.6391392802609661 time 5364.42\n",
      "epoch 1 batch id 2001 loss 0.6508327126502991 train acc 0.643426333708146 time 5963.24\n",
      "epoch 1 train acc 0.6458929727434759 time 6329.8\n",
      "best_acc: 0.6884957627118644\n",
      "epoch 1 test acc 0.6884957627118644\n",
      "epoch 2 batch id 1 loss 0.5778011083602905 train acc 0.69921875 time 7.45\n",
      "epoch 2 batch id 201 loss 0.5808925628662109 train acc 0.6936023009950248 time 606.07\n",
      "epoch 2 batch id 401 loss 0.5730077028274536 train acc 0.6934811408977556 time 1204.99\n",
      "epoch 2 batch id 601 loss 0.5427317023277283 train acc 0.6935641118968386 time 1803.53\n",
      "epoch 2 batch id 801 loss 0.5746861100196838 train acc 0.6947663077403246 time 2398.01\n",
      "epoch 2 batch id 1001 loss 0.607973039150238 train acc 0.6947583666333667 time 2981.78\n",
      "epoch 2 batch id 1201 loss 0.5801781415939331 train acc 0.6955466798501249 time 3581.69\n",
      "epoch 2 batch id 1401 loss 0.5720579624176025 train acc 0.6968264855460385 time 4180.21\n",
      "epoch 2 batch id 1601 loss 0.6068857908248901 train acc 0.6980232081511555 time 4766.33\n",
      "epoch 2 batch id 1801 loss 0.5837153196334839 train acc 0.6989563089950028 time 5358.83\n",
      "epoch 2 batch id 2001 loss 0.6313831210136414 train acc 0.6992929316591704 time 5958.62\n",
      "epoch 2 train acc 0.6998048976829432 time 6323.23\n",
      "best_acc: 0.7063265065913371\n",
      "epoch 2 test acc 0.7063265065913371\n",
      "epoch 3 batch id 1 loss 0.547822117805481 train acc 0.71875 time 7.45\n",
      "epoch 3 batch id 201 loss 0.5593545436859131 train acc 0.7110346703980099 time 605.72\n",
      "epoch 3 batch id 401 loss 0.5526950359344482 train acc 0.7091256234413965 time 1204.96\n",
      "epoch 3 batch id 601 loss 0.5225224494934082 train acc 0.7091176164725458 time 1798.43\n",
      "epoch 3 batch id 801 loss 0.5694321393966675 train acc 0.7096451700998752 time 2399.69\n",
      "epoch 3 batch id 1001 loss 0.5942665934562683 train acc 0.7086546266233766 time 2988.92\n",
      "epoch 3 batch id 1201 loss 0.5709673166275024 train acc 0.7086184689841799 time 3581.91\n",
      "epoch 3 batch id 1401 loss 0.5676500797271729 train acc 0.7093593861527481 time 4173.63\n",
      "epoch 3 batch id 1601 loss 0.5956230163574219 train acc 0.7100274242660837 time 5644.29\n",
      "epoch 3 batch id 1801 loss 0.5715867280960083 train acc 0.7104516588006663 time 7262.88\n",
      "epoch 3 batch id 2001 loss 0.6165424585342407 train acc 0.7103225730884558 time 8880.76\n",
      "epoch 3 train acc 0.7105807144538606 time 9870.2\n",
      "best_acc: 0.7136408309792843\n",
      "epoch 3 test acc 0.7136408309792843\n",
      "epoch 4 batch id 1 loss 0.5335648059844971 train acc 0.74609375 time 12.79\n",
      "epoch 4 batch id 201 loss 0.5493443012237549 train acc 0.7165539490049752 time 1635.66\n",
      "epoch 4 batch id 401 loss 0.5496439933776855 train acc 0.7159639962593516 time 3257.68\n",
      "epoch 4 batch id 601 loss 0.5185779333114624 train acc 0.7156561980033278 time 4879.16\n",
      "epoch 4 batch id 801 loss 0.5682891607284546 train acc 0.7158044631710362 time 6497.95\n",
      "epoch 4 batch id 1001 loss 0.5883808135986328 train acc 0.7146369255744256 time 8111.25\n",
      "epoch 4 batch id 1201 loss 0.5646350383758545 train acc 0.7146128226477935 time 9727.8\n",
      "epoch 4 batch id 1401 loss 0.5657011270523071 train acc 0.7150166175945754 time 11344.12\n",
      "epoch 4 batch id 1601 loss 0.5839239954948425 train acc 0.7156025530918176 time 12972.56\n",
      "epoch 4 batch id 1801 loss 0.5592299103736877 train acc 0.7160128053858967 time 14592.21\n",
      "epoch 4 batch id 2001 loss 0.6041868925094604 train acc 0.715733929910045 time 16213.13\n",
      "epoch 4 train acc 0.7159579823278182 time 17204.09\n",
      "best_acc: 0.7169470927495292\n",
      "epoch 4 test acc 0.7169470927495292\n",
      "epoch 5 batch id 1 loss 0.5234698057174683 train acc 0.75 time 12.67\n",
      "epoch 5 batch id 201 loss 0.5465553998947144 train acc 0.7195856654228856 time 1638.11\n",
      "epoch 5 batch id 401 loss 0.5509740710258484 train acc 0.7188766365336658 time 3263.17\n",
      "epoch 5 batch id 601 loss 0.5171079635620117 train acc 0.719198471297837 time 4885.32\n",
      "epoch 5 batch id 801 loss 0.5697458982467651 train acc 0.7193010689762797 time 6512.06\n",
      "epoch 5 batch id 1001 loss 0.5869258642196655 train acc 0.7184690309690309 time 8141.29\n",
      "epoch 5 batch id 1201 loss 0.5611231923103333 train acc 0.718304407785179 time 9765.52\n",
      "epoch 5 batch id 1401 loss 0.560610294342041 train acc 0.7187611527480371 time 11384.57\n",
      "epoch 5 batch id 1601 loss 0.5742831230163574 train acc 0.7193209322298564 time 13021.15\n",
      "epoch 5 batch id 1801 loss 0.5482611656188965 train acc 0.7196457697112715 time 14647.79\n",
      "epoch 5 batch id 2001 loss 0.5968235731124878 train acc 0.7193219796351824 time 16274.98\n",
      "epoch 5 train acc 0.7195610434322034 time 17265.97\n",
      "best_acc: 0.7190142419962335\n",
      "epoch 5 test acc 0.7190142419962335\n",
      "epoch 6 batch id 1 loss 0.5179210901260376 train acc 0.76953125 time 12.7\n",
      "epoch 6 batch id 201 loss 0.5407000780105591 train acc 0.7231032338308457 time 1641.74\n",
      "epoch 6 batch id 401 loss 0.5491803884506226 train acc 0.7217113466334164 time 3268.35\n",
      "epoch 6 batch id 601 loss 0.5170282125473022 train acc 0.7222207778702163 time 4892.31\n",
      "epoch 6 batch id 801 loss 0.5707476735115051 train acc 0.722397784019975 time 6516.85\n",
      "epoch 6 batch id 1001 loss 0.583047091960907 train acc 0.7214972527472527 time 8160.01\n",
      "epoch 6 batch id 1201 loss 0.5583803653717041 train acc 0.7212934533721899 time 9776.2\n",
      "epoch 6 batch id 1401 loss 0.5582610368728638 train acc 0.7216608672376874 time 11392.72\n",
      "epoch 6 batch id 1601 loss 0.5684247016906738 train acc 0.7220975171767645 time 13020.18\n",
      "epoch 6 batch id 1801 loss 0.5401952862739563 train acc 0.7223569371182677 time 14639.31\n",
      "epoch 6 batch id 2001 loss 0.5918696522712708 train acc 0.7219827586206896 time 16257.95\n",
      "epoch 6 train acc 0.7221302671845574 time 17247.41\n",
      "best_acc: 0.7210793314500942\n",
      "epoch 6 test acc 0.7210793314500942\n",
      "epoch 7 batch id 1 loss 0.5135132074356079 train acc 0.76953125 time 12.74\n",
      "epoch 7 batch id 201 loss 0.5356644988059998 train acc 0.7256490982587065 time 1634.96\n",
      "epoch 7 batch id 401 loss 0.5461812019348145 train acc 0.724088216957606 time 3253.35\n",
      "epoch 7 batch id 601 loss 0.5176787376403809 train acc 0.7244436356073212 time 4872.0\n",
      "epoch 7 batch id 801 loss 0.5709844827651978 train acc 0.724489895443196 time 6492.07\n",
      "epoch 7 batch id 1001 loss 0.581692636013031 train acc 0.7236747627372627 time 8107.57\n",
      "epoch 7 batch id 1201 loss 0.559180498123169 train acc 0.7234856369691923 time 9723.18\n",
      "epoch 7 batch id 1401 loss 0.5590878129005432 train acc 0.7238300767309065 time 11336.13\n",
      "epoch 7 batch id 1601 loss 0.5641361474990845 train acc 0.7242885306058713 time 12969.37\n",
      "epoch 7 batch id 1801 loss 0.5331904888153076 train acc 0.7245388846474181 time 14587.86\n",
      "epoch 7 batch id 2001 loss 0.5882790088653564 train acc 0.7241125530984508 time 16207.25\n",
      "epoch 7 train acc 0.7242875323681732 time 17196.19\n",
      "best_acc: 0.7230508474576272\n",
      "epoch 7 test acc 0.7230508474576272\n",
      "epoch 8 batch id 1 loss 0.5083020925521851 train acc 0.7734375 time 12.67\n",
      "epoch 8 batch id 201 loss 0.5308178663253784 train acc 0.7269900497512438 time 1648.05\n",
      "epoch 8 batch id 401 loss 0.5418986082077026 train acc 0.7251110504987531 time 3267.95\n",
      "epoch 8 batch id 601 loss 0.5171837210655212 train acc 0.7255875623960066 time 4889.53\n",
      "epoch 8 batch id 801 loss 0.5708804130554199 train acc 0.7256700608614233 time 6508.82\n",
      "epoch 8 batch id 1001 loss 0.5804886817932129 train acc 0.7249742445054945 time 8121.28\n",
      "epoch 8 batch id 1201 loss 0.5557249188423157 train acc 0.7248744535803497 time 9733.92\n",
      "epoch 8 batch id 1401 loss 0.559886634349823 train acc 0.7251293718772306 time 11348.21\n",
      "epoch 8 batch id 1601 loss 0.5598506331443787 train acc 0.7255743480637101 time 12983.36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 batch id 1801 loss 0.5259740352630615 train acc 0.7258445828706275 time 14599.82\n",
      "epoch 8 batch id 2001 loss 0.5849611759185791 train acc 0.725414636431784 time 16220.93\n",
      "epoch 8 train acc 0.7256116849105462 time 17210.5\n",
      "best_acc: 0.7243402777777778\n",
      "epoch 8 test acc 0.7243402777777778\n",
      "epoch 9 batch id 1 loss 0.5039335489273071 train acc 0.77734375 time 12.73\n",
      "epoch 9 batch id 201 loss 0.5269186496734619 train acc 0.7282143967661692 time 1646.51\n",
      "epoch 9 batch id 401 loss 0.5371017456054688 train acc 0.7269034445137157 time 3261.51\n",
      "epoch 9 batch id 601 loss 0.514401912689209 train acc 0.7272839538269551 time 4880.08\n",
      "epoch 9 batch id 801 loss 0.5709639191627502 train acc 0.7274256788389513 time 6500.87\n",
      "epoch 9 batch id 1001 loss 0.5772814750671387 train acc 0.7267068868631369 time 8128.12\n",
      "epoch 9 batch id 1201 loss 0.5546128749847412 train acc 0.7266373074521232 time 9742.69\n",
      "epoch 9 batch id 1401 loss 0.5579428672790527 train acc 0.7269138115631691 time 11355.86\n",
      "epoch 9 batch id 1601 loss 0.5567575693130493 train acc 0.7273871798875703 time 12989.25\n",
      "epoch 9 batch id 1801 loss 0.518964409828186 train acc 0.7276643184342032 time 14608.51\n",
      "epoch 9 batch id 2001 loss 0.5825644731521606 train acc 0.7272418478260869 time 16229.71\n",
      "epoch 9 train acc 0.7273864171374764 time 17216.98\n",
      "best_acc: 0.725291313559322\n",
      "epoch 9 test acc 0.725291313559322\n",
      "epoch 10 batch id 1 loss 0.5004022121429443 train acc 0.78125 time 12.76\n",
      "epoch 10 batch id 201 loss 0.5237337946891785 train acc 0.7301772388059702 time 1646.73\n",
      "epoch 10 batch id 401 loss 0.533239483833313 train acc 0.7284523067331671 time 3262.78\n",
      "epoch 10 batch id 601 loss 0.5115940570831299 train acc 0.7286683652246256 time 4880.15\n",
      "epoch 10 batch id 801 loss 0.5718696117401123 train acc 0.7286399812734082 time 6495.94\n",
      "epoch 10 batch id 1001 loss 0.5745631456375122 train acc 0.7278346653346653 time 8121.85\n",
      "epoch 10 batch id 1201 loss 0.5524265170097351 train acc 0.7278082067027477 time 9735.16\n",
      "epoch 10 batch id 1401 loss 0.5561655163764954 train acc 0.7281099437901499 time 11350.7\n",
      "epoch 10 batch id 1601 loss 0.5537481307983398 train acc 0.728514405059338 time 12998.44\n",
      "epoch 10 batch id 1801 loss 0.5128175020217896 train acc 0.7288724146307607 time 14626.69\n",
      "epoch 10 batch id 2001 loss 0.5800024271011353 train acc 0.7284404672663668 time 16242.39\n",
      "epoch 10 train acc 0.7285505679143126 time 17229.02\n",
      "best_acc: 0.7261005178907721\n",
      "epoch 10 test acc 0.7261005178907721\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "for e in range(num_epochs):\n",
    "    t0 = time.time()\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    for batch_id, (pep_token_ids, label) in enumerate(train_dataloader):\n",
    "#         print(batch_id, round(time.time()-t0, 2))  # batch256->2100 loop, each 5 sec -> per 1 epoch, 3h\n",
    "        \n",
    "        pep_token_ids = pep_token_ids.long().to(device)\n",
    "        label = torch.reshape(label.float(), (-1, 1)).to(device)\n",
    "        \n",
    "        pred = model(pep_token_ids)\n",
    "        loss = F.binary_cross_entropy(pred, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_acc += calc_accuracy(pred, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {} time {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1), round(time.time()-t0, 2)))\n",
    "        \n",
    "    print(\"epoch {} train acc {} time {}\".format(e+1, train_acc / (batch_id+1), round(time.time()-t0,2)))\n",
    "    \n",
    "    model.eval()\n",
    "    for batch_id, (pep_token_ids, label) in enumerate(valid_dataloader):\n",
    "        pep_token_ids = pep_token_ids.long().to(device)\n",
    "        label = label.long().to(device)\n",
    "        label = torch.reshape(label, (-1, 1))\n",
    "        pred = model(pep_token_ids)\n",
    "        \n",
    "        test_acc += calc_accuracy(pred, label)\n",
    "    test_acc = test_acc / (batch_id+1)\n",
    "    if test_acc > best_acc:\n",
    "        best_acc=test_acc\n",
    "        print(f\"best_acc: {best_acc}\")\n",
    "        torch.save({\"best_acc\":best_acc, \"model\":model.state_dict()},f'./finetuning_211221.pl')\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 211220 - pep / ts seperate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T06:30:25.652141Z",
     "start_time": "2021-12-06T03:03:00.038592Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 0.7282492518424988 train acc 0.5078125 time 10.04\n",
      "epoch 1 batch id 201 loss 0.6480684280395508 train acc 0.4993198072139303 time 2702.1\n",
      "epoch 1 batch id 401 loss 0.6338706016540527 train acc 0.5020554083541147 time 5607.34\n",
      "epoch 1 batch id 601 loss 0.5915693044662476 train acc 0.5678556572379367 time 8516.95\n",
      "epoch 1 batch id 801 loss 0.6123077869415283 train acc 0.6009968008739076 time 11428.16\n",
      "epoch 1 batch id 1001 loss 0.6052110195159912 train acc 0.6203171828171828 time 14324.08\n",
      "epoch 1 batch id 1201 loss 0.5544191598892212 train acc 0.6339085917985012 time 17219.76\n",
      "epoch 1 batch id 1401 loss 0.5964603424072266 train acc 0.6426603765167738 time 20251.29\n",
      "epoch 1 batch id 1601 loss 0.5929796695709229 train acc 0.6501771353841349 time 23284.44\n",
      "epoch 1 batch id 1801 loss 0.5663135051727295 train acc 0.6554800284564131 time 26316.99\n",
      "epoch 1 batch id 2001 loss 0.6105095148086548 train acc 0.6596974950024987 time 29351.44\n",
      "epoch 1 train acc 0.6622013300376648 time 31204.96\n",
      "best_acc: 371.8140625\n",
      "epoch 1 test acc 0.7002148069679849\n",
      "epoch 2 batch id 1 loss 0.5747990608215332 train acc 0.7265625 time 18.23\n",
      "epoch 2 batch id 201 loss 0.5343297123908997 train acc 0.7193330223880597 time 3043.62\n",
      "epoch 2 batch id 401 loss 0.5578093528747559 train acc 0.7247701059850374 time 6087.21\n",
      "epoch 2 batch id 601 loss 0.5317457914352417 train acc 0.7282848897670549 time 9129.95\n",
      "epoch 2 batch id 801 loss 0.5568350553512573 train acc 0.7302444210362048 time 12173.35\n",
      "epoch 2 batch id 1001 loss 0.5581809878349304 train acc 0.7308980082417582 time 15215.52\n",
      "epoch 2 batch id 1201 loss 0.496437668800354 train acc 0.7323877237718568 time 18258.31\n",
      "epoch 2 batch id 1401 loss 0.5790587067604065 train acc 0.7331816559600286 time 21298.57\n",
      "epoch 2 batch id 1601 loss 0.5430903434753418 train acc 0.7339431410056215 time 24338.98\n",
      "epoch 2 batch id 1801 loss 0.5249117016792297 train acc 0.7346092448639645 time 27380.54\n",
      "epoch 2 batch id 2001 loss 0.5879701375961304 train acc 0.7352378498250874 time 30420.62\n",
      "epoch 2 train acc 0.7357601055959107 time 32278.37\n",
      "best_acc: 394.23375\n",
      "epoch 2 test acc 0.7424364406779661\n",
      "epoch 3 batch id 1 loss 0.5339414477348328 train acc 0.74609375 time 18.3\n",
      "epoch 3 batch id 201 loss 0.48907679319381714 train acc 0.7430037313432836 time 3001.71\n",
      "epoch 3 batch id 401 loss 0.5288266539573669 train acc 0.742771976309227 time 5942.53\n",
      "epoch 3 batch id 601 loss 0.5161395072937012 train acc 0.7436824043261231 time 8750.04\n",
      "epoch 3 batch id 801 loss 0.5372279286384583 train acc 0.7441479400749064 time 11557.52\n",
      "epoch 3 batch id 1001 loss 0.5374903678894043 train acc 0.7435494193306693 time 14364.62\n",
      "epoch 3 batch id 1201 loss 0.48433393239974976 train acc 0.74401865632806 time 17171.96\n",
      "epoch 3 batch id 1401 loss 0.574160099029541 train acc 0.7441894182726624 time 19978.77\n",
      "epoch 3 batch id 1601 loss 0.5154938101768494 train acc 0.7443955925983761 time 22785.17\n",
      "epoch 3 batch id 1801 loss 0.5140821933746338 train acc 0.744592847723487 time 25590.35\n",
      "epoch 3 batch id 2001 loss 0.5718081593513489 train acc 0.7447272457521239 time 28395.64\n",
      "epoch 3 train acc 0.7450328515940274 time 30108.58\n",
      "best_acc: 396.7159375\n",
      "epoch 3 test acc 0.7471109934086629\n",
      "epoch 4 batch id 1 loss 0.5111203193664551 train acc 0.7421875 time 17.09\n",
      "epoch 4 batch id 201 loss 0.4822709858417511 train acc 0.7491837686567164 time 2815.91\n",
      "epoch 4 batch id 401 loss 0.5130435824394226 train acc 0.7482563123441397 time 5621.21\n",
      "epoch 4 batch id 601 loss 0.5033142566680908 train acc 0.7485505927620633 time 8424.82\n",
      "epoch 4 batch id 801 loss 0.5294045209884644 train acc 0.7491368211610487 time 11228.28\n",
      "epoch 4 batch id 1001 loss 0.5225324630737305 train acc 0.7482244318181818 time 14030.9\n",
      "epoch 4 batch id 1201 loss 0.47982269525527954 train acc 0.748500598459617 time 16834.03\n",
      "epoch 4 batch id 1401 loss 0.5683783292770386 train acc 0.7485278372591007 time 19636.96\n",
      "epoch 4 batch id 1601 loss 0.5044275522232056 train acc 0.7486117075265459 time 22439.69\n",
      "epoch 4 batch id 1801 loss 0.5091903209686279 train acc 0.748676950305386 time 25242.66\n",
      "epoch 4 batch id 2001 loss 0.5568742752075195 train acc 0.7488423756871564 time 28046.59\n",
      "epoch 4 train acc 0.7490108265234059 time 29758.8\n",
      "best_acc: 398.05359375\n",
      "epoch 4 test acc 0.7496301200564972\n",
      "epoch 5 batch id 1 loss 0.49403494596481323 train acc 0.74609375 time 16.98\n",
      "epoch 5 batch id 201 loss 0.47472959756851196 train acc 0.7514575559701493 time 2817.17\n",
      "epoch 5 batch id 401 loss 0.5061918497085571 train acc 0.7506916302992519 time 5616.8\n",
      "epoch 5 batch id 601 loss 0.49928051233291626 train acc 0.7511829242928453 time 8420.48\n",
      "epoch 5 batch id 801 loss 0.5246136784553528 train acc 0.7514630149812734 time 11223.0\n",
      "epoch 5 batch id 1001 loss 0.5132496953010559 train acc 0.7505463286713286 time 14026.75\n",
      "epoch 5 batch id 1201 loss 0.47824805974960327 train acc 0.7507773470024979 time 16829.9\n",
      "epoch 5 batch id 1401 loss 0.565491795539856 train acc 0.7507137758743755 time 19633.23\n",
      "epoch 5 batch id 1601 loss 0.499496191740036 train acc 0.7508417590568395 time 22434.91\n",
      "epoch 5 batch id 1801 loss 0.5003695487976074 train acc 0.7508025055524709 time 25235.82\n",
      "epoch 5 batch id 2001 loss 0.5492379069328308 train acc 0.7508608976761619 time 28036.98\n",
      "epoch 5 train acc 0.7509860207324455 time 29748.04\n",
      "best_acc: 398.77515625\n",
      "epoch 5 test acc 0.7509889948210923\n",
      "epoch 6 batch id 1 loss 0.48362159729003906 train acc 0.7578125 time 17.04\n",
      "epoch 6 batch id 201 loss 0.4704952836036682 train acc 0.7529734141791045 time 2813.48\n",
      "epoch 6 batch id 401 loss 0.5041213631629944 train acc 0.7524548004987531 time 5602.05\n",
      "epoch 6 batch id 601 loss 0.49752095341682434 train acc 0.7528598169717138 time 8391.85\n",
      "epoch 6 batch id 801 loss 0.5198589563369751 train acc 0.7531357287765293 time 11183.62\n",
      "epoch 6 batch id 1001 loss 0.5073162317276001 train acc 0.7522009240759241 time 13975.72\n",
      "epoch 6 batch id 1201 loss 0.47733911871910095 train acc 0.7523613134887593 time 16763.59\n",
      "epoch 6 batch id 1401 loss 0.5639556646347046 train acc 0.7523281361527481 time 19565.54\n",
      "epoch 6 batch id 1601 loss 0.49552541971206665 train acc 0.7523715646470955 time 22367.83\n",
      "epoch 6 batch id 1801 loss 0.4927540123462677 train acc 0.7523424486396446 time 25171.95\n",
      "epoch 6 batch id 2001 loss 0.5441861152648926 train acc 0.7524050474762619 time 27976.95\n",
      "epoch 6 train acc 0.7525143134584342 time 29689.83\n",
      "best_acc: 399.2165625\n",
      "epoch 6 test acc 0.7518202683615819\n",
      "epoch 7 batch id 1 loss 0.47732722759246826 train acc 0.75 time 17.01\n",
      "epoch 7 batch id 201 loss 0.4678184986114502 train acc 0.7542949315920398 time 2819.2\n",
      "epoch 7 batch id 401 loss 0.5033025145530701 train acc 0.753623753117207 time 5620.71\n",
      "epoch 7 batch id 601 loss 0.4960003197193146 train acc 0.7540687396006656 time 8422.97\n",
      "epoch 7 batch id 801 loss 0.5161111354827881 train acc 0.7543597846441947 time 11224.74\n",
      "epoch 7 batch id 1001 loss 0.503078281879425 train acc 0.7535004058441559 time 14025.69\n",
      "epoch 7 batch id 1201 loss 0.4756820499897003 train acc 0.753558232722731 time 16671.65\n",
      "epoch 7 batch id 1401 loss 0.5630097389221191 train acc 0.7534657164525339 time 19466.69\n",
      "epoch 7 batch id 1601 loss 0.4921615719795227 train acc 0.7534914701748907 time 22248.59\n",
      "epoch 7 batch id 1801 loss 0.4872135519981384 train acc 0.7534399292059967 time 25062.47\n",
      "epoch 7 batch id 2001 loss 0.5403791666030884 train acc 0.7534728729385307 time 27843.3\n",
      "epoch 7 train acc 0.7536122566081517 time 29544.08\n",
      "best_acc: 399.63453125\n",
      "epoch 7 test acc 0.7526074034839925\n",
      "epoch 8 batch id 1 loss 0.4731598198413849 train acc 0.75 time 16.88\n",
      "epoch 8 batch id 201 loss 0.4659420847892761 train acc 0.7551888992537313 time 2803.55\n",
      "epoch 8 batch id 401 loss 0.5027176141738892 train acc 0.7545881390274314 time 5590.2\n",
      "epoch 8 batch id 601 loss 0.49450862407684326 train acc 0.7549331842762064 time 8379.86\n",
      "epoch 8 batch id 801 loss 0.5130789875984192 train acc 0.755242470349563 time 11169.38\n",
      "epoch 8 batch id 1001 loss 0.49935322999954224 train acc 0.7543589223276723 time 13945.49\n",
      "epoch 8 batch id 1201 loss 0.4736998379230499 train acc 0.7544624271440467 time 16718.67\n",
      "epoch 8 batch id 1401 loss 0.5625313520431519 train acc 0.7542603497501784 time 19492.08\n",
      "epoch 8 batch id 1601 loss 0.4889988899230957 train acc 0.7543698274515928 time 22264.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 batch id 1801 loss 0.48304346203804016 train acc 0.7543465435868961 time 25036.29\n",
      "epoch 8 batch id 2001 loss 0.5375452041625977 train acc 0.7543611006996501 time 27808.03\n",
      "epoch 8 train acc 0.754491346768227 time 29500.04\n",
      "best_acc: 400.52453125\n",
      "epoch 8 test acc 0.754283486346516\n",
      "epoch 9 batch id 1 loss 0.4690837264060974 train acc 0.75390625 time 14.26\n",
      "epoch 9 batch id 201 loss 0.4643460214138031 train acc 0.755733053482587 time 2164.47\n",
      "epoch 9 batch id 401 loss 0.5001388788223267 train acc 0.7550557200748129 time 4316.5\n",
      "epoch 9 batch id 601 loss 0.4926818609237671 train acc 0.7554466514143094 time 6466.44\n",
      "epoch 9 batch id 801 loss 0.5099382996559143 train acc 0.7557837858926342 time 8617.96\n",
      "epoch 9 batch id 1001 loss 0.49767738580703735 train acc 0.7550496378621379 time 10769.21\n",
      "epoch 9 batch id 1201 loss 0.4714639186859131 train acc 0.7552820566194838 time 12920.19\n",
      "epoch 9 batch id 1401 loss 0.5621832609176636 train acc 0.7552334270164168 time 15071.85\n",
      "epoch 9 batch id 1601 loss 0.48631829023361206 train acc 0.7554287359462836 time 17223.7\n",
      "epoch 9 batch id 1801 loss 0.47914034128189087 train acc 0.7554353484175458 time 19376.03\n",
      "epoch 9 batch id 2001 loss 0.5353666543960571 train acc 0.7556592797351325 time 21528.49\n",
      "epoch 9 train acc 0.7558651550309389 time 22843.51\n",
      "best_acc: 401.57640625\n",
      "epoch 9 test acc 0.7562644185499058\n",
      "epoch 10 batch id 1 loss 0.4672521948814392 train acc 0.765625 time 14.24\n",
      "epoch 10 batch id 201 loss 0.46161115169525146 train acc 0.759717039800995 time 2165.62\n",
      "epoch 10 batch id 401 loss 0.4989391267299652 train acc 0.7589230049875312 time 4317.72\n",
      "epoch 10 batch id 601 loss 0.4916364550590515 train acc 0.7592879055740432 time 6468.6\n",
      "epoch 10 batch id 801 loss 0.5075975060462952 train acc 0.7595144740948814 time 8621.21\n",
      "epoch 10 batch id 1001 loss 0.4967806041240692 train acc 0.758530531968032 time 10773.44\n",
      "epoch 10 batch id 1201 loss 0.4680538773536682 train acc 0.75885004683597 time 12925.72\n",
      "epoch 10 batch id 1401 loss 0.5593549013137817 train acc 0.7587967300142755 time 15077.54\n",
      "epoch 10 batch id 1601 loss 0.48438572883605957 train acc 0.7588592090880699 time 17229.96\n",
      "epoch 10 batch id 1801 loss 0.4763858914375305 train acc 0.7588123785397002 time 19382.28\n",
      "epoch 10 batch id 2001 loss 0.532214343547821 train acc 0.7590833489505248 time 21533.86\n",
      "epoch 10 train acc 0.7592086402004304 time 22848.21\n",
      "best_acc: 402.801875\n",
      "epoch 10 test acc 0.7585722693032015\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "for e in range(num_epochs):\n",
    "    t0 = time.time()\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    for batch_id, (pep_token_ids, label) in enumerate(train_dataloader):\n",
    "#         print(batch_id, round(time.time()-t0, 2))  # batch256->2100 loop, each 5 sec -> per 1 epoch, 3h\n",
    "        \n",
    "        pep_token_ids = pep_token_ids.long().to(device)\n",
    "        label = torch.reshape(label.float(), (-1, 1)).to(device)\n",
    "        \n",
    "        pred = model(pep_token_ids)\n",
    "        loss = F.binary_cross_entropy(pred, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_acc += calc_accuracy(pred, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {} time {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1), round(time.time()-t0, 2)))\n",
    "        \n",
    "    print(\"epoch {} train acc {} time {}\".format(e+1, train_acc / (batch_id+1), round(time.time()-t0,2)))\n",
    "    \n",
    "    model.eval()\n",
    "    for batch_id, (pep_token_ids, label) in enumerate(valid_dataloader):\n",
    "        pep_token_ids = pep_token_ids.long().to(device)\n",
    "        label = label.long().to(device)\n",
    "        label = torch.reshape(label, (-1, 1))\n",
    "        pred = model(pep_token_ids)\n",
    "        \n",
    "        test_acc += calc_accuracy(pred, label)\n",
    "    if test_acc > best_acc:\n",
    "        best_acc=test_acc\n",
    "        torch.save({\"best_acc\":best_acc / (batch_id+1),\"model\":model.state_dict()},f'./finetuning.pl')\n",
    "        print(f\"best_acc: {best_acc}\")\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ProtEmbedding",
   "language": "python",
   "name": "protembedding"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
