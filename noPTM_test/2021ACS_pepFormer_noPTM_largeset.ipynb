{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T14:43:57.461822Z",
     "start_time": "2021-10-04T14:43:56.537881Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T14:42:55.782605Z",
     "start_time": "2021-10-04T14:42:54.810452Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import time\n",
    "import pickle\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T14:42:55.792230Z",
     "start_time": "2021-10-04T14:42:55.784731Z"
    }
   },
   "outputs": [],
   "source": [
    "def genData(file,max_len):\n",
    "    aa_dict={'A':1,'R':2,'N':3,'D':4,'C':5,'Q':6,'E':7,'G':8,'H':9,'I':10,\n",
    "             'L':11,'K':12,'M':13,'F':14,'P':15,'O':16,'S':17,'U':18,'T':19,\n",
    "             'W':20,'Y':21,'V':22,'X':23}\n",
    "    with open(file, 'r') as inf:\n",
    "        lines = inf.read().splitlines()\n",
    "        \n",
    "    long_pep_counter=0\n",
    "    pep_codes=[]\n",
    "    labels=[]\n",
    "    for pep in lines:\n",
    "        pep,label=pep.split(\",\")\n",
    "        labels.append(int(label))\n",
    "        if not len(pep) > max_len:\n",
    "            current_pep=[]\n",
    "            for aa in pep:\n",
    "                current_pep.append(aa_dict[aa])\n",
    "            pep_codes.append(torch.tensor(current_pep))\n",
    "        else:\n",
    "            long_pep_counter += 1\n",
    "    print(\"length > {}:\".format(max_len),long_pep_counter)\n",
    "    data = rnn_utils.pad_sequence(pep_codes, batch_first=True)  # padding\n",
    "    return data,torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T14:42:58.895293Z",
     "start_time": "2021-10-04T14:42:58.890982Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net):\n",
    "    acc_sum, n = 0.0, 0\n",
    "    for x, y in data_iter:\n",
    "        x,y=x.to(device),y.to(device)\n",
    "        outputs=net.trainModel(x)\n",
    "        acc_sum += (outputs.argmax(dim=1) == y).float().sum().item()\n",
    "        n += y.shape[0]\n",
    "    return acc_sum / n\n",
    "\n",
    "def to_log(log):\n",
    "    with open(\"../compareModel/2021ACS_PepFormer/modelLog.log\",\"a+\") as f:\n",
    "        f.write(log+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T14:43:00.064629Z",
     "start_time": "2021-10-04T14:43:00.058463Z"
    }
   },
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    seq1_ls=[]\n",
    "    seq2_ls=[]\n",
    "    label1_ls=[]\n",
    "    label2_ls=[]\n",
    "    label_ls=[]\n",
    "    batch_size=len(batch)\n",
    "    for i in range(int(batch_size/2)):\n",
    "        seq1,label1=batch[i][0],batch[i][1]\n",
    "        seq2,label2=batch[i+int(batch_size/2)][0],batch[i+int(batch_size/2)][1]\n",
    "        label1_ls.append(label1.unsqueeze(0))\n",
    "        label2_ls.append(label2.unsqueeze(0))\n",
    "        label=(label1^label2)\n",
    "        seq1_ls.append(seq1.unsqueeze(0))\n",
    "        seq2_ls.append(seq2.unsqueeze(0))\n",
    "        label_ls.append(label.unsqueeze(0))\n",
    "    seq1=torch.cat(seq1_ls).to(device)\n",
    "    seq2=torch.cat(seq2_ls).to(device)\n",
    "    label=torch.cat(label_ls).to(device)\n",
    "    label1=torch.cat(label1_ls).to(device)\n",
    "    label2=torch.cat(label2_ls).to(device)\n",
    "    return seq1,seq2,label,label1,label2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Make Data X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T14:44:03.018659Z",
     "start_time": "2021-10-04T14:44:00.150165Z"
    }
   },
   "outputs": [],
   "source": [
    "df_detect_peptide_train = pd.read_csv('../data/df_detect_peptide_train_noptm_210930_includeDigest.csv')\n",
    "df_detect_peptide_test = pd.read_csv('../data/df_detect_peptide_test_noptm_210930_includeDigest.csv')\n",
    "\n",
    "tra, val = train_test_split(df_detect_peptide_train[['PEP', 'ID']], test_size=0.2, random_state=7)\n",
    "tra.to_csv('../compareModel/2021ACS_PepFormer/detect_peptide_train_noptm_includeDigest.csv', header=False, index=False)\n",
    "val.to_csv('../compareModel/2021ACS_PepFormer/detect_peptide_val_noptm_includeDigest.csv', header=False, index=False)\n",
    "df_detect_peptide_test[['PEP', 'ID']].to_csv('../compareModel/2021ACS_PepFormer/detect_peptide_test_noptm_includeDigest.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T14:44:32.872959Z",
     "start_time": "2021-10-04T14:44:21.135593Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length > 81: 0\n",
      "torch.Size([337179, 30]) torch.Size([337179])\n",
      "length > 81: 0\n",
      "torch.Size([84295, 30]) torch.Size([84295])\n",
      "length > 81: 0\n",
      "torch.Size([88998, 30]) torch.Size([88998])\n"
     ]
    }
   ],
   "source": [
    "train_data,train_label=genData(\"../compareModel/2021ACS_PepFormer/detect_peptide_train_noptm_includeDigest.csv\",81)\n",
    "print(train_data.shape, train_label.shape)\n",
    "train_dataset = Data.TensorDataset(train_data, train_label)\n",
    "\n",
    "val_data,val_label=genData(\"../compareModel/2021ACS_PepFormer/detect_peptide_val_noptm_includeDigest.csv\",81)\n",
    "print(val_data.shape, val_label.shape)\n",
    "val_dataset = Data.TensorDataset(val_data, val_label)\n",
    "\n",
    "test_data,test_label=genData(\"../compareModel/2021ACS_PepFormer/detect_peptide_test_noptm_includeDigest.csv\",81)\n",
    "print(test_data.shape, test_label.shape)\n",
    "test_dataset = Data.TensorDataset(test_data, test_label)\n",
    "\n",
    "batch_size=256\n",
    "train_iter = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "train_iter_cont = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \n",
    "                                                                  shuffle=True, collate_fn=collate)\n",
    "val_iter = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_iter = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T14:44:37.734471Z",
     "start_time": "2021-10-04T14:44:37.721651Z"
    }
   },
   "outputs": [],
   "source": [
    "class newModel(nn.Module):\n",
    "    def __init__(self, vocab_size=24):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = 25\n",
    "        self.batch_size = 256\n",
    "        self.emb_dim = 512\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, self.emb_dim, padding_idx=0)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=1)\n",
    "        \n",
    "        self.gru = nn.GRU(self.emb_dim, self.hidden_dim, num_layers=2, \n",
    "                               bidirectional=True, dropout=0.2)\n",
    "        \n",
    "        \n",
    "        self.block1=nn.Sequential(nn.Linear(1600,1024),\n",
    "                                            nn.BatchNorm1d(1024),\n",
    "                                            nn.LeakyReLU(),\n",
    "                                            nn.Linear(1024,256),\n",
    "                                 )\n",
    "\n",
    "        self.block2=nn.Sequential(\n",
    "                                               nn.BatchNorm1d(256),\n",
    "                                               nn.LeakyReLU(),\n",
    "                                               nn.Linear(256,128),\n",
    "                                               nn.BatchNorm1d(128),\n",
    "                                               nn.LeakyReLU(),\n",
    "                                               nn.Linear(128,64),\n",
    "                                               nn.BatchNorm1d(64),\n",
    "                                               nn.LeakyReLU(),\n",
    "                                               nn.Linear(64,2)\n",
    "                                            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x=self.embedding(x)\n",
    "        output=self.transformer_encoder(x).permute(1, 0, 2)\n",
    "        output,hn=self.gru(output)\n",
    "        output=output.permute(1,0,2)\n",
    "        hn=hn.permute(1,0,2)\n",
    "        output=output.reshape(output.shape[0],-1)\n",
    "        hn=hn.reshape(output.shape[0],-1)\n",
    "        output=torch.cat([output,hn],1)\n",
    "        return self.block1(output)\n",
    "\n",
    "    def trainModel(self, x):\n",
    "        with torch.no_grad():\n",
    "            output=self.forward(x)\n",
    "        return self.block2(output)\n",
    "\n",
    "\n",
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        # euclidean_distance: [128]\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +     # calmp夹断用法\n",
    "                                      (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))     \n",
    "        \n",
    "        return loss_contrastive\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T17:45:35.107185Z",
     "start_time": "2021-10-04T17:45:35.104014Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\",0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T15:24:54.822646Z",
     "start_time": "2021-10-04T17:45:41.000678Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 139.59932, loss1: 1.30797, loss2_3: 138.29135\n",
      "\ttrain_acc: 0.7901, test_acc: \u001b[31m0.7879826798742512\u001b[0m, time: 276.00\n",
      "best_acc: 0.7879826798742512\n",
      "epoch: 2, loss: 106.43848, loss1: 0.86041, loss2_3: 105.57807\n",
      "\ttrain_acc: 0.8031, test_acc: \u001b[31m0.8038910967435791\u001b[0m, time: 293.25\n",
      "best_acc: 0.8038910967435791\n",
      "epoch: 3, loss: 100.72217, loss1: 0.81356, loss2_3: 99.90861\n",
      "\ttrain_acc: 0.8303, test_acc: \u001b[31m0.82855448128596\u001b[0m, time: 293.20\n",
      "best_acc: 0.82855448128596\n",
      "epoch: 4, loss: 97.44314, loss1: 0.78819, loss2_3: 96.65495\n",
      "\ttrain_acc: 0.8354, test_acc: \u001b[31m0.8329794175217985\u001b[0m, time: 292.15\n",
      "best_acc: 0.8329794175217985\n",
      "epoch: 5, loss: 96.03921, loss1: 0.77320, loss2_3: 95.26601\n",
      "\ttrain_acc: 0.8340, test_acc: \u001b[31m0.8321964529331515\u001b[0m, time: 292.70\n",
      "epoch: 6, loss: 94.32667, loss1: 0.75987, loss2_3: 93.56680\n",
      "\ttrain_acc: 0.8336, test_acc: \u001b[31m0.8307372916543093\u001b[0m, time: 294.04\n",
      "epoch: 7, loss: 93.50809, loss1: 0.75036, loss2_3: 92.75773\n",
      "\ttrain_acc: 0.8402, test_acc: \u001b[31m0.8384483065424996\u001b[0m, time: 292.62\n",
      "best_acc: 0.8384483065424996\n",
      "epoch: 8, loss: 92.58314, loss1: 0.74245, loss2_3: 91.84069\n",
      "\ttrain_acc: 0.8384, test_acc: \u001b[31m0.8362061806750104\u001b[0m, time: 293.30\n",
      "epoch: 9, loss: 92.18603, loss1: 0.73853, loss2_3: 91.44750\n",
      "\ttrain_acc: 0.8395, test_acc: \u001b[31m0.838163592146628\u001b[0m, time: 293.92\n",
      "epoch: 10, loss: 91.55943, loss1: 0.72922, loss2_3: 90.83021\n",
      "\ttrain_acc: 0.8424, test_acc: \u001b[31m0.8402989501156652\u001b[0m, time: 292.70\n",
      "best_acc: 0.8402989501156652\n",
      "epoch: 11, loss: 91.00368, loss1: 0.72883, loss2_3: 90.27485\n",
      "\ttrain_acc: 0.8383, test_acc: \u001b[31m0.8363959902722582\u001b[0m, time: 292.94\n",
      "epoch: 12, loss: 90.76490, loss1: 0.72259, loss2_3: 90.04232\n",
      "\ttrain_acc: 0.8474, test_acc: \u001b[31m0.8447357494513317\u001b[0m, time: 293.88\n",
      "best_acc: 0.8447357494513317\n",
      "epoch: 13, loss: 90.47474, loss1: 0.71720, loss2_3: 89.75753\n",
      "\ttrain_acc: 0.8491, test_acc: \u001b[31m0.8487810664926745\u001b[0m, time: 292.70\n",
      "best_acc: 0.8487810664926745\n",
      "epoch: 14, loss: 89.83359, loss1: 0.71311, loss2_3: 89.12047\n",
      "\ttrain_acc: 0.8478, test_acc: \u001b[31m0.8459576487336141\u001b[0m, time: 293.21\n",
      "epoch: 15, loss: 89.14451, loss1: 0.70735, loss2_3: 88.43716\n",
      "\ttrain_acc: 0.8434, test_acc: \u001b[31m0.840417581113945\u001b[0m, time: 293.08\n",
      "epoch: 16, loss: 88.89692, loss1: 0.70477, loss2_3: 88.19214\n",
      "\ttrain_acc: 0.8514, test_acc: \u001b[31m0.8486980247938787\u001b[0m, time: 292.80\n",
      "epoch: 17, loss: 88.62695, loss1: 0.70139, loss2_3: 87.92556\n",
      "\ttrain_acc: 0.8511, test_acc: \u001b[31m0.848437036597663\u001b[0m, time: 292.35\n",
      "epoch: 18, loss: 88.08640, loss1: 0.69881, loss2_3: 87.38759\n",
      "\ttrain_acc: 0.8523, test_acc: \u001b[31m0.8491606856871701\u001b[0m, time: 276.27\n",
      "best_acc: 0.8491606856871701\n",
      "epoch: 19, loss: 87.60514, loss1: 0.69568, loss2_3: 86.90946\n",
      "\ttrain_acc: 0.8515, test_acc: \u001b[31m0.8489352867904384\u001b[0m, time: 270.00\n",
      "epoch: 20, loss: 87.39385, loss1: 0.69166, loss2_3: 86.70219\n",
      "\ttrain_acc: 0.8528, test_acc: \u001b[31m0.8502876801708287\u001b[0m, time: 269.53\n",
      "best_acc: 0.8502876801708287\n",
      "epoch: 21, loss: 87.10257, loss1: 0.68927, loss2_3: 86.41330\n",
      "\ttrain_acc: 0.8543, test_acc: \u001b[31m0.8512485912568954\u001b[0m, time: 270.07\n",
      "best_acc: 0.8512485912568954\n",
      "epoch: 22, loss: 86.81506, loss1: 0.68924, loss2_3: 86.12583\n",
      "\ttrain_acc: 0.8539, test_acc: \u001b[31m0.8512485912568954\u001b[0m, time: 269.65\n",
      "epoch: 23, loss: 86.42936, loss1: 0.68599, loss2_3: 85.74336\n",
      "\ttrain_acc: 0.8524, test_acc: \u001b[31m0.8500978705735809\u001b[0m, time: 269.48\n",
      "epoch: 24, loss: 86.39584, loss1: 0.68351, loss2_3: 85.71233\n",
      "\ttrain_acc: 0.8547, test_acc: \u001b[31m0.8520434189453704\u001b[0m, time: 270.08\n",
      "best_acc: 0.8520434189453704\n",
      "epoch: 25, loss: 86.10487, loss1: 0.68159, loss2_3: 85.42328\n",
      "\ttrain_acc: 0.8549, test_acc: \u001b[31m0.8519959665460585\u001b[0m, time: 269.96\n",
      "epoch: 26, loss: 85.86967, loss1: 0.68298, loss2_3: 85.18669\n",
      "\ttrain_acc: 0.8549, test_acc: \u001b[31m0.8521145975443384\u001b[0m, time: 269.73\n",
      "best_acc: 0.8521145975443384\n",
      "epoch: 27, loss: 85.80008, loss1: 0.67688, loss2_3: 85.12319\n",
      "\ttrain_acc: 0.8508, test_acc: \u001b[31m0.848377721098523\u001b[0m, time: 269.66\n",
      "epoch: 28, loss: 85.67752, loss1: 0.68046, loss2_3: 84.99706\n",
      "\ttrain_acc: 0.8556, test_acc: \u001b[31m0.8525298060383179\u001b[0m, time: 270.19\n",
      "best_acc: 0.8525298060383179\n",
      "epoch: 29, loss: 85.45452, loss1: 0.67473, loss2_3: 84.77979\n",
      "\ttrain_acc: 0.8567, test_acc: \u001b[31m0.8541787769144077\u001b[0m, time: 269.91\n",
      "best_acc: 0.8541787769144077\n",
      "epoch: 30, loss: 85.23549, loss1: 0.67377, loss2_3: 84.56172\n",
      "\ttrain_acc: 0.8571, test_acc: \u001b[31m0.8533483599264488\u001b[0m, time: 269.67\n",
      "epoch: 31, loss: 85.16415, loss1: 0.67145, loss2_3: 84.49270\n",
      "\ttrain_acc: 0.8559, test_acc: \u001b[31m0.8522095023429622\u001b[0m, time: 269.76\n",
      "epoch: 32, loss: 84.99341, loss1: 0.67360, loss2_3: 84.31981\n",
      "\ttrain_acc: 0.8550, test_acc: \u001b[31m0.8523044071415861\u001b[0m, time: 270.47\n",
      "epoch: 33, loss: 84.81977, loss1: 0.67217, loss2_3: 84.14759\n",
      "\ttrain_acc: 0.8573, test_acc: \u001b[31m0.8542025031140638\u001b[0m, time: 269.07\n",
      "best_acc: 0.8542025031140638\n",
      "epoch: 34, loss: 84.93262, loss1: 0.67094, loss2_3: 84.26168\n",
      "\ttrain_acc: 0.8565, test_acc: \u001b[31m0.8539059256183641\u001b[0m, time: 269.60\n",
      "epoch: 35, loss: 84.68227, loss1: 0.67141, loss2_3: 84.01087\n",
      "\ttrain_acc: 0.8558, test_acc: \u001b[31m0.8532534551278249\u001b[0m, time: 269.86\n",
      "epoch: 36, loss: 84.47114, loss1: 0.66839, loss2_3: 83.80275\n",
      "\ttrain_acc: 0.8565, test_acc: \u001b[31m0.853182276528857\u001b[0m, time: 269.18\n",
      "epoch: 37, loss: 84.54871, loss1: 0.66955, loss2_3: 83.87916\n",
      "\ttrain_acc: 0.8562, test_acc: \u001b[31m0.8527433418352215\u001b[0m, time: 269.31\n",
      "epoch: 38, loss: 84.42989, loss1: 0.66861, loss2_3: 83.76128\n",
      "\ttrain_acc: 0.8572, test_acc: \u001b[31m0.8537042529212884\u001b[0m, time: 269.31\n",
      "epoch: 39, loss: 84.39531, loss1: 0.66892, loss2_3: 83.72639\n",
      "\ttrain_acc: 0.8576, test_acc: \u001b[31m0.853917788718192\u001b[0m, time: 270.22\n",
      "epoch: 40, loss: 84.35230, loss1: 0.66642, loss2_3: 83.68588\n",
      "\ttrain_acc: 0.8561, test_acc: \u001b[31m0.8529331514324693\u001b[0m, time: 269.28\n",
      "epoch: 41, loss: 84.21038, loss1: 0.66867, loss2_3: 83.54172\n",
      "\ttrain_acc: 0.8570, test_acc: \u001b[31m0.853953378017676\u001b[0m, time: 260.72\n",
      "epoch: 42, loss: 83.96357, loss1: 0.66617, loss2_3: 83.29740\n",
      "\ttrain_acc: 0.8580, test_acc: \u001b[31m0.854760068805979\u001b[0m, time: 248.66\n",
      "best_acc: 0.854760068805979\n",
      "epoch: 43, loss: 83.96544, loss1: 0.66633, loss2_3: 83.29911\n",
      "\ttrain_acc: 0.8555, test_acc: \u001b[31m0.8521976392431342\u001b[0m, time: 245.35\n",
      "epoch: 44, loss: 83.89972, loss1: 0.66558, loss2_3: 83.23414\n",
      "\ttrain_acc: 0.8589, test_acc: \u001b[31m0.8550329201020227\u001b[0m, time: 244.66\n",
      "best_acc: 0.8550329201020227\n",
      "epoch: 45, loss: 83.79672, loss1: 0.66569, loss2_3: 83.13103\n",
      "\ttrain_acc: 0.8562, test_acc: \u001b[31m0.853146687229373\u001b[0m, time: 245.08\n",
      "epoch: 46, loss: 83.66950, loss1: 0.66314, loss2_3: 83.00636\n",
      "\ttrain_acc: 0.8597, test_acc: \u001b[31m0.8549973308025387\u001b[0m, time: 244.95\n",
      "epoch: 47, loss: 83.69949, loss1: 0.66425, loss2_3: 83.03524\n",
      "\ttrain_acc: 0.8584, test_acc: \u001b[31m0.8544160389109674\u001b[0m, time: 244.89\n",
      "epoch: 48, loss: 83.41319, loss1: 0.66403, loss2_3: 82.74916\n",
      "\ttrain_acc: 0.8604, test_acc: \u001b[31m0.856325997983273\u001b[0m, time: 196.72\n",
      "best_acc: 0.856325997983273\n",
      "epoch: 49, loss: 83.55737, loss1: 0.66187, loss2_3: 82.89550\n",
      "\ttrain_acc: 0.8597, test_acc: \u001b[31m0.8556142119935939\u001b[0m, time: 176.01\n",
      "epoch: 50, loss: 83.45299, loss1: 0.66187, loss2_3: 82.79112\n",
      "\ttrain_acc: 0.8593, test_acc: \u001b[31m0.8552939082982384\u001b[0m, time: 177.16\n",
      "epoch: 51, loss: 83.34841, loss1: 0.65841, loss2_3: 82.68999\n",
      "\ttrain_acc: 0.8559, test_acc: \u001b[31m0.8524704905391779\u001b[0m, time: 177.08\n",
      "epoch: 52, loss: 83.23033, loss1: 0.66213, loss2_3: 82.56820\n",
      "\ttrain_acc: 0.8587, test_acc: \u001b[31m0.854724479506495\u001b[0m, time: 176.89\n",
      "epoch: 53, loss: 83.19866, loss1: 0.66145, loss2_3: 82.53721\n",
      "\ttrain_acc: 0.8599, test_acc: \u001b[31m0.8556972536923898\u001b[0m, time: 175.83\n",
      "epoch: 54, loss: 83.05323, loss1: 0.65970, loss2_3: 82.39353\n",
      "\ttrain_acc: 0.8543, test_acc: \u001b[31m0.8511536864582715\u001b[0m, time: 174.82\n",
      "epoch: 55, loss: 83.17415, loss1: 0.65859, loss2_3: 82.51556\n",
      "\ttrain_acc: 0.8578, test_acc: \u001b[31m0.8535144433240406\u001b[0m, time: 174.91\n",
      "epoch: 56, loss: 82.94321, loss1: 0.66011, loss2_3: 82.28310\n",
      "\ttrain_acc: 0.8607, test_acc: \u001b[31m0.856385313482413\u001b[0m, time: 175.41\n",
      "best_acc: 0.856385313482413\n",
      "epoch: 57, loss: 83.02656, loss1: 0.65773, loss2_3: 82.36883\n",
      "\ttrain_acc: 0.8582, test_acc: \u001b[31m0.8535500326235246\u001b[0m, time: 176.41\n",
      "epoch: 58, loss: 82.88802, loss1: 0.65777, loss2_3: 82.23025\n",
      "\ttrain_acc: 0.8583, test_acc: \u001b[31m0.8540957352156119\u001b[0m, time: 176.87\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 59, loss: 82.83264, loss1: 0.65754, loss2_3: 82.17510\n",
      "\ttrain_acc: 0.8556, test_acc: \u001b[31m0.8509638768610238\u001b[0m, time: 177.08\n",
      "epoch: 60, loss: 82.75300, loss1: 0.65638, loss2_3: 82.09662\n",
      "\ttrain_acc: 0.8586, test_acc: \u001b[31m0.8548312474049469\u001b[0m, time: 176.53\n",
      "epoch: 61, loss: 82.74594, loss1: 0.65736, loss2_3: 82.08858\n",
      "\ttrain_acc: 0.8596, test_acc: \u001b[31m0.8548075212052909\u001b[0m, time: 175.41\n",
      "epoch: 62, loss: 82.73167, loss1: 0.65721, loss2_3: 82.07446\n",
      "\ttrain_acc: 0.8616, test_acc: \u001b[31m0.8572038673705439\u001b[0m, time: 175.06\n",
      "best_acc: 0.8572038673705439\n",
      "epoch: 63, loss: 82.65339, loss1: 0.65544, loss2_3: 81.99794\n",
      "\ttrain_acc: 0.8608, test_acc: \u001b[31m0.854724479506495\u001b[0m, time: 175.02\n",
      "epoch: 64, loss: 82.53259, loss1: 0.65437, loss2_3: 81.87823\n",
      "\ttrain_acc: 0.8606, test_acc: \u001b[31m0.856290408683789\u001b[0m, time: 175.40\n",
      "epoch: 65, loss: 82.53944, loss1: 0.65527, loss2_3: 81.88417\n",
      "\ttrain_acc: 0.8613, test_acc: \u001b[31m0.8563971765822409\u001b[0m, time: 176.60\n",
      "epoch: 66, loss: 82.50135, loss1: 0.65294, loss2_3: 81.84840\n",
      "\ttrain_acc: 0.8602, test_acc: \u001b[31m0.8557565691915298\u001b[0m, time: 176.99\n",
      "epoch: 67, loss: 82.49261, loss1: 0.65415, loss2_3: 81.83846\n",
      "\ttrain_acc: 0.8549, test_acc: \u001b[31m0.8504063111691085\u001b[0m, time: 176.88\n",
      "epoch: 68, loss: 82.35683, loss1: 0.65290, loss2_3: 81.70393\n",
      "\ttrain_acc: 0.8609, test_acc: \u001b[31m0.8553769499970342\u001b[0m, time: 176.33\n",
      "epoch: 69, loss: 82.29837, loss1: 0.65479, loss2_3: 81.64358\n",
      "\ttrain_acc: 0.8513, test_acc: \u001b[31m0.846847381220713\u001b[0m, time: 174.80\n",
      "epoch: 70, loss: 82.28409, loss1: 0.65350, loss2_3: 81.63059\n",
      "\ttrain_acc: 0.8597, test_acc: \u001b[31m0.8551990034996144\u001b[0m, time: 174.75\n",
      "epoch: 71, loss: 82.27579, loss1: 0.65485, loss2_3: 81.62095\n",
      "\ttrain_acc: 0.8591, test_acc: \u001b[31m0.8546058485082152\u001b[0m, time: 174.86\n",
      "epoch: 72, loss: 82.20076, loss1: 0.65643, loss2_3: 81.54433\n",
      "\ttrain_acc: 0.8623, test_acc: \u001b[31m0.8568835636751884\u001b[0m, time: 175.81\n",
      "epoch: 73, loss: 82.20190, loss1: 0.65357, loss2_3: 81.54833\n",
      "\ttrain_acc: 0.8629, test_acc: \u001b[31m0.8580461474583309\u001b[0m, time: 176.90\n",
      "best_acc: 0.8580461474583309\n",
      "epoch: 74, loss: 82.27735, loss1: 0.65255, loss2_3: 81.62480\n",
      "\ttrain_acc: 0.8583, test_acc: \u001b[31m0.8536330743223204\u001b[0m, time: 176.74\n",
      "epoch: 75, loss: 82.02050, loss1: 0.65101, loss2_3: 81.36949\n",
      "\ttrain_acc: 0.8614, test_acc: \u001b[31m0.8558870632896376\u001b[0m, time: 176.93\n",
      "epoch: 76, loss: 81.94772, loss1: 0.65325, loss2_3: 81.29448\n",
      "\ttrain_acc: 0.8618, test_acc: \u001b[31m0.8568479743757044\u001b[0m, time: 175.94\n",
      "epoch: 77, loss: 81.89225, loss1: 0.65129, loss2_3: 81.24096\n",
      "\ttrain_acc: 0.8620, test_acc: \u001b[31m0.8573224983688238\u001b[0m, time: 174.92\n",
      "epoch: 78, loss: 81.86615, loss1: 0.65153, loss2_3: 81.21463\n",
      "\ttrain_acc: 0.8633, test_acc: \u001b[31m0.8577732961622873\u001b[0m, time: 174.82\n",
      "epoch: 79, loss: 82.00079, loss1: 0.65230, loss2_3: 81.34848\n",
      "\ttrain_acc: 0.8621, test_acc: \u001b[31m0.8563141348834451\u001b[0m, time: 174.66\n",
      "epoch: 80, loss: 81.91429, loss1: 0.65102, loss2_3: 81.26328\n",
      "\ttrain_acc: 0.8630, test_acc: \u001b[31m0.8576190758645234\u001b[0m, time: 236.82\n",
      "epoch: 81, loss: 81.88322, loss1: 0.64948, loss2_3: 81.23373\n",
      "\ttrain_acc: 0.8616, test_acc: \u001b[31m0.856385313482413\u001b[0m, time: 287.91\n",
      "epoch: 82, loss: 81.86560, loss1: 0.64906, loss2_3: 81.21654\n",
      "\ttrain_acc: 0.8633, test_acc: \u001b[31m0.8578563378610831\u001b[0m, time: 351.52\n",
      "epoch: 83, loss: 81.79526, loss1: 0.65096, loss2_3: 81.14430\n",
      "\ttrain_acc: 0.8582, test_acc: \u001b[31m0.8528856990331574\u001b[0m, time: 358.71\n",
      "epoch: 84, loss: 81.77957, loss1: 0.65143, loss2_3: 81.12815\n",
      "\ttrain_acc: 0.8633, test_acc: \u001b[31m0.8580461474583309\u001b[0m, time: 358.33\n",
      "epoch: 85, loss: 81.66921, loss1: 0.64820, loss2_3: 81.02101\n",
      "\ttrain_acc: 0.8637, test_acc: \u001b[31m0.857880064060739\u001b[0m, time: 358.25\n",
      "epoch: 86, loss: 81.62195, loss1: 0.64891, loss2_3: 80.97304\n",
      "\ttrain_acc: 0.8624, test_acc: \u001b[31m0.8565276706803487\u001b[0m, time: 358.91\n",
      "epoch: 87, loss: 81.66743, loss1: 0.65150, loss2_3: 81.01593\n",
      "\ttrain_acc: 0.8628, test_acc: \u001b[31m0.8564920813808649\u001b[0m, time: 358.84\n",
      "epoch: 88, loss: 81.63732, loss1: 0.64906, loss2_3: 80.98826\n",
      "\ttrain_acc: 0.8626, test_acc: \u001b[31m0.8566344385788006\u001b[0m, time: 358.45\n",
      "epoch: 89, loss: 81.49179, loss1: 0.65057, loss2_3: 80.84122\n",
      "\ttrain_acc: 0.8615, test_acc: \u001b[31m0.8559463787887776\u001b[0m, time: 357.52\n",
      "epoch: 90, loss: 81.56527, loss1: 0.65065, loss2_3: 80.91461\n",
      "\ttrain_acc: 0.8635, test_acc: \u001b[31m0.8574411293671036\u001b[0m, time: 358.19\n",
      "epoch: 91, loss: 81.43290, loss1: 0.64882, loss2_3: 80.78408\n",
      "\ttrain_acc: 0.8600, test_acc: \u001b[31m0.8544516282104514\u001b[0m, time: 358.56\n",
      "epoch: 92, loss: 81.50069, loss1: 0.64873, loss2_3: 80.85195\n",
      "\ttrain_acc: 0.8639, test_acc: \u001b[31m0.8574767186665876\u001b[0m, time: 358.07\n",
      "epoch: 93, loss: 81.43307, loss1: 0.64787, loss2_3: 80.78519\n",
      "\ttrain_acc: 0.8641, test_acc: \u001b[31m0.8581647784566108\u001b[0m, time: 357.82\n",
      "best_acc: 0.8581647784566108\n",
      "epoch: 94, loss: 81.52517, loss1: 0.65036, loss2_3: 80.87481\n",
      "\ttrain_acc: 0.8629, test_acc: \u001b[31m0.8566225754789727\u001b[0m, time: 358.30\n",
      "epoch: 95, loss: 81.42303, loss1: 0.64660, loss2_3: 80.77643\n",
      "\ttrain_acc: 0.8642, test_acc: \u001b[31m0.8584850821519663\u001b[0m, time: 358.00\n",
      "best_acc: 0.8584850821519663\n",
      "epoch: 96, loss: 81.45788, loss1: 0.64579, loss2_3: 80.81209\n",
      "\ttrain_acc: 0.8594, test_acc: \u001b[31m0.8536686636218044\u001b[0m, time: 358.16\n",
      "epoch: 97, loss: 81.28586, loss1: 0.64761, loss2_3: 80.63825\n",
      "\ttrain_acc: 0.8649, test_acc: \u001b[31m0.8584732190521384\u001b[0m, time: 358.56\n",
      "epoch: 98, loss: 81.36464, loss1: 0.64476, loss2_3: 80.71988\n",
      "\ttrain_acc: 0.8645, test_acc: \u001b[31m0.8581647784566108\u001b[0m, time: 359.51\n",
      "epoch: 99, loss: 81.41727, loss1: 0.64722, loss2_3: 80.77006\n",
      "\ttrain_acc: 0.8616, test_acc: \u001b[31m0.8567649326769085\u001b[0m, time: 358.06\n",
      "epoch: 100, loss: 81.35476, loss1: 0.64732, loss2_3: 80.70744\n",
      "\ttrain_acc: 0.8638, test_acc: \u001b[31m0.85710896257192\u001b[0m, time: 358.15\n",
      "epoch: 101, loss: 81.21465, loss1: 0.64669, loss2_3: 80.56796\n",
      "\ttrain_acc: 0.8612, test_acc: \u001b[31m0.8549380153033987\u001b[0m, time: 358.28\n",
      "epoch: 102, loss: 81.29073, loss1: 0.64769, loss2_3: 80.64304\n",
      "\ttrain_acc: 0.8580, test_acc: \u001b[31m0.8517942938489828\u001b[0m, time: 359.99\n",
      "epoch: 103, loss: 81.18766, loss1: 0.64760, loss2_3: 80.54005\n",
      "\ttrain_acc: 0.8574, test_acc: \u001b[31m0.8525179429384898\u001b[0m, time: 358.26\n",
      "epoch: 104, loss: 81.19995, loss1: 0.64643, loss2_3: 80.55352\n",
      "\ttrain_acc: 0.8647, test_acc: \u001b[31m0.8578563378610831\u001b[0m, time: 358.10\n",
      "epoch: 105, loss: 81.13051, loss1: 0.64562, loss2_3: 80.48489\n",
      "\ttrain_acc: 0.8596, test_acc: \u001b[31m0.8546058485082152\u001b[0m, time: 358.48\n",
      "epoch: 106, loss: 81.14627, loss1: 0.64356, loss2_3: 80.50271\n",
      "\ttrain_acc: 0.8616, test_acc: \u001b[31m0.8549142891037428\u001b[0m, time: 358.36\n",
      "epoch: 107, loss: 81.06218, loss1: 0.64597, loss2_3: 80.41620\n",
      "\ttrain_acc: 0.8658, test_acc: \u001b[31m0.8583664511536865\u001b[0m, time: 358.01\n",
      "epoch: 108, loss: 80.95251, loss1: 0.64535, loss2_3: 80.30716\n",
      "\ttrain_acc: 0.8607, test_acc: \u001b[31m0.8550329201020227\u001b[0m, time: 359.40\n",
      "epoch: 109, loss: 80.99230, loss1: 0.64596, loss2_3: 80.34634\n",
      "\ttrain_acc: 0.8617, test_acc: \u001b[31m0.8560531466872294\u001b[0m, time: 357.98\n",
      "epoch: 110, loss: 81.04177, loss1: 0.64454, loss2_3: 80.39723\n",
      "\ttrain_acc: 0.8587, test_acc: \u001b[31m0.8533364968266208\u001b[0m, time: 357.77\n",
      "epoch: 111, loss: 80.98773, loss1: 0.64471, loss2_3: 80.34302\n",
      "\ttrain_acc: 0.8659, test_acc: \u001b[31m0.8588528382466338\u001b[0m, time: 357.32\n",
      "best_acc: 0.8588528382466338\n",
      "epoch: 112, loss: 80.97639, loss1: 0.64432, loss2_3: 80.33206\n",
      "\ttrain_acc: 0.8657, test_acc: \u001b[31m0.8584850821519663\u001b[0m, time: 358.23\n",
      "epoch: 113, loss: 80.93094, loss1: 0.64413, loss2_3: 80.28681\n",
      "\ttrain_acc: 0.8653, test_acc: \u001b[31m0.8580342843585029\u001b[0m, time: 358.57\n",
      "epoch: 114, loss: 80.83462, loss1: 0.64597, loss2_3: 80.18865\n",
      "\ttrain_acc: 0.8569, test_acc: \u001b[31m0.851604484251735\u001b[0m, time: 359.13\n",
      "epoch: 115, loss: 80.90749, loss1: 0.64483, loss2_3: 80.26266\n",
      "\ttrain_acc: 0.8650, test_acc: \u001b[31m0.8569547422741562\u001b[0m, time: 357.89\n",
      "epoch: 116, loss: 80.93121, loss1: 0.64356, loss2_3: 80.28765\n",
      "\ttrain_acc: 0.8643, test_acc: \u001b[31m0.8576546651640073\u001b[0m, time: 357.88\n",
      "epoch: 117, loss: 80.93104, loss1: 0.64331, loss2_3: 80.28774\n",
      "\ttrain_acc: 0.8653, test_acc: \u001b[31m0.8582596832552346\u001b[0m, time: 359.19\n",
      "epoch: 118, loss: 80.87927, loss1: 0.64420, loss2_3: 80.23506\n",
      "\ttrain_acc: 0.8651, test_acc: \u001b[31m0.85716827807106\u001b[0m, time: 358.22\n",
      "epoch: 119, loss: 80.70063, loss1: 0.64552, loss2_3: 80.05511\n",
      "\ttrain_acc: 0.8628, test_acc: \u001b[31m0.8556498012930779\u001b[0m, time: 359.76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 120, loss: 80.77429, loss1: 0.64381, loss2_3: 80.13048\n",
      "\ttrain_acc: 0.8657, test_acc: \u001b[31m0.8589714692449137\u001b[0m, time: 358.13\n",
      "best_acc: 0.8589714692449137\n",
      "epoch: 121, loss: 80.73055, loss1: 0.64456, loss2_3: 80.08599\n",
      "\ttrain_acc: 0.8641, test_acc: \u001b[31m0.8569903315736402\u001b[0m, time: 358.16\n",
      "epoch: 122, loss: 80.74287, loss1: 0.64237, loss2_3: 80.10050\n",
      "\ttrain_acc: 0.8656, test_acc: \u001b[31m0.8590307847440536\u001b[0m, time: 359.10\n",
      "best_acc: 0.8590307847440536\n",
      "epoch: 123, loss: 80.65782, loss1: 0.64276, loss2_3: 80.01506\n",
      "\ttrain_acc: 0.8663, test_acc: \u001b[31m0.8582596832552346\u001b[0m, time: 358.15\n",
      "epoch: 124, loss: 80.63107, loss1: 0.64195, loss2_3: 79.98911\n",
      "\ttrain_acc: 0.8655, test_acc: \u001b[31m0.8583545880538584\u001b[0m, time: 358.34\n",
      "epoch: 125, loss: 80.64049, loss1: 0.64331, loss2_3: 79.99718\n",
      "\ttrain_acc: 0.8646, test_acc: \u001b[31m0.8562785455839611\u001b[0m, time: 358.25\n",
      "epoch: 126, loss: 80.76110, loss1: 0.64376, loss2_3: 80.11735\n",
      "\ttrain_acc: 0.8655, test_acc: \u001b[31m0.8577377068628033\u001b[0m, time: 357.99\n",
      "epoch: 127, loss: 80.64473, loss1: 0.64387, loss2_3: 80.00086\n",
      "\ttrain_acc: 0.8624, test_acc: \u001b[31m0.8544634913102793\u001b[0m, time: 358.72\n",
      "epoch: 128, loss: 80.61181, loss1: 0.64316, loss2_3: 79.96866\n",
      "\ttrain_acc: 0.8654, test_acc: \u001b[31m0.857939379559879\u001b[0m, time: 358.36\n",
      "epoch: 129, loss: 80.60320, loss1: 0.64195, loss2_3: 79.96125\n",
      "\ttrain_acc: 0.8660, test_acc: \u001b[31m0.8580817367578149\u001b[0m, time: 357.56\n",
      "epoch: 130, loss: 80.53717, loss1: 0.64164, loss2_3: 79.89552\n",
      "\ttrain_acc: 0.8661, test_acc: \u001b[31m0.8576665282638354\u001b[0m, time: 357.59\n",
      "epoch: 131, loss: 80.29953, loss1: 0.64230, loss2_3: 79.65723\n",
      "\ttrain_acc: 0.8660, test_acc: \u001b[31m0.8581885046562667\u001b[0m, time: 358.14\n",
      "epoch: 132, loss: 80.55458, loss1: 0.64127, loss2_3: 79.91331\n",
      "\ttrain_acc: 0.8664, test_acc: \u001b[31m0.8590663740435376\u001b[0m, time: 357.51\n",
      "best_acc: 0.8590663740435376\n",
      "epoch: 133, loss: 80.44012, loss1: 0.64102, loss2_3: 79.79910\n",
      "\ttrain_acc: 0.8616, test_acc: \u001b[31m0.8541906400142357\u001b[0m, time: 358.65\n",
      "epoch: 134, loss: 80.34943, loss1: 0.64286, loss2_3: 79.70657\n",
      "\ttrain_acc: 0.8655, test_acc: \u001b[31m0.8576546651640073\u001b[0m, time: 357.58\n",
      "epoch: 135, loss: 80.40843, loss1: 0.64284, loss2_3: 79.76559\n",
      "\ttrain_acc: 0.8672, test_acc: \u001b[31m0.8588053858473219\u001b[0m, time: 357.51\n",
      "epoch: 136, loss: 80.54983, loss1: 0.64340, loss2_3: 79.90643\n",
      "\ttrain_acc: 0.8665, test_acc: \u001b[31m0.858663028649386\u001b[0m, time: 315.78\n",
      "epoch: 137, loss: 80.42244, loss1: 0.64200, loss2_3: 79.78044\n",
      "\ttrain_acc: 0.8668, test_acc: \u001b[31m0.8580935998576428\u001b[0m, time: 300.09\n",
      "epoch: 138, loss: 80.34875, loss1: 0.64056, loss2_3: 79.70819\n",
      "\ttrain_acc: 0.8667, test_acc: \u001b[31m0.8579631057595349\u001b[0m, time: 300.95\n",
      "epoch: 139, loss: 80.28540, loss1: 0.64031, loss2_3: 79.64509\n",
      "\ttrain_acc: 0.8660, test_acc: \u001b[31m0.8582478201554066\u001b[0m, time: 300.07\n",
      "epoch: 140, loss: 80.35250, loss1: 0.64173, loss2_3: 79.71076\n",
      "\ttrain_acc: 0.8636, test_acc: \u001b[31m0.8558040215908417\u001b[0m, time: 300.14\n",
      "epoch: 141, loss: 80.30650, loss1: 0.64209, loss2_3: 79.66441\n",
      "\ttrain_acc: 0.8607, test_acc: \u001b[31m0.8541075983154398\u001b[0m, time: 300.26\n",
      "epoch: 142, loss: 80.22901, loss1: 0.63986, loss2_3: 79.58915\n",
      "\ttrain_acc: 0.8660, test_acc: \u001b[31m0.8578444747612551\u001b[0m, time: 298.48\n",
      "epoch: 143, loss: 80.23503, loss1: 0.64106, loss2_3: 79.59397\n",
      "\ttrain_acc: 0.8668, test_acc: \u001b[31m0.8592205943413014\u001b[0m, time: 299.95\n",
      "best_acc: 0.8592205943413014\n",
      "epoch: 144, loss: 80.12110, loss1: 0.63986, loss2_3: 79.48123\n",
      "\ttrain_acc: 0.8664, test_acc: \u001b[31m0.8584376297526544\u001b[0m, time: 300.57\n",
      "epoch: 145, loss: 80.19115, loss1: 0.63987, loss2_3: 79.55128\n",
      "\ttrain_acc: 0.8649, test_acc: \u001b[31m0.857097099472092\u001b[0m, time: 300.52\n",
      "epoch: 146, loss: 80.20136, loss1: 0.64011, loss2_3: 79.56125\n",
      "\ttrain_acc: 0.8666, test_acc: \u001b[31m0.8583308618542025\u001b[0m, time: 300.93\n",
      "epoch: 147, loss: 80.09682, loss1: 0.64013, loss2_3: 79.45669\n",
      "\ttrain_acc: 0.8665, test_acc: \u001b[31m0.8582478201554066\u001b[0m, time: 300.21\n",
      "epoch: 148, loss: 80.23121, loss1: 0.63941, loss2_3: 79.59180\n",
      "\ttrain_acc: 0.8671, test_acc: \u001b[31m0.857927516460051\u001b[0m, time: 299.81\n",
      "epoch: 149, loss: 80.16175, loss1: 0.63878, loss2_3: 79.52297\n",
      "\ttrain_acc: 0.8679, test_acc: \u001b[31m0.8586274393499022\u001b[0m, time: 299.80\n",
      "epoch: 150, loss: 80.08497, loss1: 0.63978, loss2_3: 79.44519\n",
      "\ttrain_acc: 0.8663, test_acc: \u001b[31m0.8578682009609111\u001b[0m, time: 298.88\n",
      "epoch: 151, loss: 79.98913, loss1: 0.63608, loss2_3: 79.35305\n",
      "\ttrain_acc: 0.8674, test_acc: \u001b[31m0.859564624236313\u001b[0m, time: 300.92\n",
      "best_acc: 0.859564624236313\n",
      "epoch: 152, loss: 80.04957, loss1: 0.63828, loss2_3: 79.41129\n",
      "\ttrain_acc: 0.8678, test_acc: \u001b[31m0.858781659647666\u001b[0m, time: 301.11\n",
      "epoch: 153, loss: 80.01447, loss1: 0.63996, loss2_3: 79.37451\n",
      "\ttrain_acc: 0.8675, test_acc: \u001b[31m0.8591375526425055\u001b[0m, time: 300.37\n",
      "epoch: 154, loss: 79.97250, loss1: 0.64052, loss2_3: 79.33198\n",
      "\ttrain_acc: 0.8632, test_acc: \u001b[31m0.8549024260039149\u001b[0m, time: 300.23\n",
      "epoch: 155, loss: 79.98936, loss1: 0.63906, loss2_3: 79.35030\n",
      "\ttrain_acc: 0.8682, test_acc: \u001b[31m0.8584376297526544\u001b[0m, time: 301.16\n",
      "epoch: 156, loss: 79.98209, loss1: 0.63893, loss2_3: 79.34316\n",
      "\ttrain_acc: 0.8666, test_acc: \u001b[31m0.8580224212586749\u001b[0m, time: 300.52\n",
      "epoch: 157, loss: 79.88959, loss1: 0.63882, loss2_3: 79.25078\n",
      "\ttrain_acc: 0.8669, test_acc: \u001b[31m0.8585918500504182\u001b[0m, time: 300.15\n",
      "epoch: 158, loss: 79.97714, loss1: 0.63970, loss2_3: 79.33743\n",
      "\ttrain_acc: 0.8683, test_acc: \u001b[31m0.8584732190521384\u001b[0m, time: 298.60\n",
      "epoch: 159, loss: 79.80954, loss1: 0.63944, loss2_3: 79.17010\n",
      "\ttrain_acc: 0.8664, test_acc: \u001b[31m0.8567886588765644\u001b[0m, time: 301.02\n",
      "epoch: 160, loss: 79.81472, loss1: 0.64065, loss2_3: 79.17408\n",
      "\ttrain_acc: 0.8673, test_acc: \u001b[31m0.858722344148526\u001b[0m, time: 301.00\n",
      "epoch: 161, loss: 79.92381, loss1: 0.63886, loss2_3: 79.28495\n",
      "\ttrain_acc: 0.8675, test_acc: \u001b[31m0.8584732190521384\u001b[0m, time: 302.12\n",
      "epoch: 162, loss: 79.83958, loss1: 0.63822, loss2_3: 79.20136\n",
      "\ttrain_acc: 0.8640, test_acc: \u001b[31m0.8551159618008185\u001b[0m, time: 299.92\n",
      "epoch: 163, loss: 79.88637, loss1: 0.63722, loss2_3: 79.24915\n",
      "\ttrain_acc: 0.8676, test_acc: \u001b[31m0.85875793344801\u001b[0m, time: 299.92\n",
      "epoch: 164, loss: 79.99487, loss1: 0.63776, loss2_3: 79.35711\n",
      "\ttrain_acc: 0.8675, test_acc: \u001b[31m0.8588647013464619\u001b[0m, time: 299.69\n",
      "epoch: 165, loss: 79.91613, loss1: 0.63810, loss2_3: 79.27803\n",
      "\ttrain_acc: 0.8683, test_acc: \u001b[31m0.8591612788421614\u001b[0m, time: 299.80\n",
      "epoch: 166, loss: 79.82344, loss1: 0.63849, loss2_3: 79.18495\n",
      "\ttrain_acc: 0.8685, test_acc: \u001b[31m0.858746070348182\u001b[0m, time: 300.28\n",
      "epoch: 167, loss: 79.79795, loss1: 0.63712, loss2_3: 79.16083\n",
      "\ttrain_acc: 0.8682, test_acc: \u001b[31m0.85875793344801\u001b[0m, time: 300.72\n",
      "epoch: 168, loss: 79.78391, loss1: 0.63532, loss2_3: 79.14858\n",
      "\ttrain_acc: 0.8647, test_acc: \u001b[31m0.8562429562844771\u001b[0m, time: 300.54\n",
      "epoch: 169, loss: 79.81775, loss1: 0.63821, loss2_3: 79.17954\n",
      "\ttrain_acc: 0.8679, test_acc: \u001b[31m0.8581647784566108\u001b[0m, time: 302.30\n",
      "epoch: 170, loss: 79.80965, loss1: 0.63791, loss2_3: 79.17175\n",
      "\ttrain_acc: 0.8670, test_acc: \u001b[31m0.857951242659707\u001b[0m, time: 300.09\n",
      "epoch: 171, loss: 79.62591, loss1: 0.63839, loss2_3: 78.98752\n",
      "\ttrain_acc: 0.8685, test_acc: \u001b[31m0.8588647013464619\u001b[0m, time: 300.12\n",
      "epoch: 172, loss: 79.74424, loss1: 0.63670, loss2_3: 79.10755\n",
      "\ttrain_acc: 0.8674, test_acc: \u001b[31m0.8583545880538584\u001b[0m, time: 300.40\n",
      "epoch: 173, loss: 79.71116, loss1: 0.63854, loss2_3: 79.07262\n",
      "\ttrain_acc: 0.8677, test_acc: \u001b[31m0.8582478201554066\u001b[0m, time: 299.90\n",
      "epoch: 174, loss: 79.56711, loss1: 0.63814, loss2_3: 78.92897\n",
      "\ttrain_acc: 0.8675, test_acc: \u001b[31m0.8575834865650395\u001b[0m, time: 299.39\n",
      "epoch: 175, loss: 79.56575, loss1: 0.63694, loss2_3: 78.92881\n",
      "\ttrain_acc: 0.8675, test_acc: \u001b[31m0.8585325345512782\u001b[0m, time: 300.50\n",
      "epoch: 176, loss: 79.73067, loss1: 0.63651, loss2_3: 79.09415\n",
      "\ttrain_acc: 0.8687, test_acc: \u001b[31m0.8585088083516222\u001b[0m, time: 300.16\n",
      "epoch: 177, loss: 79.54879, loss1: 0.63695, loss2_3: 78.91183\n",
      "\ttrain_acc: 0.8680, test_acc: \u001b[31m0.8581647784566108\u001b[0m, time: 300.75\n",
      "epoch: 178, loss: 79.58897, loss1: 0.63594, loss2_3: 78.95303\n",
      "\ttrain_acc: 0.8676, test_acc: \u001b[31m0.8583783142535144\u001b[0m, time: 299.89\n",
      "epoch: 179, loss: 79.65200, loss1: 0.63721, loss2_3: 79.01479\n",
      "\ttrain_acc: 0.8684, test_acc: \u001b[31m0.8589477430452577\u001b[0m, time: 299.84\n",
      "epoch: 180, loss: 79.49090, loss1: 0.63700, loss2_3: 78.85389\n",
      "\ttrain_acc: 0.8670, test_acc: \u001b[31m0.8563734503825849\u001b[0m, time: 300.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 181, loss: 79.43108, loss1: 0.63476, loss2_3: 78.79632\n",
      "\ttrain_acc: 0.8688, test_acc: \u001b[31m0.8592680467406133\u001b[0m, time: 299.28\n",
      "epoch: 182, loss: 79.58684, loss1: 0.63800, loss2_3: 78.94884\n",
      "\ttrain_acc: 0.8682, test_acc: \u001b[31m0.8583071356545465\u001b[0m, time: 299.67\n",
      "epoch: 183, loss: 79.53632, loss1: 0.63639, loss2_3: 78.89993\n",
      "\ttrain_acc: 0.8647, test_acc: \u001b[31m0.8557209798920458\u001b[0m, time: 300.49\n",
      "epoch: 184, loss: 79.48507, loss1: 0.63674, loss2_3: 78.84833\n",
      "\ttrain_acc: 0.8626, test_acc: \u001b[31m0.8540720090159559\u001b[0m, time: 300.36\n",
      "epoch: 185, loss: 79.46783, loss1: 0.63749, loss2_3: 78.83034\n",
      "\ttrain_acc: 0.8687, test_acc: \u001b[31m0.8591375526425055\u001b[0m, time: 299.89\n",
      "epoch: 186, loss: 79.46192, loss1: 0.63530, loss2_3: 78.82662\n",
      "\ttrain_acc: 0.8638, test_acc: \u001b[31m0.853953378017676\u001b[0m, time: 303.40\n",
      "epoch: 187, loss: 79.46401, loss1: 0.63447, loss2_3: 78.82954\n",
      "\ttrain_acc: 0.8665, test_acc: \u001b[31m0.8572157304703719\u001b[0m, time: 299.79\n",
      "epoch: 188, loss: 79.48343, loss1: 0.63451, loss2_3: 78.84892\n",
      "\ttrain_acc: 0.8699, test_acc: \u001b[31m0.8588409751468059\u001b[0m, time: 300.51\n",
      "epoch: 189, loss: 79.49220, loss1: 0.63661, loss2_3: 78.85559\n",
      "\ttrain_acc: 0.8686, test_acc: \u001b[31m0.8575004448662435\u001b[0m, time: 300.96\n",
      "epoch: 190, loss: 79.30800, loss1: 0.63437, loss2_3: 78.67364\n",
      "\ttrain_acc: 0.8563, test_acc: \u001b[31m0.8482828162998992\u001b[0m, time: 299.83\n",
      "epoch: 191, loss: 79.27671, loss1: 0.63368, loss2_3: 78.64303\n",
      "\ttrain_acc: 0.8695, test_acc: \u001b[31m0.859540898036657\u001b[0m, time: 299.40\n",
      "epoch: 192, loss: 79.26710, loss1: 0.63585, loss2_3: 78.63125\n",
      "\ttrain_acc: 0.8696, test_acc: \u001b[31m0.8590901002431935\u001b[0m, time: 296.12\n",
      "epoch: 193, loss: 79.28030, loss1: 0.63320, loss2_3: 78.64710\n",
      "\ttrain_acc: 0.8670, test_acc: \u001b[31m0.8564090396820689\u001b[0m, time: 295.84\n",
      "epoch: 194, loss: 79.32915, loss1: 0.63483, loss2_3: 78.69432\n",
      "\ttrain_acc: 0.8683, test_acc: \u001b[31m0.8580698736579868\u001b[0m, time: 297.06\n",
      "epoch: 195, loss: 79.30298, loss1: 0.63432, loss2_3: 78.66866\n",
      "\ttrain_acc: 0.8681, test_acc: \u001b[31m0.8577377068628033\u001b[0m, time: 298.05\n",
      "epoch: 196, loss: 79.27458, loss1: 0.63455, loss2_3: 78.64003\n",
      "\ttrain_acc: 0.8585, test_acc: \u001b[31m0.8498606085770212\u001b[0m, time: 298.14\n",
      "epoch: 197, loss: 79.21548, loss1: 0.63371, loss2_3: 78.58177\n",
      "\ttrain_acc: 0.8693, test_acc: \u001b[31m0.8583427249540305\u001b[0m, time: 299.55\n",
      "epoch: 198, loss: 79.25770, loss1: 0.63502, loss2_3: 78.62268\n",
      "\ttrain_acc: 0.8702, test_acc: \u001b[31m0.859540898036657\u001b[0m, time: 299.80\n",
      "epoch: 199, loss: 79.04805, loss1: 0.63508, loss2_3: 78.41297\n",
      "\ttrain_acc: 0.8691, test_acc: \u001b[31m0.8582122308559227\u001b[0m, time: 301.52\n",
      "epoch: 200, loss: 79.10721, loss1: 0.63421, loss2_3: 78.47300\n",
      "\ttrain_acc: 0.8692, test_acc: \u001b[31m0.8586748917492141\u001b[0m, time: 300.32\n",
      "epoch: 201, loss: 79.08907, loss1: 0.63452, loss2_3: 78.45455\n",
      "\ttrain_acc: 0.8679, test_acc: \u001b[31m0.8572513197698559\u001b[0m, time: 297.67\n",
      "epoch: 202, loss: 78.95783, loss1: 0.63237, loss2_3: 78.32545\n",
      "\ttrain_acc: 0.8704, test_acc: \u001b[31m0.8590070585443976\u001b[0m, time: 297.45\n",
      "epoch: 203, loss: 79.20855, loss1: 0.63400, loss2_3: 78.57455\n",
      "\ttrain_acc: 0.8708, test_acc: \u001b[31m0.8589596061450857\u001b[0m, time: 297.10\n",
      "epoch: 204, loss: 79.19981, loss1: 0.63318, loss2_3: 78.56663\n",
      "\ttrain_acc: 0.8694, test_acc: \u001b[31m0.8588053858473219\u001b[0m, time: 295.30\n",
      "epoch: 205, loss: 79.08216, loss1: 0.63425, loss2_3: 78.44791\n",
      "\ttrain_acc: 0.8685, test_acc: \u001b[31m0.8577021175633193\u001b[0m, time: 296.90\n",
      "epoch: 206, loss: 79.09077, loss1: 0.63351, loss2_3: 78.45726\n",
      "\ttrain_acc: 0.8668, test_acc: \u001b[31m0.856349724182929\u001b[0m, time: 320.83\n",
      "epoch: 207, loss: 79.13295, loss1: 0.63504, loss2_3: 78.49791\n",
      "\ttrain_acc: 0.8706, test_acc: \u001b[31m0.8590782371433655\u001b[0m, time: 340.21\n",
      "epoch: 208, loss: 78.97841, loss1: 0.63440, loss2_3: 78.34401\n",
      "\ttrain_acc: 0.8654, test_acc: \u001b[31m0.8559938311880895\u001b[0m, time: 363.27\n",
      "epoch: 209, loss: 78.97915, loss1: 0.63378, loss2_3: 78.34537\n",
      "\ttrain_acc: 0.8686, test_acc: \u001b[31m0.8578088854617711\u001b[0m, time: 701.80\n",
      "epoch: 210, loss: 79.08233, loss1: 0.63208, loss2_3: 78.45025\n",
      "\ttrain_acc: 0.8696, test_acc: \u001b[31m0.8591375526425055\u001b[0m, time: 675.68\n",
      "epoch: 211, loss: 79.01317, loss1: 0.63456, loss2_3: 78.37861\n",
      "\ttrain_acc: 0.8681, test_acc: \u001b[31m0.8574292662672757\u001b[0m, time: 1392.05\n",
      "epoch: 212, loss: 79.02435, loss1: 0.63376, loss2_3: 78.39059\n",
      "\ttrain_acc: 0.8698, test_acc: \u001b[31m0.8589121537457738\u001b[0m, time: 983.63\n",
      "epoch: 213, loss: 78.96106, loss1: 0.63384, loss2_3: 78.32722\n",
      "\ttrain_acc: 0.8699, test_acc: \u001b[31m0.8588884275461178\u001b[0m, time: 1223.96\n",
      "epoch: 214, loss: 78.96120, loss1: 0.63367, loss2_3: 78.32753\n",
      "\ttrain_acc: 0.8707, test_acc: \u001b[31m0.8591494157423335\u001b[0m, time: 1468.03\n",
      "epoch: 215, loss: 78.94591, loss1: 0.63532, loss2_3: 78.31059\n",
      "\ttrain_acc: 0.8653, test_acc: \u001b[31m0.8550329201020227\u001b[0m, time: 1264.74\n",
      "epoch: 216, loss: 78.93610, loss1: 0.63304, loss2_3: 78.30306\n",
      "\ttrain_acc: 0.8708, test_acc: \u001b[31m0.8591375526425055\u001b[0m, time: 791.31\n",
      "epoch: 217, loss: 78.91914, loss1: 0.63248, loss2_3: 78.28666\n",
      "\ttrain_acc: 0.8705, test_acc: \u001b[31m0.8595883504359689\u001b[0m, time: 363.03\n",
      "best_acc: 0.8595883504359689\n",
      "epoch: 218, loss: 78.93981, loss1: 0.63384, loss2_3: 78.30598\n",
      "\ttrain_acc: 0.8710, test_acc: \u001b[31m0.8598137493327006\u001b[0m, time: 299.71\n",
      "best_acc: 0.8598137493327006\n",
      "epoch: 219, loss: 78.93812, loss1: 0.63178, loss2_3: 78.30633\n",
      "\ttrain_acc: 0.8705, test_acc: \u001b[31m0.8597781600332167\u001b[0m, time: 300.19\n",
      "epoch: 220, loss: 78.80936, loss1: 0.63299, loss2_3: 78.17638\n",
      "\ttrain_acc: 0.8700, test_acc: \u001b[31m0.858746070348182\u001b[0m, time: 301.17\n",
      "epoch: 221, loss: 78.83638, loss1: 0.63209, loss2_3: 78.20428\n",
      "\ttrain_acc: 0.8693, test_acc: \u001b[31m0.857939379559879\u001b[0m, time: 300.41\n",
      "epoch: 222, loss: 78.86649, loss1: 0.63292, loss2_3: 78.23357\n",
      "\ttrain_acc: 0.8695, test_acc: \u001b[31m0.8584494928524824\u001b[0m, time: 300.43\n",
      "epoch: 223, loss: 78.86811, loss1: 0.63296, loss2_3: 78.23515\n",
      "\ttrain_acc: 0.8704, test_acc: \u001b[31m0.8589477430452577\u001b[0m, time: 300.37\n",
      "epoch: 224, loss: 78.84285, loss1: 0.63216, loss2_3: 78.21069\n",
      "\ttrain_acc: 0.8685, test_acc: \u001b[31m0.8583427249540305\u001b[0m, time: 300.35\n",
      "epoch: 225, loss: 78.79681, loss1: 0.63275, loss2_3: 78.16406\n",
      "\ttrain_acc: 0.8710, test_acc: \u001b[31m0.8600272851296044\u001b[0m, time: 300.73\n",
      "best_acc: 0.8600272851296044\n",
      "epoch: 226, loss: 78.75568, loss1: 0.63234, loss2_3: 78.12334\n",
      "\ttrain_acc: 0.8704, test_acc: \u001b[31m0.85875793344801\u001b[0m, time: 302.92\n",
      "epoch: 227, loss: 78.67256, loss1: 0.63089, loss2_3: 78.04167\n",
      "\ttrain_acc: 0.8697, test_acc: \u001b[31m0.8591375526425055\u001b[0m, time: 289.52\n",
      "epoch: 228, loss: 78.70143, loss1: 0.63321, loss2_3: 78.06822\n",
      "\ttrain_acc: 0.8709, test_acc: \u001b[31m0.8590782371433655\u001b[0m, time: 244.23\n",
      "epoch: 229, loss: 78.73847, loss1: 0.63226, loss2_3: 78.10621\n",
      "\ttrain_acc: 0.8705, test_acc: \u001b[31m0.8596476659351089\u001b[0m, time: 246.01\n",
      "epoch: 230, loss: 78.80234, loss1: 0.63024, loss2_3: 78.17210\n",
      "\ttrain_acc: 0.8711, test_acc: \u001b[31m0.8592205943413014\u001b[0m, time: 245.35\n",
      "epoch: 231, loss: 78.72232, loss1: 0.63102, loss2_3: 78.09130\n",
      "\ttrain_acc: 0.8707, test_acc: \u001b[31m0.85875793344801\u001b[0m, time: 245.13\n",
      "epoch: 232, loss: 78.70574, loss1: 0.63009, loss2_3: 78.07564\n",
      "\ttrain_acc: 0.8711, test_acc: \u001b[31m0.8599086541313246\u001b[0m, time: 244.42\n",
      "epoch: 233, loss: 78.59851, loss1: 0.63006, loss2_3: 77.96845\n",
      "\ttrain_acc: 0.8695, test_acc: \u001b[31m0.8586155762500741\u001b[0m, time: 244.65\n",
      "epoch: 234, loss: 78.57887, loss1: 0.63102, loss2_3: 77.94785\n",
      "\ttrain_acc: 0.8716, test_acc: \u001b[31m0.8591850050418174\u001b[0m, time: 245.33\n",
      "epoch: 235, loss: 78.58720, loss1: 0.63068, loss2_3: 77.95652\n",
      "\ttrain_acc: 0.8718, test_acc: \u001b[31m0.8599679696304644\u001b[0m, time: 244.68\n",
      "epoch: 236, loss: 78.64772, loss1: 0.63019, loss2_3: 78.01753\n",
      "\ttrain_acc: 0.8710, test_acc: \u001b[31m0.8603594519247879\u001b[0m, time: 245.20\n",
      "best_acc: 0.8603594519247879\n",
      "epoch: 237, loss: 78.63880, loss1: 0.63229, loss2_3: 78.00651\n",
      "\ttrain_acc: 0.8703, test_acc: \u001b[31m0.8580698736579868\u001b[0m, time: 245.20\n",
      "epoch: 238, loss: 78.56394, loss1: 0.62915, loss2_3: 77.93479\n",
      "\ttrain_acc: 0.8685, test_acc: \u001b[31m0.8573699507681357\u001b[0m, time: 245.33\n",
      "epoch: 239, loss: 78.55578, loss1: 0.63170, loss2_3: 77.92408\n",
      "\ttrain_acc: 0.8709, test_acc: \u001b[31m0.8576902544634913\u001b[0m, time: 245.66\n",
      "epoch: 240, loss: 78.51817, loss1: 0.62764, loss2_3: 77.89053\n",
      "\ttrain_acc: 0.8714, test_acc: \u001b[31m0.8599205172311525\u001b[0m, time: 245.31\n",
      "epoch: 241, loss: 78.49953, loss1: 0.63065, loss2_3: 77.86888\n",
      "\ttrain_acc: 0.8717, test_acc: \u001b[31m0.8590782371433655\u001b[0m, time: 244.67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 242, loss: 78.52725, loss1: 0.63062, loss2_3: 77.89664\n",
      "\ttrain_acc: 0.8721, test_acc: \u001b[31m0.8591494157423335\u001b[0m, time: 244.41\n",
      "epoch: 243, loss: 78.55216, loss1: 0.62896, loss2_3: 77.92320\n",
      "\ttrain_acc: 0.8687, test_acc: \u001b[31m0.8568005219763924\u001b[0m, time: 243.13\n",
      "epoch: 244, loss: 78.50539, loss1: 0.62977, loss2_3: 77.87562\n",
      "\ttrain_acc: 0.8702, test_acc: \u001b[31m0.8576783913636633\u001b[0m, time: 244.97\n",
      "epoch: 245, loss: 78.39053, loss1: 0.62802, loss2_3: 77.76251\n",
      "\ttrain_acc: 0.8717, test_acc: \u001b[31m0.8592443205409573\u001b[0m, time: 245.80\n",
      "epoch: 246, loss: 78.51140, loss1: 0.62901, loss2_3: 77.88239\n",
      "\ttrain_acc: 0.8704, test_acc: \u001b[31m0.8585088083516222\u001b[0m, time: 245.60\n",
      "epoch: 247, loss: 78.49866, loss1: 0.63033, loss2_3: 77.86833\n",
      "\ttrain_acc: 0.8661, test_acc: \u001b[31m0.8549024260039149\u001b[0m, time: 245.08\n",
      "epoch: 248, loss: 78.45573, loss1: 0.63126, loss2_3: 77.82447\n",
      "\ttrain_acc: 0.8702, test_acc: \u001b[31m0.8576665282638354\u001b[0m, time: 244.37\n",
      "epoch: 249, loss: 78.36103, loss1: 0.63056, loss2_3: 77.73047\n",
      "\ttrain_acc: 0.8722, test_acc: \u001b[31m0.8594934456373451\u001b[0m, time: 244.58\n",
      "epoch: 250, loss: 78.39708, loss1: 0.62808, loss2_3: 77.76900\n",
      "\ttrain_acc: 0.8729, test_acc: \u001b[31m0.8585443976511062\u001b[0m, time: 244.69\n"
     ]
    }
   ],
   "source": [
    "for num_model in range(1):  # just one train\n",
    "    net=newModel().to(device)\n",
    "    lr = 0.0001\n",
    "    \n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr,weight_decay=5e-4)\n",
    "    criterion = ContrastiveLoss()\n",
    "    criterion_model = nn.CrossEntropyLoss(reduction='sum')\n",
    "    \n",
    "    best_acc=0\n",
    "    EPOCH=250\n",
    "    for epoch in range(EPOCH):\n",
    "        loss_ls=[]\n",
    "        loss1_ls=[]\n",
    "        loss2_3_ls=[]\n",
    "        t0=time.time()\n",
    "        net.train()\n",
    "        for seq1,seq2,label,label1,label2 in train_iter_cont:\n",
    "                output1=net(seq1)\n",
    "                output2=net(seq2)\n",
    "                output3=net.trainModel(seq1)\n",
    "                output4=net.trainModel(seq2)\n",
    "                \n",
    "                loss1=criterion(output1, output2, label)\n",
    "                loss2=criterion_model(output3,label1)\n",
    "                loss3=criterion_model(output4,label2)\n",
    "                loss=loss1+loss2+loss3\n",
    "    #             print(loss)\n",
    "                optimizer.zero_grad() \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                loss_ls.append(loss.item())\n",
    "                loss1_ls.append(loss1.item())\n",
    "                loss2_3_ls.append((loss2+loss3).item())\n",
    "\n",
    "\n",
    "        net.eval() \n",
    "        with torch.no_grad(): \n",
    "            train_acc=evaluate_accuracy(train_iter,net)\n",
    "            test_acc=evaluate_accuracy(val_iter,net)\n",
    "            \n",
    "        results=f\"epoch: {epoch+1}, loss: {np.mean(loss_ls):.5f}, loss1: {np.mean(loss1_ls):.5f}, loss2_3: {np.mean(loss2_3_ls):.5f}\\n\"\n",
    "        results+=f'\\ttrain_acc: {train_acc:.4f}, test_acc: {colored(test_acc,\"red\")}, time: {time.time()-t0:.2f}'\n",
    "        print(results)\n",
    "        if test_acc>best_acc:\n",
    "            best_acc=test_acc\n",
    "#             torch.save({\"best_acc\":best_acc,\"model\":net.state_dict()},f'compareModel/2021ACS_PepFormer/Model/{num_model}.pl')\n",
    "            print(f\"best_acc: {best_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T15:24:54.863710Z",
     "start_time": "2021-10-05T15:24:54.825716Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T15:24:54.868723Z",
     "start_time": "2021-10-05T15:24:54.865103Z"
    }
   },
   "outputs": [],
   "source": [
    "def pred(data_iter, net):\n",
    "    y_pred = []\n",
    "    for x, y in data_iter:\n",
    "        x,y=x.to(device),y.to(device)\n",
    "        outputs=net.trainModel(x)\n",
    "        for _ in outputs.argmax(dim=1):\n",
    "            y_pred.append(int(_))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T15:24:54.874244Z",
     "start_time": "2021-10-05T15:24:54.869796Z"
    }
   },
   "outputs": [],
   "source": [
    "def pred_prob(data_iter, net):\n",
    "    y_pred = []\n",
    "    for x, y in data_iter:\n",
    "        x,y=x.to(device),y.to(device)\n",
    "        outputs=net.trainModel(x)\n",
    "        for _ in outputs:\n",
    "            y_pred.append(list(map(float, _)))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T15:25:17.450033Z",
     "start_time": "2021-10-05T15:24:54.875245Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8671543180745634\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.81      0.86     44509\n",
      "           1       0.83      0.92      0.87     44489\n",
      "\n",
      "    accuracy                           0.87     88998\n",
      "   macro avg       0.87      0.87      0.87     88998\n",
      "weighted avg       0.87      0.87      0.87     88998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_acc = evaluate_accuracy(test_iter,net)\n",
    "print('Test Accuracy: {}'.format(test_acc))\n",
    "\n",
    "# prediction\n",
    "y_pred = pred(test_iter, net)\n",
    "print(classification_report(test_label, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T15:25:32.115798Z",
     "start_time": "2021-10-05T15:25:17.451191Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf auc : 0.9412683037821481\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# AUC\n",
    "probs = np.array(pred_prob(test_iter, net))[:, 1]\n",
    "rf_auc = roc_auc_score(test_label, probs)\n",
    "print('rf auc : {}'.format(rf_auc))\n",
    "# plot the roc curve for the model81\n",
    "rf_fpr, rf_tpr, _ = roc_curve(test_label, probs)\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.plot(rf_fpr, rf_tpr, marker='.', label='RF AUC = {:.4f}'.format(rf_auc), color='orange')\n",
    "plt.title('ROC curve')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()\n",
    "\n",
    "cf_matrix = confusion_matrix(test_label, y_pred)\n",
    "plt.figure(figsize=(3, 3))\n",
    "sns.heatmap(cf_matrix, annot=True, fmt=',.0f')\n",
    "plt.show()\n",
    "plt.figure(figsize=(3, 3))\n",
    "sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, \n",
    "            fmt='.2%', cmap='Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
