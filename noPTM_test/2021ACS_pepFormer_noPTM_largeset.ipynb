{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-30T16:58:45.542001Z",
     "start_time": "2021-09-30T16:58:44.527240Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import time\n",
    "import pickle\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-30T16:58:45.552380Z",
     "start_time": "2021-09-30T16:58:45.545357Z"
    }
   },
   "outputs": [],
   "source": [
    "def genData(file,max_len):\n",
    "    aa_dict={'A':1,'R':2,'N':3,'D':4,'C':5,'Q':6,'E':7,'G':8,'H':9,'I':10,\n",
    "             'L':11,'K':12,'M':13,'F':14,'P':15,'O':16,'S':17,'U':18,'T':19,\n",
    "             'W':20,'Y':21,'V':22,'X':23}\n",
    "    with open(file, 'r') as inf:\n",
    "        lines = inf.read().splitlines()\n",
    "        \n",
    "    long_pep_counter=0\n",
    "    pep_codes=[]\n",
    "    labels=[]\n",
    "    for pep in lines:\n",
    "        pep,label=pep.split(\",\")\n",
    "        labels.append(int(label))\n",
    "        if not len(pep) > max_len:\n",
    "            current_pep=[]\n",
    "            for aa in pep:\n",
    "                current_pep.append(aa_dict[aa])\n",
    "            pep_codes.append(torch.tensor(current_pep))\n",
    "        else:\n",
    "            long_pep_counter += 1\n",
    "    print(\"length > {}:\".format(max_len),long_pep_counter)\n",
    "    data = rnn_utils.pad_sequence(pep_codes, batch_first=True)  # padding\n",
    "    return data,torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-30T16:58:45.599994Z",
     "start_time": "2021-09-30T16:58:45.595618Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net):\n",
    "    acc_sum, n = 0.0, 0\n",
    "    for x, y in data_iter:\n",
    "        x,y=x.to(device),y.to(device)\n",
    "        outputs=net.trainModel(x)\n",
    "        acc_sum += (outputs.argmax(dim=1) == y).float().sum().item()\n",
    "        n += y.shape[0]\n",
    "    return acc_sum / n\n",
    "\n",
    "def to_log(log):\n",
    "    with open(\"../compareModel/2021ACS_PepFormer/modelLog.log\",\"a+\") as f:\n",
    "        f.write(log+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-30T16:58:45.817589Z",
     "start_time": "2021-09-30T16:58:45.811205Z"
    }
   },
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    seq1_ls=[]\n",
    "    seq2_ls=[]\n",
    "    label1_ls=[]\n",
    "    label2_ls=[]\n",
    "    label_ls=[]\n",
    "    batch_size=len(batch)\n",
    "    for i in range(int(batch_size/2)):\n",
    "        seq1,label1=batch[i][0],batch[i][1]\n",
    "        seq2,label2=batch[i+int(batch_size/2)][0],batch[i+int(batch_size/2)][1]\n",
    "        label1_ls.append(label1.unsqueeze(0))\n",
    "        label2_ls.append(label2.unsqueeze(0))\n",
    "        label=(label1^label2)\n",
    "        seq1_ls.append(seq1.unsqueeze(0))\n",
    "        seq2_ls.append(seq2.unsqueeze(0))\n",
    "        label_ls.append(label.unsqueeze(0))\n",
    "    seq1=torch.cat(seq1_ls).to(device)\n",
    "    seq2=torch.cat(seq2_ls).to(device)\n",
    "    label=torch.cat(label_ls).to(device)\n",
    "    label1=torch.cat(label1_ls).to(device)\n",
    "    label2=torch.cat(label2_ls).to(device)\n",
    "    return seq1,seq2,label,label1,label2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Make Data X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-30T16:58:49.922155Z",
     "start_time": "2021-09-30T16:58:48.954901Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-30T16:59:02.702539Z",
     "start_time": "2021-09-30T16:58:49.923874Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length > 30: 0\n",
      "torch.Size([337179, 30]) torch.Size([337179])\n",
      "length > 30: 0\n",
      "torch.Size([84295, 30]) torch.Size([84295])\n",
      "length > 30: 0\n",
      "torch.Size([88998, 30]) torch.Size([88998])\n"
     ]
    }
   ],
   "source": [
    "df_detect_peptide_train = pd.read_csv('../data/df_detect_peptide_train_noptm_210930_includeDigest.csv')\n",
    "df_detect_peptide_test = pd.read_csv('../data/df_detect_peptide_test_noptm_210930_includeDigest.csv')\n",
    "\n",
    "tra, val = train_test_split(df_detect_peptide_train[['PEP', 'ID']], test_size=0.2, random_state=7)\n",
    "tra.to_csv('../compareModel/2021ACS_PepFormer/detect_peptide_train_noptm_includeDigest.csv', header=False, index=False)\n",
    "val.to_csv('../compareModel/2021ACS_PepFormer/detect_peptide_val_noptm_includeDigest.csv', header=False, index=False)\n",
    "df_detect_peptide_test[['PEP', 'ID']].to_csv('../compareModel/2021ACS_PepFormer/detect_peptide_test_noptm_includeDigest.csv', header=False, index=False)\n",
    "\n",
    "train_data,train_label=genData(\"../compareModel/2021ACS_PepFormer/detect_peptide_train_noptm_includeDigest.csv\",30)\n",
    "print(train_data.shape, train_label.shape)\n",
    "train_dataset = Data.TensorDataset(train_data, train_label)\n",
    "\n",
    "val_data,val_label=genData(\"../compareModel/2021ACS_PepFormer/detect_peptide_val_noptm_includeDigest.csv\",30)\n",
    "print(val_data.shape, val_label.shape)\n",
    "val_dataset = Data.TensorDataset(val_data, val_label)\n",
    "\n",
    "test_data,test_label=genData(\"../compareModel/2021ACS_PepFormer/detect_peptide_test_noptm_includeDigest.csv\",30)\n",
    "print(test_data.shape, test_label.shape)\n",
    "test_dataset = Data.TensorDataset(test_data, test_label)\n",
    "\n",
    "batch_size=256\n",
    "train_iter = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "train_iter_cont = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \n",
    "                                                                  shuffle=True, collate_fn=collate)\n",
    "val_iter = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_iter = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-30T17:00:16.920199Z",
     "start_time": "2021-09-30T17:00:16.904299Z"
    }
   },
   "outputs": [],
   "source": [
    "class newModel(nn.Module):\n",
    "    def __init__(self, vocab_size=24):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = 25\n",
    "        self.batch_size = 256\n",
    "        self.emb_dim = 512\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, self.emb_dim, padding_idx=0)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=1)\n",
    "        \n",
    "        self.gru = nn.GRU(self.emb_dim, self.hidden_dim, num_layers=2, \n",
    "                               bidirectional=True, dropout=0.2)\n",
    "        \n",
    "        \n",
    "        self.block1=nn.Sequential(nn.Linear(1600,1024),\n",
    "                                            nn.BatchNorm1d(1024),\n",
    "                                            nn.LeakyReLU(),\n",
    "                                            nn.Linear(1024,256),\n",
    "                                 )\n",
    "\n",
    "        self.block2=nn.Sequential(\n",
    "                                               nn.BatchNorm1d(256),\n",
    "                                               nn.LeakyReLU(),\n",
    "                                               nn.Linear(256,128),\n",
    "                                               nn.BatchNorm1d(128),\n",
    "                                               nn.LeakyReLU(),\n",
    "                                               nn.Linear(128,64),\n",
    "                                               nn.BatchNorm1d(64),\n",
    "                                               nn.LeakyReLU(),\n",
    "                                               nn.Linear(64,2)\n",
    "                                            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x=self.embedding(x)\n",
    "        output=self.transformer_encoder(x).permute(1, 0, 2)\n",
    "        output,hn=self.gru(output)\n",
    "        output=output.permute(1,0,2)\n",
    "        hn=hn.permute(1,0,2)\n",
    "        output=output.reshape(output.shape[0],-1)\n",
    "        hn=hn.reshape(output.shape[0],-1)\n",
    "        output=torch.cat([output,hn],1)\n",
    "        return self.block1(output)\n",
    "\n",
    "    def trainModel(self, x):\n",
    "        with torch.no_grad():\n",
    "            output=self.forward(x)\n",
    "        return self.block2(output)\n",
    "\n",
    "\n",
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        # euclidean_distance: [128]\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +     # calmp夹断用法\n",
    "                                      (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))     \n",
    "        \n",
    "        return loss_contrastive\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-30T17:00:18.291532Z",
     "start_time": "2021-09-30T17:00:18.283008Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\",0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T02:48:32.874663Z",
     "start_time": "2021-09-30T17:00:22.028953Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 141.72971, loss1: 1.26427, loss2_3: 140.46543\n",
      "\ttrain_acc: 0.8006, test_acc: \u001b[31m0.8020404531704134\u001b[0m, time: 182.94\n",
      "best_acc: 0.8020404531704134\n",
      "epoch: 2, loss: 106.03521, loss1: 0.85918, loss2_3: 105.17603\n",
      "\ttrain_acc: 0.8136, test_acc: \u001b[31m0.8124443917195563\u001b[0m, time: 166.90\n",
      "best_acc: 0.8124443917195563\n",
      "epoch: 3, loss: 100.91383, loss1: 0.82091, loss2_3: 100.09292\n",
      "\ttrain_acc: 0.8255, test_acc: \u001b[31m0.8229313719674951\u001b[0m, time: 147.29\n",
      "best_acc: 0.8229313719674951\n",
      "epoch: 4, loss: 97.16583, loss1: 0.78581, loss2_3: 96.38002\n",
      "\ttrain_acc: 0.8373, test_acc: \u001b[31m0.8347470193961682\u001b[0m, time: 146.37\n",
      "best_acc: 0.8347470193961682\n",
      "epoch: 5, loss: 95.24379, loss1: 0.76622, loss2_3: 94.47757\n",
      "\ttrain_acc: 0.8424, test_acc: \u001b[31m0.8400616881191055\u001b[0m, time: 146.90\n",
      "best_acc: 0.8400616881191055\n",
      "epoch: 6, loss: 94.07538, loss1: 0.75839, loss2_3: 93.31699\n",
      "\ttrain_acc: 0.8387, test_acc: \u001b[31m0.8358265614805148\u001b[0m, time: 156.36\n",
      "epoch: 7, loss: 92.99706, loss1: 0.74702, loss2_3: 92.25005\n",
      "\ttrain_acc: 0.8429, test_acc: \u001b[31m0.8405243490123969\u001b[0m, time: 163.48\n",
      "best_acc: 0.8405243490123969\n",
      "epoch: 8, loss: 92.50782, loss1: 0.73752, loss2_3: 91.77029\n",
      "\ttrain_acc: 0.8402, test_acc: \u001b[31m0.8384957589418115\u001b[0m, time: 163.74\n",
      "epoch: 9, loss: 91.72144, loss1: 0.73058, loss2_3: 90.99086\n",
      "\ttrain_acc: 0.8423, test_acc: \u001b[31m0.8406311169108488\u001b[0m, time: 164.99\n",
      "best_acc: 0.8406311169108488\n",
      "epoch: 10, loss: 91.09500, loss1: 0.72676, loss2_3: 90.36824\n",
      "\ttrain_acc: 0.8467, test_acc: \u001b[31m0.8438816062637167\u001b[0m, time: 164.39\n",
      "best_acc: 0.8438816062637167\n",
      "epoch: 11, loss: 90.68668, loss1: 0.72164, loss2_3: 89.96504\n",
      "\ttrain_acc: 0.8456, test_acc: \u001b[31m0.8434189453704253\u001b[0m, time: 163.64\n",
      "epoch: 12, loss: 90.05946, loss1: 0.71624, loss2_3: 89.34322\n",
      "\ttrain_acc: 0.8441, test_acc: \u001b[31m0.8426359807817783\u001b[0m, time: 163.60\n",
      "epoch: 13, loss: 89.65223, loss1: 0.70885, loss2_3: 88.94338\n",
      "\ttrain_acc: 0.8503, test_acc: \u001b[31m0.8469185598196809\u001b[0m, time: 164.70\n",
      "best_acc: 0.8469185598196809\n",
      "epoch: 14, loss: 89.23554, loss1: 0.70757, loss2_3: 88.52798\n",
      "\ttrain_acc: 0.8494, test_acc: \u001b[31m0.8472032742155525\u001b[0m, time: 163.92\n",
      "best_acc: 0.8472032742155525\n",
      "epoch: 15, loss: 88.97484, loss1: 0.70832, loss2_3: 88.26652\n",
      "\ttrain_acc: 0.8517, test_acc: \u001b[31m0.8486624354943947\u001b[0m, time: 163.24\n",
      "best_acc: 0.8486624354943947\n",
      "epoch: 16, loss: 88.43555, loss1: 0.70387, loss2_3: 87.73169\n",
      "\ttrain_acc: 0.8517, test_acc: \u001b[31m0.8494809893825257\u001b[0m, time: 163.87\n",
      "best_acc: 0.8494809893825257\n",
      "epoch: 17, loss: 88.08645, loss1: 0.69542, loss2_3: 87.39103\n",
      "\ttrain_acc: 0.8467, test_acc: \u001b[31m0.8439883741621685\u001b[0m, time: 162.99\n",
      "epoch: 18, loss: 87.76251, loss1: 0.69285, loss2_3: 87.06966\n",
      "\ttrain_acc: 0.8498, test_acc: \u001b[31m0.8468829705201969\u001b[0m, time: 163.74\n",
      "epoch: 19, loss: 87.29905, loss1: 0.69091, loss2_3: 86.60814\n",
      "\ttrain_acc: 0.8504, test_acc: \u001b[31m0.8487454771931906\u001b[0m, time: 164.29\n",
      "epoch: 20, loss: 86.88904, loss1: 0.68813, loss2_3: 86.20092\n",
      "\ttrain_acc: 0.8543, test_acc: \u001b[31m0.8516519366510469\u001b[0m, time: 163.71\n",
      "best_acc: 0.8516519366510469\n",
      "epoch: 21, loss: 86.70420, loss1: 0.68542, loss2_3: 86.01878\n",
      "\ttrain_acc: 0.8466, test_acc: \u001b[31m0.8435613025683611\u001b[0m, time: 172.57\n",
      "epoch: 22, loss: 86.43710, loss1: 0.68456, loss2_3: 85.75255\n",
      "\ttrain_acc: 0.8542, test_acc: \u001b[31m0.8519366510469185\u001b[0m, time: 206.20\n",
      "best_acc: 0.8519366510469185\n",
      "epoch: 23, loss: 85.99123, loss1: 0.68240, loss2_3: 85.30883\n",
      "\ttrain_acc: 0.8560, test_acc: \u001b[31m0.8535856219230085\u001b[0m, time: 207.70\n",
      "best_acc: 0.8535856219230085\n",
      "epoch: 24, loss: 85.98942, loss1: 0.68108, loss2_3: 85.30834\n",
      "\ttrain_acc: 0.8553, test_acc: \u001b[31m0.8520671451450264\u001b[0m, time: 206.37\n",
      "epoch: 25, loss: 85.76191, loss1: 0.68057, loss2_3: 85.08134\n",
      "\ttrain_acc: 0.8536, test_acc: \u001b[31m0.8503825849694525\u001b[0m, time: 206.22\n",
      "epoch: 26, loss: 85.40466, loss1: 0.67744, loss2_3: 84.72722\n",
      "\ttrain_acc: 0.8559, test_acc: \u001b[31m0.853111097929889\u001b[0m, time: 206.14\n",
      "epoch: 27, loss: 85.27554, loss1: 0.67195, loss2_3: 84.60359\n",
      "\ttrain_acc: 0.8545, test_acc: \u001b[31m0.8514146746544872\u001b[0m, time: 205.36\n",
      "epoch: 28, loss: 85.24549, loss1: 0.67266, loss2_3: 84.57283\n",
      "\ttrain_acc: 0.8563, test_acc: \u001b[31m0.8541075983154398\u001b[0m, time: 207.26\n",
      "best_acc: 0.8541075983154398\n",
      "epoch: 29, loss: 85.05153, loss1: 0.67294, loss2_3: 84.37859\n",
      "\ttrain_acc: 0.8564, test_acc: \u001b[31m0.8540364197164719\u001b[0m, time: 205.44\n",
      "epoch: 30, loss: 85.07361, loss1: 0.67145, loss2_3: 84.40216\n",
      "\ttrain_acc: 0.8511, test_acc: \u001b[31m0.8487098878937066\u001b[0m, time: 205.95\n",
      "epoch: 31, loss: 84.91842, loss1: 0.67021, loss2_3: 84.24821\n",
      "\ttrain_acc: 0.8579, test_acc: \u001b[31m0.8548905629040868\u001b[0m, time: 205.90\n",
      "best_acc: 0.8548905629040868\n",
      "epoch: 32, loss: 84.79063, loss1: 0.67041, loss2_3: 84.12022\n",
      "\ttrain_acc: 0.8574, test_acc: \u001b[31m0.8549498784032268\u001b[0m, time: 206.58\n",
      "best_acc: 0.8549498784032268\n",
      "epoch: 33, loss: 84.55728, loss1: 0.66902, loss2_3: 83.88826\n",
      "\ttrain_acc: 0.8578, test_acc: \u001b[31m0.8538466101192241\u001b[0m, time: 206.00\n",
      "epoch: 34, loss: 84.43148, loss1: 0.66870, loss2_3: 83.76278\n",
      "\ttrain_acc: 0.8577, test_acc: \u001b[31m0.8544041758111395\u001b[0m, time: 206.07\n",
      "epoch: 35, loss: 84.34317, loss1: 0.66686, loss2_3: 83.67631\n",
      "\ttrain_acc: 0.8583, test_acc: \u001b[31m0.8548431105047749\u001b[0m, time: 205.65\n",
      "epoch: 36, loss: 84.34086, loss1: 0.66835, loss2_3: 83.67251\n",
      "\ttrain_acc: 0.8585, test_acc: \u001b[31m0.8557921584910138\u001b[0m, time: 206.31\n",
      "best_acc: 0.8557921584910138\n",
      "epoch: 37, loss: 84.24774, loss1: 0.66729, loss2_3: 83.58045\n",
      "\ttrain_acc: 0.8584, test_acc: \u001b[31m0.8549617415030547\u001b[0m, time: 206.23\n",
      "epoch: 38, loss: 84.22687, loss1: 0.66654, loss2_3: 83.56033\n",
      "\ttrain_acc: 0.8588, test_acc: \u001b[31m0.8547482057061511\u001b[0m, time: 205.82\n",
      "epoch: 39, loss: 84.08755, loss1: 0.66463, loss2_3: 83.42292\n",
      "\ttrain_acc: 0.8595, test_acc: \u001b[31m0.8554006761966902\u001b[0m, time: 206.17\n",
      "epoch: 40, loss: 84.05709, loss1: 0.66506, loss2_3: 83.39203\n",
      "\ttrain_acc: 0.8595, test_acc: \u001b[31m0.8562073669849932\u001b[0m, time: 206.38\n",
      "best_acc: 0.8562073669849932\n",
      "epoch: 41, loss: 83.99168, loss1: 0.66569, loss2_3: 83.32600\n",
      "\ttrain_acc: 0.8589, test_acc: \u001b[31m0.8551159618008185\u001b[0m, time: 206.50\n",
      "epoch: 42, loss: 83.96786, loss1: 0.66577, loss2_3: 83.30210\n",
      "\ttrain_acc: 0.8571, test_acc: \u001b[31m0.8542025031140638\u001b[0m, time: 206.07\n",
      "epoch: 43, loss: 83.90164, loss1: 0.66309, loss2_3: 83.23855\n",
      "\ttrain_acc: 0.8597, test_acc: \u001b[31m0.8556498012930779\u001b[0m, time: 205.30\n",
      "epoch: 44, loss: 83.85563, loss1: 0.66307, loss2_3: 83.19256\n",
      "\ttrain_acc: 0.8589, test_acc: \u001b[31m0.8546651640073551\u001b[0m, time: 206.52\n",
      "epoch: 45, loss: 83.73049, loss1: 0.66329, loss2_3: 83.06719\n",
      "\ttrain_acc: 0.8594, test_acc: \u001b[31m0.8551871403997865\u001b[0m, time: 206.77\n",
      "epoch: 46, loss: 83.70130, loss1: 0.66233, loss2_3: 83.03897\n",
      "\ttrain_acc: 0.8589, test_acc: \u001b[31m0.8550803725013346\u001b[0m, time: 207.75\n",
      "epoch: 47, loss: 83.61628, loss1: 0.66221, loss2_3: 82.95408\n",
      "\ttrain_acc: 0.8593, test_acc: \u001b[31m0.8556379381932498\u001b[0m, time: 206.91\n",
      "epoch: 48, loss: 83.57477, loss1: 0.66270, loss2_3: 82.91207\n",
      "\ttrain_acc: 0.8600, test_acc: \u001b[31m0.8560531466872294\u001b[0m, time: 205.34\n",
      "epoch: 49, loss: 83.51356, loss1: 0.66147, loss2_3: 82.85209\n",
      "\ttrain_acc: 0.8594, test_acc: \u001b[31m0.8550447832018506\u001b[0m, time: 199.12\n",
      "epoch: 50, loss: 83.55109, loss1: 0.65971, loss2_3: 82.89138\n",
      "\ttrain_acc: 0.8581, test_acc: \u001b[31m0.853965241117504\u001b[0m, time: 183.93\n",
      "epoch: 51, loss: 83.35218, loss1: 0.65962, loss2_3: 82.69256\n",
      "\ttrain_acc: 0.8575, test_acc: \u001b[31m0.8526128477371137\u001b[0m, time: 182.85\n",
      "epoch: 52, loss: 83.36000, loss1: 0.66054, loss2_3: 82.69946\n",
      "\ttrain_acc: 0.8605, test_acc: \u001b[31m0.8564090396820689\u001b[0m, time: 183.02\n",
      "best_acc: 0.8564090396820689\n",
      "epoch: 53, loss: 83.39859, loss1: 0.65854, loss2_3: 82.74005\n",
      "\ttrain_acc: 0.8595, test_acc: \u001b[31m0.8548905629040868\u001b[0m, time: 183.92\n",
      "epoch: 54, loss: 83.12872, loss1: 0.65977, loss2_3: 82.46895\n",
      "\ttrain_acc: 0.8600, test_acc: \u001b[31m0.8552464558989263\u001b[0m, time: 183.66\n",
      "epoch: 55, loss: 83.26987, loss1: 0.65976, loss2_3: 82.61011\n",
      "\ttrain_acc: 0.8598, test_acc: \u001b[31m0.8554955809953141\u001b[0m, time: 183.41\n",
      "epoch: 56, loss: 83.23808, loss1: 0.66015, loss2_3: 82.57793\n",
      "\ttrain_acc: 0.8610, test_acc: \u001b[31m0.8552583189987544\u001b[0m, time: 183.53\n",
      "epoch: 57, loss: 83.01258, loss1: 0.65741, loss2_3: 82.35518\n",
      "\ttrain_acc: 0.8613, test_acc: \u001b[31m0.8571801411708879\u001b[0m, time: 184.12\n",
      "best_acc: 0.8571801411708879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 58, loss: 83.01435, loss1: 0.65806, loss2_3: 82.35629\n",
      "\ttrain_acc: 0.8598, test_acc: \u001b[31m0.8551752772999585\u001b[0m, time: 182.64\n",
      "epoch: 59, loss: 82.96769, loss1: 0.65893, loss2_3: 82.30876\n",
      "\ttrain_acc: 0.8617, test_acc: \u001b[31m0.8567174802775965\u001b[0m, time: 184.47\n",
      "epoch: 60, loss: 82.94025, loss1: 0.65653, loss2_3: 82.28371\n",
      "\ttrain_acc: 0.8550, test_acc: \u001b[31m0.850762204163948\u001b[0m, time: 183.61\n",
      "epoch: 61, loss: 82.91787, loss1: 0.65620, loss2_3: 82.26167\n",
      "\ttrain_acc: 0.8606, test_acc: \u001b[31m0.8562073669849932\u001b[0m, time: 182.57\n",
      "epoch: 62, loss: 82.81753, loss1: 0.65992, loss2_3: 82.15761\n",
      "\ttrain_acc: 0.8615, test_acc: \u001b[31m0.8572869090693398\u001b[0m, time: 183.27\n",
      "best_acc: 0.8572869090693398\n",
      "epoch: 63, loss: 82.83877, loss1: 0.65574, loss2_3: 82.18303\n",
      "\ttrain_acc: 0.8615, test_acc: \u001b[31m0.8561243252861973\u001b[0m, time: 183.93\n",
      "epoch: 64, loss: 82.89647, loss1: 0.65693, loss2_3: 82.23954\n",
      "\ttrain_acc: 0.8597, test_acc: \u001b[31m0.8553650868972063\u001b[0m, time: 183.61\n",
      "epoch: 65, loss: 82.76395, loss1: 0.65781, loss2_3: 82.10614\n",
      "\ttrain_acc: 0.8605, test_acc: \u001b[31m0.8558040215908417\u001b[0m, time: 183.13\n",
      "epoch: 66, loss: 82.71498, loss1: 0.65610, loss2_3: 82.05888\n",
      "\ttrain_acc: 0.8610, test_acc: \u001b[31m0.8559819680882614\u001b[0m, time: 182.71\n",
      "epoch: 67, loss: 82.60466, loss1: 0.65531, loss2_3: 81.94935\n",
      "\ttrain_acc: 0.8596, test_acc: \u001b[31m0.8561599145856812\u001b[0m, time: 184.15\n",
      "epoch: 68, loss: 82.65334, loss1: 0.65482, loss2_3: 81.99852\n",
      "\ttrain_acc: 0.8613, test_acc: \u001b[31m0.8569310160745003\u001b[0m, time: 182.97\n",
      "epoch: 69, loss: 82.65069, loss1: 0.65275, loss2_3: 81.99794\n",
      "\ttrain_acc: 0.8598, test_acc: \u001b[31m0.8557921584910138\u001b[0m, time: 183.44\n",
      "epoch: 70, loss: 82.51354, loss1: 0.65368, loss2_3: 81.85986\n",
      "\ttrain_acc: 0.8618, test_acc: \u001b[31m0.8561955038851652\u001b[0m, time: 184.35\n",
      "epoch: 71, loss: 82.41678, loss1: 0.65607, loss2_3: 81.76072\n",
      "\ttrain_acc: 0.8624, test_acc: \u001b[31m0.8568479743757044\u001b[0m, time: 182.92\n",
      "epoch: 72, loss: 82.48797, loss1: 0.65426, loss2_3: 81.83371\n",
      "\ttrain_acc: 0.8627, test_acc: \u001b[31m0.8577970223619432\u001b[0m, time: 183.56\n",
      "best_acc: 0.8577970223619432\n",
      "epoch: 73, loss: 82.36607, loss1: 0.65303, loss2_3: 81.71304\n",
      "\ttrain_acc: 0.8635, test_acc: \u001b[31m0.8576902544634913\u001b[0m, time: 183.66\n",
      "epoch: 74, loss: 82.40278, loss1: 0.65305, loss2_3: 81.74973\n",
      "\ttrain_acc: 0.8628, test_acc: \u001b[31m0.8565158075805208\u001b[0m, time: 183.19\n",
      "epoch: 75, loss: 82.21560, loss1: 0.65363, loss2_3: 81.56198\n",
      "\ttrain_acc: 0.8627, test_acc: \u001b[31m0.8575478972655555\u001b[0m, time: 183.21\n",
      "epoch: 76, loss: 82.22059, loss1: 0.65172, loss2_3: 81.56887\n",
      "\ttrain_acc: 0.8611, test_acc: \u001b[31m0.8564683551812089\u001b[0m, time: 183.16\n",
      "epoch: 77, loss: 82.07245, loss1: 0.65286, loss2_3: 81.41959\n",
      "\ttrain_acc: 0.8628, test_acc: \u001b[31m0.8578088854617711\u001b[0m, time: 182.77\n",
      "best_acc: 0.8578088854617711\n",
      "epoch: 78, loss: 82.22618, loss1: 0.65092, loss2_3: 81.57526\n",
      "\ttrain_acc: 0.8628, test_acc: \u001b[31m0.8578919271605671\u001b[0m, time: 183.01\n",
      "best_acc: 0.8578919271605671\n",
      "epoch: 79, loss: 82.15453, loss1: 0.65150, loss2_3: 81.50303\n",
      "\ttrain_acc: 0.8615, test_acc: \u001b[31m0.8553176344978943\u001b[0m, time: 183.51\n",
      "epoch: 80, loss: 82.10630, loss1: 0.65119, loss2_3: 81.45511\n",
      "\ttrain_acc: 0.8633, test_acc: \u001b[31m0.8583664511536865\u001b[0m, time: 186.59\n",
      "best_acc: 0.8583664511536865\n",
      "epoch: 81, loss: 82.12420, loss1: 0.65362, loss2_3: 81.47058\n",
      "\ttrain_acc: 0.8629, test_acc: \u001b[31m0.8569428791743282\u001b[0m, time: 190.11\n",
      "epoch: 82, loss: 82.02376, loss1: 0.65192, loss2_3: 81.37184\n",
      "\ttrain_acc: 0.8628, test_acc: \u001b[31m0.8563734503825849\u001b[0m, time: 189.56\n",
      "epoch: 83, loss: 81.92291, loss1: 0.65049, loss2_3: 81.27242\n",
      "\ttrain_acc: 0.8613, test_acc: \u001b[31m0.8561005990865413\u001b[0m, time: 189.91\n",
      "epoch: 84, loss: 81.99864, loss1: 0.65101, loss2_3: 81.34763\n",
      "\ttrain_acc: 0.8623, test_acc: \u001b[31m0.8572157304703719\u001b[0m, time: 189.52\n",
      "epoch: 85, loss: 81.84485, loss1: 0.65148, loss2_3: 81.19337\n",
      "\ttrain_acc: 0.8629, test_acc: \u001b[31m0.8572869090693398\u001b[0m, time: 191.14\n",
      "epoch: 86, loss: 81.81722, loss1: 0.64923, loss2_3: 81.16799\n",
      "\ttrain_acc: 0.8632, test_acc: \u001b[31m0.8573224983688238\u001b[0m, time: 189.91\n",
      "epoch: 87, loss: 81.88014, loss1: 0.64822, loss2_3: 81.23192\n",
      "\ttrain_acc: 0.8623, test_acc: \u001b[31m0.8566344385788006\u001b[0m, time: 190.21\n",
      "epoch: 88, loss: 81.82731, loss1: 0.64787, loss2_3: 81.17943\n",
      "\ttrain_acc: 0.8638, test_acc: \u001b[31m0.8583664511536865\u001b[0m, time: 191.04\n",
      "epoch: 89, loss: 81.76228, loss1: 0.65145, loss2_3: 81.11082\n",
      "\ttrain_acc: 0.8638, test_acc: \u001b[31m0.8573462245684798\u001b[0m, time: 190.49\n",
      "epoch: 90, loss: 81.74620, loss1: 0.64807, loss2_3: 81.09813\n",
      "\ttrain_acc: 0.8641, test_acc: \u001b[31m0.8586393024497301\u001b[0m, time: 189.83\n",
      "best_acc: 0.8586393024497301\n",
      "epoch: 91, loss: 81.79265, loss1: 0.64852, loss2_3: 81.14413\n",
      "\ttrain_acc: 0.8640, test_acc: \u001b[31m0.8575597603653835\u001b[0m, time: 189.88\n",
      "epoch: 92, loss: 81.67505, loss1: 0.65172, loss2_3: 81.02333\n",
      "\ttrain_acc: 0.8635, test_acc: \u001b[31m0.8569784684738122\u001b[0m, time: 189.72\n",
      "epoch: 93, loss: 81.75151, loss1: 0.64749, loss2_3: 81.10402\n",
      "\ttrain_acc: 0.8641, test_acc: \u001b[31m0.8589121537457738\u001b[0m, time: 190.85\n",
      "best_acc: 0.8589121537457738\n",
      "epoch: 94, loss: 81.64045, loss1: 0.64883, loss2_3: 80.99162\n",
      "\ttrain_acc: 0.8639, test_acc: \u001b[31m0.8567174802775965\u001b[0m, time: 190.09\n",
      "epoch: 95, loss: 81.68830, loss1: 0.64701, loss2_3: 81.04129\n",
      "\ttrain_acc: 0.8629, test_acc: \u001b[31m0.8580224212586749\u001b[0m, time: 190.51\n",
      "epoch: 96, loss: 81.70481, loss1: 0.64716, loss2_3: 81.05764\n",
      "\ttrain_acc: 0.8625, test_acc: \u001b[31m0.8566344385788006\u001b[0m, time: 189.63\n",
      "epoch: 97, loss: 81.60911, loss1: 0.64798, loss2_3: 80.96113\n",
      "\ttrain_acc: 0.8641, test_acc: \u001b[31m0.8576072127646954\u001b[0m, time: 190.36\n",
      "epoch: 98, loss: 81.45117, loss1: 0.64668, loss2_3: 80.80450\n",
      "\ttrain_acc: 0.8647, test_acc: \u001b[31m0.8586511655495581\u001b[0m, time: 190.25\n",
      "epoch: 99, loss: 81.53704, loss1: 0.64670, loss2_3: 80.89034\n",
      "\ttrain_acc: 0.8636, test_acc: \u001b[31m0.8576309389643514\u001b[0m, time: 190.13\n",
      "epoch: 100, loss: 81.43649, loss1: 0.64579, loss2_3: 80.79070\n",
      "\ttrain_acc: 0.8626, test_acc: \u001b[31m0.8569666053739842\u001b[0m, time: 189.91\n",
      "epoch: 101, loss: 81.52208, loss1: 0.64827, loss2_3: 80.87382\n",
      "\ttrain_acc: 0.8647, test_acc: \u001b[31m0.8569903315736402\u001b[0m, time: 189.74\n",
      "epoch: 102, loss: 81.62536, loss1: 0.64795, loss2_3: 80.97741\n",
      "\ttrain_acc: 0.8624, test_acc: \u001b[31m0.8575478972655555\u001b[0m, time: 190.47\n",
      "epoch: 103, loss: 81.46564, loss1: 0.64667, loss2_3: 80.81898\n",
      "\ttrain_acc: 0.8636, test_acc: \u001b[31m0.8567649326769085\u001b[0m, time: 190.12\n",
      "epoch: 104, loss: 81.45261, loss1: 0.64590, loss2_3: 80.80670\n",
      "\ttrain_acc: 0.8642, test_acc: \u001b[31m0.857880064060739\u001b[0m, time: 189.41\n",
      "epoch: 105, loss: 81.32129, loss1: 0.64479, loss2_3: 80.67650\n",
      "\ttrain_acc: 0.8641, test_acc: \u001b[31m0.8570259208731241\u001b[0m, time: 171.98\n",
      "epoch: 106, loss: 81.47767, loss1: 0.64542, loss2_3: 80.83225\n",
      "\ttrain_acc: 0.8645, test_acc: \u001b[31m0.8585562607509342\u001b[0m, time: 116.06\n",
      "epoch: 107, loss: 81.47180, loss1: 0.64477, loss2_3: 80.82703\n",
      "\ttrain_acc: 0.8636, test_acc: \u001b[31m0.8576190758645234\u001b[0m, time: 115.04\n",
      "epoch: 108, loss: 81.33753, loss1: 0.64478, loss2_3: 80.69275\n",
      "\ttrain_acc: 0.8640, test_acc: \u001b[31m0.8572157304703719\u001b[0m, time: 113.72\n",
      "epoch: 109, loss: 81.31947, loss1: 0.64544, loss2_3: 80.67402\n",
      "\ttrain_acc: 0.8629, test_acc: \u001b[31m0.855554896494454\u001b[0m, time: 111.69\n",
      "epoch: 110, loss: 81.23442, loss1: 0.64636, loss2_3: 80.58806\n",
      "\ttrain_acc: 0.8648, test_acc: \u001b[31m0.8575004448662435\u001b[0m, time: 112.07\n",
      "epoch: 111, loss: 81.26925, loss1: 0.64600, loss2_3: 80.62325\n",
      "\ttrain_acc: 0.8649, test_acc: \u001b[31m0.8578088854617711\u001b[0m, time: 115.17\n",
      "epoch: 112, loss: 81.22393, loss1: 0.64561, loss2_3: 80.57832\n",
      "\ttrain_acc: 0.8640, test_acc: \u001b[31m0.8577021175633193\u001b[0m, time: 115.24\n",
      "epoch: 113, loss: 81.17112, loss1: 0.64490, loss2_3: 80.52623\n",
      "\ttrain_acc: 0.8658, test_acc: \u001b[31m0.8584494928524824\u001b[0m, time: 116.23\n",
      "epoch: 114, loss: 81.21801, loss1: 0.64744, loss2_3: 80.57057\n",
      "\ttrain_acc: 0.8643, test_acc: \u001b[31m0.8577495699626312\u001b[0m, time: 117.15\n",
      "epoch: 115, loss: 81.23889, loss1: 0.64430, loss2_3: 80.59459\n",
      "\ttrain_acc: 0.8653, test_acc: \u001b[31m0.8587935227474939\u001b[0m, time: 116.19\n",
      "epoch: 116, loss: 81.23346, loss1: 0.64320, loss2_3: 80.59026\n",
      "\ttrain_acc: 0.8650, test_acc: \u001b[31m0.8575123079660716\u001b[0m, time: 114.40\n",
      "epoch: 117, loss: 81.09928, loss1: 0.64629, loss2_3: 80.45299\n",
      "\ttrain_acc: 0.8616, test_acc: \u001b[31m0.8546177116080432\u001b[0m, time: 115.71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 118, loss: 81.17464, loss1: 0.64407, loss2_3: 80.53058\n",
      "\ttrain_acc: 0.8646, test_acc: \u001b[31m0.8573818138679637\u001b[0m, time: 115.50\n",
      "epoch: 119, loss: 81.17422, loss1: 0.64581, loss2_3: 80.52841\n",
      "\ttrain_acc: 0.8589, test_acc: \u001b[31m0.8525060798386619\u001b[0m, time: 115.69\n",
      "epoch: 120, loss: 81.22434, loss1: 0.64533, loss2_3: 80.57901\n",
      "\ttrain_acc: 0.8657, test_acc: \u001b[31m0.8588884275461178\u001b[0m, time: 115.92\n",
      "epoch: 121, loss: 81.11629, loss1: 0.64232, loss2_3: 80.47397\n",
      "\ttrain_acc: 0.8634, test_acc: \u001b[31m0.8574174031674476\u001b[0m, time: 115.24\n",
      "epoch: 122, loss: 81.10393, loss1: 0.64410, loss2_3: 80.45983\n",
      "\ttrain_acc: 0.8652, test_acc: \u001b[31m0.8578088854617711\u001b[0m, time: 116.18\n",
      "epoch: 123, loss: 81.05865, loss1: 0.64247, loss2_3: 80.41619\n",
      "\ttrain_acc: 0.8649, test_acc: \u001b[31m0.8580817367578149\u001b[0m, time: 114.68\n",
      "epoch: 124, loss: 81.03256, loss1: 0.64517, loss2_3: 80.38739\n",
      "\ttrain_acc: 0.8623, test_acc: \u001b[31m0.8563734503825849\u001b[0m, time: 115.68\n",
      "epoch: 125, loss: 81.01588, loss1: 0.64400, loss2_3: 80.37189\n",
      "\ttrain_acc: 0.8656, test_acc: \u001b[31m0.8581529153567827\u001b[0m, time: 114.84\n",
      "epoch: 126, loss: 81.09075, loss1: 0.64312, loss2_3: 80.44763\n",
      "\ttrain_acc: 0.8647, test_acc: \u001b[31m0.8568717005753603\u001b[0m, time: 115.27\n",
      "epoch: 127, loss: 80.98956, loss1: 0.64424, loss2_3: 80.34532\n",
      "\ttrain_acc: 0.8655, test_acc: \u001b[31m0.8586274393499022\u001b[0m, time: 115.72\n",
      "epoch: 128, loss: 80.88918, loss1: 0.64179, loss2_3: 80.24739\n",
      "\ttrain_acc: 0.8656, test_acc: \u001b[31m0.8577495699626312\u001b[0m, time: 114.71\n",
      "epoch: 129, loss: 80.94831, loss1: 0.64289, loss2_3: 80.30542\n",
      "\ttrain_acc: 0.8642, test_acc: \u001b[31m0.8575597603653835\u001b[0m, time: 115.51\n",
      "epoch: 130, loss: 80.98356, loss1: 0.64278, loss2_3: 80.34078\n",
      "\ttrain_acc: 0.8660, test_acc: \u001b[31m0.8584376297526544\u001b[0m, time: 115.35\n",
      "epoch: 131, loss: 80.92281, loss1: 0.64356, loss2_3: 80.27925\n",
      "\ttrain_acc: 0.8649, test_acc: \u001b[31m0.857951242659707\u001b[0m, time: 115.67\n",
      "epoch: 132, loss: 80.88799, loss1: 0.64194, loss2_3: 80.24605\n",
      "\ttrain_acc: 0.8648, test_acc: \u001b[31m0.8578444747612551\u001b[0m, time: 115.86\n",
      "epoch: 133, loss: 80.90581, loss1: 0.64099, loss2_3: 80.26483\n",
      "\ttrain_acc: 0.8658, test_acc: \u001b[31m0.8588647013464619\u001b[0m, time: 114.73\n",
      "epoch: 134, loss: 80.93842, loss1: 0.64379, loss2_3: 80.29464\n",
      "\ttrain_acc: 0.8662, test_acc: \u001b[31m0.8581766415564387\u001b[0m, time: 115.77\n",
      "epoch: 135, loss: 80.88006, loss1: 0.64206, loss2_3: 80.23800\n",
      "\ttrain_acc: 0.8658, test_acc: \u001b[31m0.8572750459695119\u001b[0m, time: 113.48\n",
      "epoch: 136, loss: 80.86076, loss1: 0.64134, loss2_3: 80.21943\n",
      "\ttrain_acc: 0.8653, test_acc: \u001b[31m0.8585562607509342\u001b[0m, time: 113.29\n",
      "epoch: 137, loss: 80.81830, loss1: 0.64233, loss2_3: 80.17596\n",
      "\ttrain_acc: 0.8660, test_acc: \u001b[31m0.8585443976511062\u001b[0m, time: 115.33\n",
      "epoch: 138, loss: 80.92646, loss1: 0.64317, loss2_3: 80.28330\n",
      "\ttrain_acc: 0.8657, test_acc: \u001b[31m0.858686754849042\u001b[0m, time: 115.76\n",
      "epoch: 139, loss: 80.83422, loss1: 0.64208, loss2_3: 80.19214\n",
      "\ttrain_acc: 0.8639, test_acc: \u001b[31m0.8576665282638354\u001b[0m, time: 116.04\n",
      "epoch: 140, loss: 80.86403, loss1: 0.64187, loss2_3: 80.22216\n",
      "\ttrain_acc: 0.8657, test_acc: \u001b[31m0.8588053858473219\u001b[0m, time: 115.16\n",
      "epoch: 141, loss: 80.73567, loss1: 0.64400, loss2_3: 80.09167\n",
      "\ttrain_acc: 0.8661, test_acc: \u001b[31m0.85869861794887\u001b[0m, time: 115.49\n",
      "epoch: 142, loss: 80.76736, loss1: 0.64390, loss2_3: 80.12347\n",
      "\ttrain_acc: 0.8632, test_acc: \u001b[31m0.8566818909781125\u001b[0m, time: 115.18\n",
      "epoch: 143, loss: 80.79400, loss1: 0.64298, loss2_3: 80.15102\n",
      "\ttrain_acc: 0.8660, test_acc: \u001b[31m0.8591494157423335\u001b[0m, time: 116.28\n",
      "best_acc: 0.8591494157423335\n",
      "epoch: 144, loss: 80.78736, loss1: 0.64267, loss2_3: 80.14469\n",
      "\ttrain_acc: 0.8660, test_acc: \u001b[31m0.8583783142535144\u001b[0m, time: 116.33\n",
      "epoch: 145, loss: 80.66091, loss1: 0.64141, loss2_3: 80.01950\n",
      "\ttrain_acc: 0.8651, test_acc: \u001b[31m0.8579631057595349\u001b[0m, time: 115.97\n",
      "epoch: 146, loss: 80.71194, loss1: 0.64124, loss2_3: 80.07071\n",
      "\ttrain_acc: 0.8667, test_acc: \u001b[31m0.8583189987543746\u001b[0m, time: 115.94\n",
      "epoch: 147, loss: 80.62561, loss1: 0.64081, loss2_3: 79.98479\n",
      "\ttrain_acc: 0.8669, test_acc: \u001b[31m0.8584257666528264\u001b[0m, time: 113.95\n",
      "epoch: 148, loss: 80.57849, loss1: 0.64047, loss2_3: 79.93803\n",
      "\ttrain_acc: 0.8661, test_acc: \u001b[31m0.8581766415564387\u001b[0m, time: 116.25\n",
      "epoch: 149, loss: 80.66486, loss1: 0.64205, loss2_3: 80.02280\n",
      "\ttrain_acc: 0.8662, test_acc: \u001b[31m0.8576783913636633\u001b[0m, time: 117.25\n",
      "epoch: 150, loss: 80.61084, loss1: 0.64217, loss2_3: 79.96867\n",
      "\ttrain_acc: 0.8635, test_acc: \u001b[31m0.8561836407853373\u001b[0m, time: 115.42\n",
      "epoch: 151, loss: 80.40557, loss1: 0.63965, loss2_3: 79.76592\n",
      "\ttrain_acc: 0.8650, test_acc: \u001b[31m0.8572750459695119\u001b[0m, time: 115.28\n",
      "epoch: 152, loss: 80.47715, loss1: 0.64303, loss2_3: 79.83412\n",
      "\ttrain_acc: 0.8654, test_acc: \u001b[31m0.8577970223619432\u001b[0m, time: 115.29\n",
      "epoch: 153, loss: 80.53774, loss1: 0.64072, loss2_3: 79.89702\n",
      "\ttrain_acc: 0.8657, test_acc: \u001b[31m0.8582478201554066\u001b[0m, time: 115.84\n",
      "epoch: 154, loss: 80.47126, loss1: 0.64082, loss2_3: 79.83044\n",
      "\ttrain_acc: 0.8665, test_acc: \u001b[31m0.8584613559523103\u001b[0m, time: 114.67\n",
      "epoch: 155, loss: 80.54281, loss1: 0.64034, loss2_3: 79.90248\n",
      "\ttrain_acc: 0.8636, test_acc: \u001b[31m0.8559345156889495\u001b[0m, time: 113.78\n",
      "epoch: 156, loss: 80.46718, loss1: 0.64053, loss2_3: 79.82665\n",
      "\ttrain_acc: 0.8670, test_acc: \u001b[31m0.8593866777388932\u001b[0m, time: 115.97\n",
      "best_acc: 0.8593866777388932\n",
      "epoch: 157, loss: 80.35538, loss1: 0.63999, loss2_3: 79.71539\n",
      "\ttrain_acc: 0.8673, test_acc: \u001b[31m0.8590189216442257\u001b[0m, time: 113.19\n",
      "epoch: 158, loss: 80.41499, loss1: 0.64033, loss2_3: 79.77466\n",
      "\ttrain_acc: 0.8651, test_acc: \u001b[31m0.8572750459695119\u001b[0m, time: 118.02\n",
      "epoch: 159, loss: 80.39331, loss1: 0.64167, loss2_3: 79.75164\n",
      "\ttrain_acc: 0.8661, test_acc: \u001b[31m0.8578444747612551\u001b[0m, time: 117.42\n",
      "epoch: 160, loss: 80.41055, loss1: 0.64078, loss2_3: 79.76977\n",
      "\ttrain_acc: 0.8653, test_acc: \u001b[31m0.8578919271605671\u001b[0m, time: 116.88\n",
      "epoch: 161, loss: 80.35562, loss1: 0.64200, loss2_3: 79.71362\n",
      "\ttrain_acc: 0.8675, test_acc: \u001b[31m0.858722344148526\u001b[0m, time: 115.98\n",
      "epoch: 162, loss: 80.29481, loss1: 0.63921, loss2_3: 79.65559\n",
      "\ttrain_acc: 0.8662, test_acc: \u001b[31m0.857951242659707\u001b[0m, time: 115.34\n",
      "epoch: 163, loss: 80.26034, loss1: 0.63872, loss2_3: 79.62162\n",
      "\ttrain_acc: 0.8673, test_acc: \u001b[31m0.859481582537517\u001b[0m, time: 114.33\n",
      "best_acc: 0.859481582537517\n",
      "epoch: 164, loss: 80.37028, loss1: 0.63941, loss2_3: 79.73088\n",
      "\ttrain_acc: 0.8666, test_acc: \u001b[31m0.8578919271605671\u001b[0m, time: 117.00\n",
      "epoch: 165, loss: 80.29931, loss1: 0.63979, loss2_3: 79.65952\n",
      "\ttrain_acc: 0.8673, test_acc: \u001b[31m0.8586274393499022\u001b[0m, time: 116.30\n",
      "epoch: 166, loss: 80.33918, loss1: 0.64099, loss2_3: 79.69819\n",
      "\ttrain_acc: 0.8660, test_acc: \u001b[31m0.857974968859363\u001b[0m, time: 114.27\n",
      "epoch: 167, loss: 80.36674, loss1: 0.64092, loss2_3: 79.72582\n",
      "\ttrain_acc: 0.8660, test_acc: \u001b[31m0.8592799098404413\u001b[0m, time: 115.59\n",
      "epoch: 168, loss: 80.31638, loss1: 0.64309, loss2_3: 79.67329\n",
      "\ttrain_acc: 0.8676, test_acc: \u001b[31m0.8601103268284003\u001b[0m, time: 116.34\n",
      "best_acc: 0.8601103268284003\n",
      "epoch: 169, loss: 80.28163, loss1: 0.63837, loss2_3: 79.64325\n",
      "\ttrain_acc: 0.8661, test_acc: \u001b[31m0.8585799869505902\u001b[0m, time: 113.08\n",
      "epoch: 170, loss: 80.29426, loss1: 0.63918, loss2_3: 79.65508\n",
      "\ttrain_acc: 0.8674, test_acc: \u001b[31m0.8591731419419895\u001b[0m, time: 116.50\n",
      "epoch: 171, loss: 80.07530, loss1: 0.63770, loss2_3: 79.43760\n",
      "\ttrain_acc: 0.8655, test_acc: \u001b[31m0.8579631057595349\u001b[0m, time: 115.05\n",
      "epoch: 172, loss: 80.25020, loss1: 0.64053, loss2_3: 79.60967\n",
      "\ttrain_acc: 0.8661, test_acc: \u001b[31m0.8581054629574708\u001b[0m, time: 114.48\n",
      "epoch: 173, loss: 80.21890, loss1: 0.63815, loss2_3: 79.58074\n",
      "\ttrain_acc: 0.8661, test_acc: \u001b[31m0.8583189987543746\u001b[0m, time: 115.79\n",
      "epoch: 174, loss: 80.33987, loss1: 0.63976, loss2_3: 79.70012\n",
      "\ttrain_acc: 0.8645, test_acc: \u001b[31m0.8570852363722641\u001b[0m, time: 116.72\n",
      "epoch: 175, loss: 80.23641, loss1: 0.63914, loss2_3: 79.59727\n",
      "\ttrain_acc: 0.8676, test_acc: \u001b[31m0.8597900231330446\u001b[0m, time: 116.09\n",
      "epoch: 176, loss: 80.12131, loss1: 0.63894, loss2_3: 79.48237\n",
      "\ttrain_acc: 0.8670, test_acc: \u001b[31m0.8592087312414733\u001b[0m, time: 114.80\n",
      "epoch: 177, loss: 80.07424, loss1: 0.63700, loss2_3: 79.43724\n",
      "\ttrain_acc: 0.8650, test_acc: \u001b[31m0.8566818909781125\u001b[0m, time: 116.30\n",
      "epoch: 178, loss: 80.30219, loss1: 0.63808, loss2_3: 79.66411\n",
      "\ttrain_acc: 0.8677, test_acc: \u001b[31m0.8588647013464619\u001b[0m, time: 113.38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 179, loss: 80.17669, loss1: 0.63985, loss2_3: 79.53684\n",
      "\ttrain_acc: 0.8658, test_acc: \u001b[31m0.857097099472092\u001b[0m, time: 115.87\n",
      "epoch: 180, loss: 80.12540, loss1: 0.63856, loss2_3: 79.48684\n",
      "\ttrain_acc: 0.8643, test_acc: \u001b[31m0.8559226525891216\u001b[0m, time: 116.19\n",
      "epoch: 181, loss: 80.15438, loss1: 0.63826, loss2_3: 79.51612\n",
      "\ttrain_acc: 0.8674, test_acc: \u001b[31m0.8585088083516222\u001b[0m, time: 115.39\n",
      "epoch: 182, loss: 80.07898, loss1: 0.63603, loss2_3: 79.44295\n",
      "\ttrain_acc: 0.8677, test_acc: \u001b[31m0.8584613559523103\u001b[0m, time: 114.94\n",
      "epoch: 183, loss: 80.07644, loss1: 0.64061, loss2_3: 79.43583\n",
      "\ttrain_acc: 0.8659, test_acc: \u001b[31m0.8577258437629752\u001b[0m, time: 114.72\n",
      "epoch: 184, loss: 80.12499, loss1: 0.63823, loss2_3: 79.48676\n",
      "\ttrain_acc: 0.8682, test_acc: \u001b[31m0.8597662969333887\u001b[0m, time: 116.89\n",
      "epoch: 185, loss: 80.11017, loss1: 0.63621, loss2_3: 79.47396\n",
      "\ttrain_acc: 0.8671, test_acc: \u001b[31m0.8584613559523103\u001b[0m, time: 115.12\n",
      "epoch: 186, loss: 80.08404, loss1: 0.63919, loss2_3: 79.44485\n",
      "\ttrain_acc: 0.8651, test_acc: \u001b[31m0.8572513197698559\u001b[0m, time: 115.88\n",
      "epoch: 187, loss: 79.93057, loss1: 0.63735, loss2_3: 79.29322\n",
      "\ttrain_acc: 0.8670, test_acc: \u001b[31m0.8578563378610831\u001b[0m, time: 115.79\n",
      "epoch: 188, loss: 80.00332, loss1: 0.63951, loss2_3: 79.36381\n",
      "\ttrain_acc: 0.8682, test_acc: \u001b[31m0.8593748146390652\u001b[0m, time: 114.65\n",
      "epoch: 189, loss: 79.96367, loss1: 0.63899, loss2_3: 79.32469\n",
      "\ttrain_acc: 0.8665, test_acc: \u001b[31m0.857097099472092\u001b[0m, time: 117.40\n",
      "epoch: 190, loss: 79.97241, loss1: 0.63511, loss2_3: 79.33730\n",
      "\ttrain_acc: 0.8682, test_acc: \u001b[31m0.8599323803309805\u001b[0m, time: 114.41\n",
      "epoch: 191, loss: 79.97567, loss1: 0.63604, loss2_3: 79.33963\n",
      "\ttrain_acc: 0.8683, test_acc: \u001b[31m0.8580580105581589\u001b[0m, time: 115.69\n",
      "epoch: 192, loss: 79.92237, loss1: 0.63748, loss2_3: 79.28490\n",
      "\ttrain_acc: 0.8650, test_acc: \u001b[31m0.8572750459695119\u001b[0m, time: 114.52\n",
      "epoch: 193, loss: 80.04986, loss1: 0.63648, loss2_3: 79.41339\n",
      "\ttrain_acc: 0.8682, test_acc: \u001b[31m0.8596002135357969\u001b[0m, time: 112.99\n",
      "epoch: 194, loss: 80.19911, loss1: 0.64063, loss2_3: 79.55848\n",
      "\ttrain_acc: 0.8688, test_acc: \u001b[31m0.8592561836407854\u001b[0m, time: 115.90\n",
      "epoch: 195, loss: 79.91214, loss1: 0.63834, loss2_3: 79.27380\n",
      "\ttrain_acc: 0.8683, test_acc: \u001b[31m0.8585918500504182\u001b[0m, time: 114.85\n",
      "epoch: 196, loss: 79.95068, loss1: 0.63696, loss2_3: 79.31372\n",
      "\ttrain_acc: 0.8679, test_acc: \u001b[31m0.8586748917492141\u001b[0m, time: 116.23\n",
      "epoch: 197, loss: 79.91822, loss1: 0.63789, loss2_3: 79.28033\n",
      "\ttrain_acc: 0.8672, test_acc: \u001b[31m0.8578088854617711\u001b[0m, time: 115.87\n",
      "epoch: 198, loss: 79.80496, loss1: 0.63885, loss2_3: 79.16611\n",
      "\ttrain_acc: 0.8686, test_acc: \u001b[31m0.8588884275461178\u001b[0m, time: 113.55\n",
      "epoch: 199, loss: 79.88926, loss1: 0.63682, loss2_3: 79.25244\n",
      "\ttrain_acc: 0.8637, test_acc: \u001b[31m0.8545346699092473\u001b[0m, time: 116.12\n",
      "epoch: 200, loss: 79.98817, loss1: 0.63781, loss2_3: 79.35036\n",
      "\ttrain_acc: 0.8653, test_acc: \u001b[31m0.8563378610831011\u001b[0m, time: 113.27\n",
      "epoch: 201, loss: 79.99094, loss1: 0.63635, loss2_3: 79.35459\n",
      "\ttrain_acc: 0.8685, test_acc: \u001b[31m0.8598256124325286\u001b[0m, time: 101.03\n",
      "epoch: 202, loss: 79.77874, loss1: 0.63733, loss2_3: 79.14140\n",
      "\ttrain_acc: 0.8673, test_acc: \u001b[31m0.8583783142535144\u001b[0m, time: 82.72\n",
      "epoch: 203, loss: 79.83415, loss1: 0.63756, loss2_3: 79.19659\n",
      "\ttrain_acc: 0.8691, test_acc: \u001b[31m0.8595527611364849\u001b[0m, time: 82.76\n",
      "epoch: 204, loss: 79.67849, loss1: 0.63661, loss2_3: 79.04188\n",
      "\ttrain_acc: 0.8690, test_acc: \u001b[31m0.8601815054273682\u001b[0m, time: 82.81\n",
      "best_acc: 0.8601815054273682\n",
      "epoch: 205, loss: 79.79691, loss1: 0.63633, loss2_3: 79.16058\n",
      "\ttrain_acc: 0.8640, test_acc: \u001b[31m0.8558158846906697\u001b[0m, time: 82.75\n",
      "epoch: 206, loss: 79.77484, loss1: 0.63579, loss2_3: 79.13905\n",
      "\ttrain_acc: 0.8685, test_acc: \u001b[31m0.858734207248354\u001b[0m, time: 82.73\n",
      "epoch: 207, loss: 79.74895, loss1: 0.63535, loss2_3: 79.11360\n",
      "\ttrain_acc: 0.8659, test_acc: \u001b[31m0.8566700278782846\u001b[0m, time: 82.84\n",
      "epoch: 208, loss: 79.69733, loss1: 0.63822, loss2_3: 79.05911\n",
      "\ttrain_acc: 0.8686, test_acc: \u001b[31m0.8590070585443976\u001b[0m, time: 82.84\n",
      "epoch: 209, loss: 79.72123, loss1: 0.63849, loss2_3: 79.08275\n",
      "\ttrain_acc: 0.8682, test_acc: \u001b[31m0.85875793344801\u001b[0m, time: 82.76\n",
      "epoch: 210, loss: 79.75037, loss1: 0.63591, loss2_3: 79.11446\n",
      "\ttrain_acc: 0.8679, test_acc: \u001b[31m0.8584257666528264\u001b[0m, time: 82.82\n",
      "epoch: 211, loss: 79.72473, loss1: 0.63732, loss2_3: 79.08740\n",
      "\ttrain_acc: 0.8676, test_acc: \u001b[31m0.8580935998576428\u001b[0m, time: 82.61\n",
      "epoch: 212, loss: 79.61379, loss1: 0.63705, loss2_3: 78.97674\n",
      "\ttrain_acc: 0.8687, test_acc: \u001b[31m0.859540898036657\u001b[0m, time: 82.76\n",
      "epoch: 213, loss: 79.66562, loss1: 0.63552, loss2_3: 79.03009\n",
      "\ttrain_acc: 0.8689, test_acc: \u001b[31m0.8585799869505902\u001b[0m, time: 82.96\n",
      "epoch: 214, loss: 79.64090, loss1: 0.63788, loss2_3: 79.00302\n",
      "\ttrain_acc: 0.8690, test_acc: \u001b[31m0.8584613559523103\u001b[0m, time: 111.58\n",
      "epoch: 215, loss: 79.47611, loss1: 0.63612, loss2_3: 78.83999\n",
      "\ttrain_acc: 0.8633, test_acc: \u001b[31m0.8552820451984103\u001b[0m, time: 119.28\n",
      "epoch: 216, loss: 79.57211, loss1: 0.63440, loss2_3: 78.93771\n",
      "\ttrain_acc: 0.8688, test_acc: \u001b[31m0.8589240168456017\u001b[0m, time: 91.36\n",
      "epoch: 217, loss: 79.66644, loss1: 0.63515, loss2_3: 79.03129\n",
      "\ttrain_acc: 0.8694, test_acc: \u001b[31m0.8594578563378611\u001b[0m, time: 91.42\n",
      "epoch: 218, loss: 79.48277, loss1: 0.63576, loss2_3: 78.84701\n",
      "\ttrain_acc: 0.8700, test_acc: \u001b[31m0.858746070348182\u001b[0m, time: 91.39\n",
      "epoch: 219, loss: 79.49186, loss1: 0.63574, loss2_3: 78.85612\n",
      "\ttrain_acc: 0.8683, test_acc: \u001b[31m0.8581054629574708\u001b[0m, time: 82.76\n",
      "epoch: 220, loss: 79.50081, loss1: 0.63560, loss2_3: 78.86521\n",
      "\ttrain_acc: 0.8665, test_acc: \u001b[31m0.8580224212586749\u001b[0m, time: 96.38\n",
      "epoch: 221, loss: 79.55514, loss1: 0.63580, loss2_3: 78.91935\n",
      "\ttrain_acc: 0.8692, test_acc: \u001b[31m0.859469719437689\u001b[0m, time: 141.09\n",
      "epoch: 222, loss: 79.45371, loss1: 0.63236, loss2_3: 78.82135\n",
      "\ttrain_acc: 0.8691, test_acc: \u001b[31m0.859517171837001\u001b[0m, time: 141.49\n",
      "epoch: 223, loss: 79.53440, loss1: 0.63610, loss2_3: 78.89829\n",
      "\ttrain_acc: 0.8665, test_acc: \u001b[31m0.8565395337801768\u001b[0m, time: 141.03\n",
      "epoch: 224, loss: 79.57898, loss1: 0.63583, loss2_3: 78.94315\n",
      "\ttrain_acc: 0.8680, test_acc: \u001b[31m0.8582240939557506\u001b[0m, time: 141.04\n",
      "epoch: 225, loss: 79.49734, loss1: 0.63567, loss2_3: 78.86167\n",
      "\ttrain_acc: 0.8691, test_acc: \u001b[31m0.8596713921347648\u001b[0m, time: 141.18\n",
      "epoch: 226, loss: 79.34748, loss1: 0.63360, loss2_3: 78.71388\n",
      "\ttrain_acc: 0.8692, test_acc: \u001b[31m0.8585918500504182\u001b[0m, time: 140.87\n",
      "epoch: 227, loss: 79.40899, loss1: 0.63499, loss2_3: 78.77400\n",
      "\ttrain_acc: 0.8678, test_acc: \u001b[31m0.8576546651640073\u001b[0m, time: 140.38\n",
      "epoch: 228, loss: 79.41804, loss1: 0.63163, loss2_3: 78.78641\n",
      "\ttrain_acc: 0.8697, test_acc: \u001b[31m0.8590663740435376\u001b[0m, time: 124.20\n",
      "epoch: 229, loss: 79.46168, loss1: 0.63313, loss2_3: 78.82855\n",
      "\ttrain_acc: 0.8697, test_acc: \u001b[31m0.8591375526425055\u001b[0m, time: 82.77\n",
      "epoch: 230, loss: 79.37574, loss1: 0.63296, loss2_3: 78.74277\n",
      "\ttrain_acc: 0.8692, test_acc: \u001b[31m0.857951242659707\u001b[0m, time: 82.87\n",
      "epoch: 231, loss: 79.28771, loss1: 0.63429, loss2_3: 78.65342\n",
      "\ttrain_acc: 0.8694, test_acc: \u001b[31m0.8585206714514503\u001b[0m, time: 82.80\n",
      "epoch: 232, loss: 79.42824, loss1: 0.63583, loss2_3: 78.79241\n",
      "\ttrain_acc: 0.8697, test_acc: \u001b[31m0.8585443976511062\u001b[0m, time: 82.87\n",
      "epoch: 233, loss: 79.26350, loss1: 0.63412, loss2_3: 78.62939\n",
      "\ttrain_acc: 0.8692, test_acc: \u001b[31m0.8590307847440536\u001b[0m, time: 82.86\n",
      "epoch: 234, loss: 79.30034, loss1: 0.63386, loss2_3: 78.66649\n",
      "\ttrain_acc: 0.8686, test_acc: \u001b[31m0.8576902544634913\u001b[0m, time: 82.77\n",
      "epoch: 235, loss: 79.34863, loss1: 0.63143, loss2_3: 78.71720\n",
      "\ttrain_acc: 0.8684, test_acc: \u001b[31m0.8579631057595349\u001b[0m, time: 82.74\n",
      "epoch: 236, loss: 79.20435, loss1: 0.63535, loss2_3: 78.56899\n",
      "\ttrain_acc: 0.8680, test_acc: \u001b[31m0.8567174802775965\u001b[0m, time: 82.70\n",
      "epoch: 237, loss: 79.30858, loss1: 0.63591, loss2_3: 78.67267\n",
      "\ttrain_acc: 0.8703, test_acc: \u001b[31m0.8591138264428495\u001b[0m, time: 82.68\n",
      "epoch: 238, loss: 79.31368, loss1: 0.63620, loss2_3: 78.67748\n",
      "\ttrain_acc: 0.8682, test_acc: \u001b[31m0.8591612788421614\u001b[0m, time: 82.76\n",
      "epoch: 239, loss: 79.31639, loss1: 0.63280, loss2_3: 78.68359\n",
      "\ttrain_acc: 0.8693, test_acc: \u001b[31m0.8588528382466338\u001b[0m, time: 89.31\n",
      "epoch: 240, loss: 79.33800, loss1: 0.63512, loss2_3: 78.70288\n",
      "\ttrain_acc: 0.8673, test_acc: \u001b[31m0.8575834865650395\u001b[0m, time: 88.63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 241, loss: 79.19835, loss1: 0.63267, loss2_3: 78.56568\n",
      "\ttrain_acc: 0.8680, test_acc: \u001b[31m0.8578682009609111\u001b[0m, time: 82.78\n",
      "epoch: 242, loss: 79.26755, loss1: 0.63614, loss2_3: 78.63142\n",
      "\ttrain_acc: 0.8696, test_acc: \u001b[31m0.8594104039385492\u001b[0m, time: 82.82\n",
      "epoch: 243, loss: 79.37116, loss1: 0.63459, loss2_3: 78.73657\n",
      "\ttrain_acc: 0.8696, test_acc: \u001b[31m0.8581054629574708\u001b[0m, time: 82.79\n",
      "epoch: 244, loss: 79.14375, loss1: 0.63611, loss2_3: 78.50765\n",
      "\ttrain_acc: 0.8703, test_acc: \u001b[31m0.8591612788421614\u001b[0m, time: 82.87\n",
      "epoch: 245, loss: 79.14292, loss1: 0.63443, loss2_3: 78.50849\n",
      "\ttrain_acc: 0.8701, test_acc: \u001b[31m0.8582240939557506\u001b[0m, time: 82.84\n",
      "epoch: 246, loss: 79.15617, loss1: 0.63455, loss2_3: 78.52162\n",
      "\ttrain_acc: 0.8688, test_acc: \u001b[31m0.8582596832552346\u001b[0m, time: 82.88\n",
      "epoch: 247, loss: 79.25144, loss1: 0.63312, loss2_3: 78.61832\n",
      "\ttrain_acc: 0.8704, test_acc: \u001b[31m0.8598374755323566\u001b[0m, time: 82.82\n",
      "epoch: 248, loss: 79.07768, loss1: 0.63400, loss2_3: 78.44368\n",
      "\ttrain_acc: 0.8702, test_acc: \u001b[31m0.8601459161278843\u001b[0m, time: 82.69\n",
      "epoch: 249, loss: 79.13403, loss1: 0.63130, loss2_3: 78.50273\n",
      "\ttrain_acc: 0.8707, test_acc: \u001b[31m0.858746070348182\u001b[0m, time: 82.71\n",
      "epoch: 250, loss: 79.17353, loss1: 0.63344, loss2_3: 78.54009\n",
      "\ttrain_acc: 0.8702, test_acc: \u001b[31m0.8592443205409573\u001b[0m, time: 82.68\n"
     ]
    }
   ],
   "source": [
    "for num_model in range(1):  # just one train\n",
    "    net=newModel().to(device)\n",
    "    lr = 0.0001\n",
    "    \n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr,weight_decay=5e-4)\n",
    "    criterion = ContrastiveLoss()\n",
    "    criterion_model = nn.CrossEntropyLoss(reduction='sum')\n",
    "    \n",
    "    best_acc=0\n",
    "    EPOCH=250\n",
    "    for epoch in range(EPOCH):\n",
    "        loss_ls=[]\n",
    "        loss1_ls=[]\n",
    "        loss2_3_ls=[]\n",
    "        t0=time.time()\n",
    "        net.train()\n",
    "        for seq1,seq2,label,label1,label2 in train_iter_cont:\n",
    "                output1=net(seq1)\n",
    "                output2=net(seq2)\n",
    "                output3=net.trainModel(seq1)\n",
    "                output4=net.trainModel(seq2)\n",
    "                \n",
    "                loss1=criterion(output1, output2, label)\n",
    "                loss2=criterion_model(output3,label1)\n",
    "                loss3=criterion_model(output4,label2)\n",
    "                loss=loss1+loss2+loss3\n",
    "    #             print(loss)\n",
    "                optimizer.zero_grad() \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                loss_ls.append(loss.item())\n",
    "                loss1_ls.append(loss1.item())\n",
    "                loss2_3_ls.append((loss2+loss3).item())\n",
    "\n",
    "\n",
    "        net.eval() \n",
    "        with torch.no_grad(): \n",
    "            train_acc=evaluate_accuracy(train_iter,net)\n",
    "            test_acc=evaluate_accuracy(val_iter,net)\n",
    "            \n",
    "        results=f\"epoch: {epoch+1}, loss: {np.mean(loss_ls):.5f}, loss1: {np.mean(loss1_ls):.5f}, loss2_3: {np.mean(loss2_3_ls):.5f}\\n\"\n",
    "        results+=f'\\ttrain_acc: {train_acc:.4f}, test_acc: {colored(test_acc,\"red\")}, time: {time.time()-t0:.2f}'\n",
    "        print(results)\n",
    "        if test_acc>best_acc:\n",
    "            best_acc=test_acc\n",
    "#             torch.save({\"best_acc\":best_acc,\"model\":net.state_dict()},f'compareModel/2021ACS_PepFormer/Model/{num_model}.pl')\n",
    "            print(f\"best_acc: {best_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T02:48:32.899680Z",
     "start_time": "2021-10-01T02:48:32.875998Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T02:48:32.904237Z",
     "start_time": "2021-10-01T02:48:32.901029Z"
    }
   },
   "outputs": [],
   "source": [
    "def pred(data_iter, net):\n",
    "    y_pred = []\n",
    "    for x, y in data_iter:\n",
    "        x,y=x.to(device),y.to(device)\n",
    "        outputs=net.trainModel(x)\n",
    "        for _ in outputs.argmax(dim=1):\n",
    "            y_pred.append(int(_))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T02:48:32.911422Z",
     "start_time": "2021-10-01T02:48:32.905318Z"
    }
   },
   "outputs": [],
   "source": [
    "def pred_prob(data_iter, net):\n",
    "    y_pred = []\n",
    "    for x, y in data_iter:\n",
    "        x,y=x.to(device),y.to(device)\n",
    "        outputs=net.trainModel(x)\n",
    "        for _ in outputs:\n",
    "            y_pred.append(list(map(float, _)))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T02:48:40.679555Z",
     "start_time": "2021-10-01T02:48:32.912418Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8695813389064923\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.82      0.86     44509\n",
      "           1       0.84      0.92      0.88     44489\n",
      "\n",
      "    accuracy                           0.87     88998\n",
      "   macro avg       0.87      0.87      0.87     88998\n",
      "weighted avg       0.87      0.87      0.87     88998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_acc = evaluate_accuracy(test_iter,net)\n",
    "print('Test Accuracy: {}'.format(test_acc))\n",
    "\n",
    "# prediction\n",
    "y_pred = pred(test_iter, net)\n",
    "print(classification_report(test_label, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T02:48:47.567845Z",
     "start_time": "2021-10-01T02:48:40.680726Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf auc : 0.9434979526444047\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMwAAADCCAYAAAAFKC2CAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAUHUlEQVR4nO3dd3hUVf7H8fd30ggJ0kMnhC7VAoIKKogruCBNqiAoiIq4iFIXN8KigIprRRHdSJEF6YILWEAWUFlAQVaqIEgPvf5ISGbO74/EmEDIzCUzyQn5vnju8zBzy5ybJ5+ce+7ce79ijEEp5RtXbjdAqbxEA6OUAxoYpRzQwCjlgAZGKQc0MEo5EBzoDwhv+4Get061Pa53bjfBGtHFwySr+eE3D/D6e3Nx47tZbiMQAh4Ypa6JKyi3W5ApDYyyk9g5WtDAKDtpD6OUA5LjwxOfaGCUnbSHUcoBHcMo5YD2MEo5oIFRygE9JFPKgSDtYZTynZ5WVsoBHcMo5YCOYZRyQHsYpRzQMYxSDmgPo5QDLjt/Ne1slVKWHpLZeSpCKVeQ98kLEWkpIjtEZJeIDM9iuYYi4haRh7w2y+FuKJUzxOV9ymp1kSBgItAKqAV0E5FaV1nuFeALX5qlgVFWEpfL6+TFbcAuY8yvxphLwCygbSbLPQPMA4760i4NjLKSiHidvCgH7E/3+kDqe+k/oxzQHpjka7s0MMpK4hLvk0g/EdmQbuqXfhOZbPbyRze9CQwzxrh9bZeeJVNWcnk/5MIYMxmYfJXZB4AK6V6XBw5dtkwDYFZqb1UCeEBEko0xC6/2mRoYZSUfDrm8WQ9UE5EY4CDQFeiefgFjTEy6z5sCfJ5VWEADoywlruwFxhiTLCIDSDn7FQTEGWO2iMiTqfN9Hrekp4FRVvJDD4MxZgmw5LL3Mg2KMaa3L9vUwCgr+TKGyQ0aGGUnO6+M0cAoO2kPo5QD2R30B4oGRlnJH4P+QNDAKCvpIZlSDmgPo5QDOobJBpdL+Pb1Dhw6cYGOLy2jwx2VGdntVmqWL0rTIfP5cdfxTNe77+YKTHj8DoJcwpSvtjNh3iYApg9pQbWyhQEoEhHG6QuJNB40j9trluKtp5pyKcnNIxOW8+uRsxSOCGX6kBY8OGpJpp+RW+bNms6yxfMBiKlSjcEjxxAaFpY2/9zZs7w+NpbDB/cTGhrGc38dTUyVamnz3W43Ax7rRomSUYyZ8C4AH018g/Vr11ClWg2Gxo4F4Oulizl39gztu/TIwb2zt4ex80DxMgNa12HH/lNpr7fsO0nX8V+yZsvhq67jcglvPnEnbUcv4eYBs+nUtCo1KxQBoOdrX9N40DwaD5rHwu9/5bO1ewAY2K4+3cZ/Sez0dfRrlXKv0YjOt/DqnI2B27lrcPxYPAvnzODduJl8OGMBHo+HlV8vy7DMzGkfUqVaDT6YPo8hf3uZ9998JcP8BbNnULFS2qVUXDh/jq0/b+KD6fNwezzs2b2TxMQEvlzyGW06dsmR/UrPl6uVc4PXwIhITREZJiJvi8hbqf+/MScaB1CueAQtG0Tz8Vfb097bceA0vxw8k+V6DatFsfvIWfbGnyMp2cOc1btofVulK5br2KQKs1ftAiDJ7SE8NJiCYcEkuT3ElL6BssUjsgxmbnG73SQmJuJOTiYxIYFiJUpmmL9vz6/c3KARABUrxRB/+BCnTp4A4NjRI6z7bhUt23RIW17ERVJSEsYYLiUmEhQUwpwZU2jX6WGCg0NybsdSuVwur1NuyPJTRWQYKXeqCbCOlCtABZiZ1T3S/vRa3zsYOXUtHuOsennZ4gU5cPx82uuDJy5QrnhEhmXurFWG+NMX2X34bMpnzd3IxKfvYsCDdZn07y2M7tGQ0TPWZ38n/KxEyVJ06taLHu3/RNcH76VgZCQNGt2RYZnK1aqzZuVyALZv/R/x8Yc5djQegPfffJW+Tz+X4ZeuYEQETe9pwVO9O1O6TDkiIiPZse1n7rirWc7tWDp+uIEsILyNYfoAtY0xSenfFJF/AFuA8ZmtlHojTz+A4HoPE1yp6TU1rlWDihw9fZGNu4/TtE4ZR+tKJtdWXJ65zndVYU5q7wKwec8J7h66EEgJ0+GT/4eIMH1IC5KSPQyP+56jZy463g9/O3f2LN+t/oZpc5cSWagQY0YO5utln9OiZeu0Zbr07MP7b7zCk706EVO5GlWr1SQoKIi13/6HIkWLUb1mLX76MeMfg849HqNzj8cA+Me4F+nV92mWLprHD+u+J6ZKdR5+tB85xdZBv7d+zQOUzeT9MqnzMmWMmWyMaWCMaXCtYQG4/cbStL4tmu2TuzNtcAvuqVeWuEHNfVr34IkLlC8Rmfa6XPEIDp28kPY6yCW0vT2GuWt2Z7r+8M43M+7THxjZ9VbG/GsDM1f+Qv82da55X/xp44a1lC5bniJFixEcHEKTe+5l6/82ZVgmIiKSwS+MYdLUOQyNfZkzp09Rumw5tmzexNo1K+nZoSVjY4ey6Yd1jB81IsO6u3ZsA6BcxWi+WrqYF16awN5fd3Fw/285tYt5tod5FlguIr/wx/3RFYGqwIAAtguA2OnriJ2+DoCmdcrwbLv6PPbGCp/W3fDLUaqWKUx0VCEOnbxAp6ZV6f368rT5zeuXZ+eB0xw8ceGKdXs0r86yDfs4feESBcOC8RiDxxgKhtlxUrFkqdJs37KZhISLhIUVYOOG/1K9Zu0My5w/d5awAuGEhISwdNE86t50CxERkfR5aiB9nhoIwE8/rmfuv6YyfNS4DOtO/XAizw6LxZ2cjMeT8nfR5RISEhJyZgdTP89GWf4GGGOWiUh1Up7AUY6U8csBYL2T+6D97cHGlfjH43dSonA48//Wis17TvDgqCWUKVaQ956+m/ZjluL2GAZNXsPiUQ8Q5BKmLt/BtnRn2jo1rcLs1buu2HZ4aDA9mlen9Yspp5Hf/mwzM4fdx6VkD73SBS433Vi7Hk2btaB/7y4EBQVRtfqNPND2IT5fMBuA1u07s2/vHl4dMxKXy0V0TBWeGzHap21/+58VVL+xNsVLRgFQq049+vXoQEzV6lSpViNg+3Q5W08ri3E4mHYqvO0Hgf2APGR7XO/cboI1oouHZZmIGsO+8Pp7s+OV+3M8VXYcYyh1maAgO3sYDYyykqVHZBoYZac8OehXKrfYOujXwCgraQ+jlAPawyjlgPYwSjmggVHKAUuPyDQwyk7awyjlgA76lXJAexilHLC0g8kbD8FQ+Y8/7un3VnZcRNqKyGYR2ZRa8q+Jt21qD6OslN1DsnRlx+8j9R4uEVlkjNmabrHlwCJjjBGResBsoGaW7cpWq5QKEBHvkxdey44bY86bP24Ii+DKorFX0B5GWckPg/7Myo43unwhEWkPjAOigD97bVd2W6VUILhEvE5+KDuOMWaBMaYm0A4Y461d2sMoK/nSw/ih7Hj6ba0SkSoiUsIYk/mzh9EeRlnKJd4nL9LKjotIKCllxxelX0BEqkrqN6QicgsQCpzIaqPawygrZXcM42PZ8Y7AIyKSBFwEuhgvT4XRwCgrBeVA2XFjzCvAK5evlxUNjLKSXkumlANBei2ZUr6ztIPRwCg76dXKSjngsrSL0cAoK2lglHJAB/1KOWBpB6OBUXbSHkYpB/SLS6Uc8MelMYGggVFWsjQvGhhlJ/3iUikH8u2g/9S8JwL9EXlG0YYBr9SeZ1zc+G6W83XQr5QDOuhXygFLj8g0MMpO+XYMo9S1sDQvGhhlJ+1hlHIgyM68aGCUnfR+GKUcCLL0EZMaGGUl7WGUckB7GKUckEwfvp/7NDDKSsHawyjlO/0eRikHLB3za2CUnYIt7WEsPVJU+Z0fisL6Unb84dSy45tF5DsRqe9tm9rDKCtl934YH8uO7wHuNsacEpFWpJT/u6JwbHoaGGUlPxyRpZUdBxCR38uOpwXGGPNduuXXklIHM0saGGUlP5wl86nseDp9gKXeNqqBUVby5dKY1DLj6UuNT06trAw+lh1P3U4zUgLTxNtnamCUlXy5vN8fZcdFpB7wEdDKGJNlBWXQwChL+eHiy7Sy48BBUsqOd0+/gIhUBOYDPY0xO33ZqAZGWSm7gfGx7HgsUBx4L/WxTsnGmAZZbVcDo6zkj+8tfSg73hfo62SbGhhlJX2Qn1IO6IP8lHLAzrhoYJSltIdRygG9p18pByzNiwZG2UkPyZRyQB+CoZQD2sMo5YCledHAKDvpWTKlHLD1kCzPPQTD7XbTuWM7BvS/stjsnl9307N7FxrcVIepH/8zw7xvV6/iwT/fT+uW9/HPD/+4heKN11/jofZtGDliaNp7ixctZMb0qYHbiWxwuYTvZw5j3ltPAjDyiQfY/cVLrJ01nLWzhnN/k1qZrvfMw834Ye5INsz5K1PH9SYsNOVvZd3q5Vg59XnWz/4rc998gkIRBQC4vX5l1n06gjWfDKFyhRIAFI4MZ9HEp3NgL/3zEIxAyHOBmTF9GpUrV8l03g2FizBsxEh6Pdonw/tut5uxL/+d9yZ9xIJF/2bZks/ZvWsX586d46dNG5m7YDEet5tfdu4gISGBRQsX0Llr90w/I7cN6N6MHXviM7z3ziff0LjreBp3Hc8Xa7ZesU7ZkoXp3+1u7nz4VRp0GkuQy0Wn+28F4P3Y7rzw9mc07DyWRd/8xKBe9wIwsGdzug35iNh3FtOvU1MARvRryatxXwR4D1MEiXidckOeCkz8kSOsXrWS9h0fynR+8eLFqVO3HsHBGY80f/7fZipUiKZ8hQqEhIbS8oE/s/Kb5bhcQlJSEsYYEhITCQ4OZkrcR3Tv0ZOQkJCc2CVHykUVoWWT2ny84DvvC18mOCiI8LAQgoJchBcI5fCxMwBUi45izQ+7AFixdjvt7r0JgKRkN+FhIRQMDyEp2U1M+RKUjSqStmygiQ//csM1B0ZEHvVnQ3zx6vixDHp+CC6Xs2YfjY+ndJnSaa+jSpUiPj6eiIhIWtz3J7p0bEe5cuWJLFSILT//TLPmLfzddL94bUhHRr61EI8n463pT3a9i3WfjmDSiw9TpFD4FesdOnaGN6ctZ+fSMez56mXOnr/I8rXbAdi6+zCt76kLQIf7bqF8qaIpnxX3JRNf6MaA7s2YNGsVowe0YfR7nwd4D//gEu9TbshODzP6ajNEpJ+IbBCRDenHC9nxn5XfUKxYMWrVruN4XZPJsw9+v9/i0T6PM3v+ZwweOpyJ77xF/2f+wvy5cxjy3EAmT3ov2+32l1ZN63D05Dk2btuf4f0P56ymVptRNOo6niPHzzL+uQ5XrFukUDit76nLja1fpPKfRhIRHkrXBxoC8MSoGTzR+S6+nTGUyIJhXEpyA7B550Hu7vU6Lfu9TaXyxTl87AyCMH38o8S99AhRxQoFdH9dIl6n3JDlWTIR2Xy1WUCpq62X/uEECcmZP6nDqU0bf2TlyhWsWb2KxMRELlw4z4hhgxn3ygSv65YqVZojh4+kvT4aH09UVFSGZbZtSzn2j46uxKvjXubjaTMYOngQv/22l+joSv7YhWy5/abKtL67Li2b1CYsNIQbIgoQ99IjPPbCtLRl4uZ/y/y3n7xi3eaNarL30AmOnzoPwMIVP9G4fgyzlqxn59542vSfCEDVilG0alr7ivWH921Jz2FxvDG8M2MmLSG6bDH6d7uHURMXB2hv8+73MKWA+4FTl70vgPMD6WwYOOh5Bg56HoD16/7L1ClxPoUFoHaduuzbt5cDB/ZTKqoUy5b8m3GvvZ5hmYnvvEXsqL+TnJyMx53yV9YlLhIuJvh3R65R7DuLiH1nEQBNb63Gs4/cy2MvTKN0iRs4cvwsAG2b12fr7sNXrLv/yEluqxtDeIEQLiYk0ey2Gvy4dR8AJYtGcuzUeUSE4Y/fz4dz12RYt0ebRixbvYXT5y5SsEAoHo/B4zEULBDYMV5e/R7mcyDSGLPp8hkisjIQDXJq9qczAejcpRvHjx2jW5eOXDh/HpfLxSfTp7Jg0RIiIyMZMTKWp/r1xeNx0659R6pWrZa2jRXLv6ZOnbpERaV0mvVuupmO7dpQvXp1atSsmSv75auXB7ajXo3yGGP47fBJnnkp5edRpmRh3ovtTvtn3mf9z7+x4OuNfP+vYSS7Pfy0/QD/nPctAJ1bNuCJLncB8NmKTUz7bG3atsMLhNCjTSNa938XgLc/WcHMCX25lJRMrxFTArpfdsYFxBi/HDFdlb8Oya4HRRsOyO0mWOPixnezzMSGPWe9/t40iLkhx3Ol3/QrK1l6RKaBUXbSwCjlgN4Po5QDlhYg08AoO+mD/JRywNK8aGCUnTQwSjmgg36lHLB10J+n7odR+Yj4MHnbhPey4zVF5HsRSRSRwb40S3sYZaXsXnzpY9nxk8BfgHY+tytbrVIqQPxwT39a2XFjzCXg97LjaYwxR40x64EkX9ulgVFW8sMtypmVHS+X3XZpYJSVfLlFOf2dvalT+hLkPpcdd0LHMMpOOVR23CkNjLJSTpQdvxYaGGWl7MbFl7LjIlIa2ADcAHhE5FmgljHm7NW2q4FRVvLHxZc+lB0/Qsqhms80MMpKtn7Tr4FRVtKLL5VyQO+HUcoBO+OigVGWyqsP8lMqd9iZFw2MspOeJVPKAR30K+WAnXHRwChL6aBfKQcszYsGRtlJA6OUA/qYJaUc0NPKSjmgp5WVcsDSvGhglJ00MEo5YOugP+BFYW0hIv1SnzKS7+nP4trlp+eS9fO+SL6hP4trlJ8Co1S2aWCUciA/BUaP2f+gP4trlG8G/Ur5Q37qYZTKtus+MN6qUOUnIhInIkdF5OfcbktedV0HJl0VqlZALaCbiNTK3VblqilAy9xuRF52XQcGH6pQ5SfGmFWklKlT1+h6D0xAqlCp/Ot6D0xAqlCp/Ot6D0xAqlCp/Ot6D0xaFSoRCSWlCtWiXG6TysOu68AYY5KB36tQbQNmG2O25G6rco+IzAS+B2qIyAER6ZPbbcpr9Jt+pRy4rnsYpfxNA6OUAxoYpRzQwCjlgAZGKQc0MEo5oIFRygENjFIO/D+kiuhJYz14IQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 216x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# AUC\n",
    "probs = np.array(pred_prob(test_iter, net))[:, 1]\n",
    "rf_auc = roc_auc_score(test_label, probs)\n",
    "print('rf auc : {}'.format(rf_auc))\n",
    "# plot the roc curve for the model81\n",
    "rf_fpr, rf_tpr, _ = roc_curve(test_label, probs)\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.plot(rf_fpr, rf_tpr, marker='.', label='RF AUC = {:.4f}'.format(rf_auc), color='orange')\n",
    "plt.title('ROC curve')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()\n",
    "\n",
    "cf_matrix = confusion_matrix(test_label, y_pred)\n",
    "plt.figure(figsize=(3, 3))\n",
    "sns.heatmap(cf_matrix, annot=True, fmt=',.0f')\n",
    "plt.show()\n",
    "plt.figure(figsize=(3, 3))\n",
    "sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, \n",
    "            fmt='.2%', cmap='Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
